{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    " \n",
    "\n",
    "# Get attention map from moondream \n",
    "\n",
    "# Finetune the model based on that  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get attention map from LLAMA3.2 Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/integration/test/tsne_visualisation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/integration/test/tsne_visualisation'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "image = Image.open('image_4.png')\n",
    "image2 = Image.open('college.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "CHECKPOINTS_DIR = os.getcwd() + \"/checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   42C    P3             54W /  300W |    2688MiB /  46068MiB |      6%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CACHE_DIR = \"cache\"\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "# model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "# model = MllamaForConditionalGeneration.from_pretrained(\n",
    "#     model_id,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     device_map = \"auto\",\n",
    "#     cache_dir=CACHE_DIR,\n",
    "#     token = huggingface_token \n",
    "# )\n",
    "# processor = AutoProcessor.from_pretrained(model_id, cache_dir=CACHE_DIR, auth_token=huggingface_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": [\n",
    "#         {\"type\": \"image\"},\n",
    "#         {\"type\": \"text\", \"text\": \"Can you please describe this image in just one sentence?\"}\n",
    "#     ]}\n",
    "# ]\n",
    "\n",
    "# input_text = processor.apply_chat_template(\n",
    "#     messages, add_generation_prompt=True,\n",
    "# )\n",
    "# inputs = processor(\n",
    "#     image,\n",
    "#     input_text,\n",
    "#     add_special_tokens=False,\n",
    "#     return_tensors=\"pt\",\n",
    "# ).to(model.device)\n",
    "# output = model.generate(**inputs, max_new_tokens=70)\n",
    "\n",
    "# print(processor.decode(output[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "\n",
    "\n",
    "# ## The image depicts a rabbit dressed in a blue coat and brown vest, standing on a dirt road in front of a stone house.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:39 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   41C    P5             40W /  300W |    2667MiB /  46068MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod -R 777 /home/integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2de17d246549ba81fbb6906cfdb59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import torch\n",
    "\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "import os \n",
    "\n",
    "expert_model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "DTYPE = torch.bfloat16\n",
    "# DTYPE = torch.float32\n",
    "\n",
    "# Clear cuda memory \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "expert_model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    expert_model_id,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir= \"checkpoints\",\n",
    "    token = huggingface_token \n",
    ")\n",
    "expert_model_processor = AutoProcessor.from_pretrained(\n",
    "    expert_model_id, \n",
    "    cache_dir=CHECKPOINTS_DIR,\n",
    "    token = huggingface_token,\n",
    "    device_map=\"auto\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   43C    P2             63W /  300W |   23672MiB /  46068MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# prompt = \"<|image|><|begin_of_text|>If I had to write a haiku for this one\"\n",
    "# inputs = expert_model_processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# output = expert_model.generate(**inputs, max_new_tokens=30)\n",
    "# print(expert_model_processor.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hololens_image = Image.open('samplehololensimage.jpg')\n",
    "hololens_image;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentionDecoderLayer (tensor([[[ 1.8921e-03,  4.0894e-03, -3.3188e-04,  ...,  1.9043e-02,\n",
      "          -4.4250e-03, -2.3651e-03],\n",
      "         [ 8.7280e-03,  1.7853e-03,  1.4526e-02,  ...,  1.0864e-02,\n",
      "          -1.7090e-02,  3.1433e-03],\n",
      "         [-2.6245e-03,  6.1035e-05,  2.2278e-03,  ...,  2.2705e-02,\n",
      "          -6.8359e-03, -3.2043e-03],\n",
      "         ...,\n",
      "         [ 4.1199e-04, -5.6152e-03, -2.0142e-03,  ..., -3.6011e-03,\n",
      "           6.7139e-04, -1.5869e-03],\n",
      "         [-2.8229e-04,  1.3794e-02, -3.3569e-04,  ..., -3.1128e-02,\n",
      "           7.4463e-03,  1.1108e-02],\n",
      "         [-8.9111e-03,  4.8828e-04,  1.1658e-02,  ..., -2.0264e-02,\n",
      "          -2.5757e-02,  1.7578e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1128,  0.0574, -0.0723,  ...,  0.8477,  0.1738,  0.1138],\n",
      "         [ 0.0011,  0.0032,  0.0098,  ..., -0.0184, -0.0115,  0.0048],\n",
      "         [-0.1035,  0.0444, -0.0625,  ...,  0.7656,  0.1553,  0.1025],\n",
      "         ...,\n",
      "         [ 0.0084, -0.0287, -0.0027,  ..., -0.0059, -0.0094, -0.0052],\n",
      "         [ 0.0217,  0.0063,  0.0031,  ..., -0.0311,  0.0075,  0.0127],\n",
      "         [-0.0054, -0.0123,  0.0059,  ..., -0.0422, -0.0231,  0.0075]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1963e-01,  6.2256e-02, -6.6895e-02,  ...,  8.6328e-01,\n",
      "           1.8848e-01,  1.0889e-01],\n",
      "         [ 3.7537e-03,  1.3672e-02,  1.9043e-02,  ..., -5.5542e-03,\n",
      "          -5.4932e-04, -7.6294e-03],\n",
      "         [-1.0791e-01, -5.9509e-04, -7.5684e-02,  ...,  7.1875e-01,\n",
      "           1.8945e-01,  4.5654e-02],\n",
      "         ...,\n",
      "         [-6.1035e-04, -5.0781e-02,  3.6865e-02,  ..., -2.1973e-02,\n",
      "          -4.8218e-03, -5.3955e-02],\n",
      "         [ 9.7656e-04, -6.1035e-03,  4.7913e-03,  ..., -4.1504e-02,\n",
      "           1.5259e-03,  1.4771e-02],\n",
      "         [-3.4668e-02, -4.1016e-02,  4.6387e-02,  ..., -7.4219e-02,\n",
      "          -2.0874e-02,  2.2705e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1201,  0.0588, -0.0630,  ...,  0.8633,  0.1855,  0.1035],\n",
      "         [-0.0282, -0.0233,  0.0135,  ..., -0.0200,  0.0121,  0.0138],\n",
      "         [-0.0889,  0.0282, -0.0796,  ...,  0.6758,  0.1670,  0.0386],\n",
      "         ...,\n",
      "         [ 0.0083, -0.0771,  0.0674,  ..., -0.0425, -0.0079, -0.0537],\n",
      "         [ 0.0354, -0.0139,  0.0299,  ..., -0.0938, -0.0317,  0.0505],\n",
      "         [-0.0771, -0.0820, -0.0391,  ..., -0.1201, -0.0232,  0.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0991,  0.0723, -0.0432,  ...,  0.9141,  0.1855,  0.1040],\n",
      "         [-0.0232, -0.0173,  0.0137,  ...,  0.0037,  0.0192,  0.0039],\n",
      "         [-0.0137,  0.0864, -0.0635,  ...,  0.7773,  0.1562,  0.0226],\n",
      "         ...,\n",
      "         [-0.0162, -0.0537,  0.0713,  ..., -0.0938, -0.0129, -0.0503],\n",
      "         [ 0.0605, -0.0337,  0.0591,  ..., -0.0718, -0.0537,  0.0781],\n",
      "         [-0.0674, -0.0811, -0.0659,  ..., -0.1206, -0.0344,  0.0261]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1226,  0.0718, -0.0417,  ...,  0.8906,  0.2197,  0.1138],\n",
      "         [ 0.0035, -0.0161,  0.0312,  ...,  0.0496,  0.0157,  0.0107],\n",
      "         [-0.0220,  0.0962, -0.1050,  ...,  0.7461,  0.1885,  0.0225],\n",
      "         ...,\n",
      "         [ 0.0082, -0.0332,  0.0337,  ..., -0.0947, -0.0771, -0.0190],\n",
      "         [ 0.0806, -0.0101,  0.0325,  ..., -0.0815, -0.1064,  0.0952],\n",
      "         [-0.0884, -0.0374, -0.1235,  ..., -0.0635, -0.0322, -0.0099]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1157,  0.0781, -0.0281,  ...,  0.8828,  0.2363,  0.1206],\n",
      "         [ 0.0035, -0.0079,  0.0024,  ...,  0.0129,  0.0229,  0.0146],\n",
      "         [-0.0240,  0.1182, -0.1230,  ...,  0.7109,  0.1729,  0.0229],\n",
      "         ...,\n",
      "         [ 0.0029,  0.0571,  0.0032,  ..., -0.2158, -0.0630, -0.0493],\n",
      "         [ 0.0674,  0.0413,  0.0610,  ..., -0.1504, -0.0249,  0.0122],\n",
      "         [-0.1128,  0.0200, -0.0542,  ..., -0.1055,  0.0090, -0.0410]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1221,  0.0981, -0.0195,  ...,  0.8398,  0.2451,  0.1167],\n",
      "         [ 0.0046, -0.0028,  0.0148,  ...,  0.0530,  0.0181,  0.0383],\n",
      "         [-0.0427,  0.1553, -0.0996,  ...,  0.6094,  0.1709,  0.0383],\n",
      "         ...,\n",
      "         [ 0.0024,  0.0505,  0.0513,  ..., -0.1221, -0.0369, -0.0879],\n",
      "         [ 0.1025, -0.0135,  0.0208,  ..., -0.1118, -0.0378,  0.0684],\n",
      "         [-0.0796, -0.0124, -0.0186,  ..., -0.0869, -0.0205, -0.0168]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0884,  0.1069,  0.0081,  ...,  0.8633,  0.2266,  0.0830],\n",
      "         [-0.0962,  0.0014,  0.1230,  ...,  0.0244, -0.0244,  0.0664],\n",
      "         [-0.0096,  0.1504, -0.0586,  ...,  0.6406,  0.1484,  0.0096],\n",
      "         ...,\n",
      "         [-0.0225,  0.0280, -0.0591,  ..., -0.2266,  0.0708, -0.0100],\n",
      "         [ 0.0977, -0.0376, -0.0850,  ..., -0.1504,  0.1709,  0.0481],\n",
      "         [-0.1514, -0.0630, -0.0796,  ..., -0.1309,  0.0449,  0.0366]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0728,  0.1650, -0.0155,  ...,  0.8008,  0.2207,  0.1104],\n",
      "         [-0.0957,  0.0142,  0.0898,  ..., -0.0403, -0.0527,  0.0464],\n",
      "         [-0.0215,  0.1226, -0.0199,  ...,  0.5625,  0.1240, -0.0010],\n",
      "         ...,\n",
      "         [-0.0410, -0.0635, -0.0276,  ..., -0.1523,  0.0986, -0.0391],\n",
      "         [ 0.0491,  0.0356, -0.0304,  ..., -0.1348,  0.1504,  0.0275],\n",
      "         [-0.1611, -0.0063, -0.0062,  ..., -0.1416, -0.0166,  0.0698]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0806,  0.1982, -0.0013,  ...,  0.7344,  0.2305,  0.1157],\n",
      "         [-0.0820,  0.0859,  0.0728,  ...,  0.0281, -0.0247, -0.0236],\n",
      "         [-0.0459,  0.1152, -0.0189,  ...,  0.5273,  0.1191, -0.0391],\n",
      "         ...,\n",
      "         [ 0.0150, -0.1211,  0.0046,  ..., -0.1084,  0.0291, -0.0366],\n",
      "         [ 0.0510,  0.0131, -0.0037,  ..., -0.1670,  0.1089, -0.0266],\n",
      "         [-0.1582, -0.0618, -0.0386,  ..., -0.1602,  0.0199,  0.0247]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0684,  0.2217, -0.0308,  ...,  0.6875,  0.2500,  0.1465],\n",
      "         [-0.1001,  0.1514,  0.0598,  ..., -0.0332, -0.0164, -0.0034],\n",
      "         [-0.0261,  0.1953, -0.0400,  ...,  0.5195,  0.0386, -0.0713],\n",
      "         ...,\n",
      "         [ 0.0347, -0.0654, -0.0432,  ..., -0.1299,  0.2031, -0.0581],\n",
      "         [ 0.0449, -0.0188, -0.0283,  ..., -0.0957,  0.1914, -0.0559],\n",
      "         [-0.1523, -0.0403, -0.0320,  ..., -0.1719,  0.1035,  0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0027,  0.3047, -0.0178,  ...,  0.5703,  0.2812,  0.1631],\n",
      "         [-0.0762,  0.1494,  0.0273,  ..., -0.0327, -0.0383, -0.0134],\n",
      "         [-0.0522,  0.2031, -0.0430,  ...,  0.3574,  0.0437, -0.0354],\n",
      "         ...,\n",
      "         [ 0.0723, -0.0742, -0.0547,  ..., -0.1387,  0.1982, -0.1133],\n",
      "         [ 0.0737, -0.0747, -0.0483,  ..., -0.0801,  0.1582, -0.0044],\n",
      "         [-0.1216,  0.0046, -0.0762,  ..., -0.1660,  0.0820,  0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0503,  0.2734,  0.0208,  ...,  0.5938,  0.2891,  0.1240],\n",
      "         [-0.1523,  0.0508,  0.0674,  ...,  0.1172, -0.0767,  0.1055],\n",
      "         [-0.1484,  0.1426,  0.0079,  ...,  0.3594,  0.0859, -0.0737],\n",
      "         ...,\n",
      "         [ 0.0430, -0.1348, -0.0532,  ..., -0.0776,  0.1621, -0.0437],\n",
      "         [-0.1514, -0.2598,  0.0156,  ...,  0.0359,  0.0859,  0.0732],\n",
      "         [-0.1934, -0.2227, -0.1816,  ...,  0.0625,  0.0996,  0.1836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-6.1279e-02,  3.0078e-01, -2.1729e-02,  ...,  5.7031e-01,\n",
      "           2.9883e-01,  1.2256e-01],\n",
      "         [-1.3184e-01,  7.2266e-02,  8.3984e-02,  ...,  6.6406e-02,\n",
      "          -6.9824e-02,  4.0283e-02],\n",
      "         [-2.1680e-01,  9.4727e-02, -1.6968e-02,  ...,  3.5352e-01,\n",
      "           9.5703e-02, -9.1309e-02],\n",
      "         ...,\n",
      "         [ 3.6133e-02,  2.6855e-02, -7.8125e-02,  ..., -1.5234e-01,\n",
      "           1.6406e-01, -4.9072e-02],\n",
      "         [-4.8828e-04, -2.5781e-01, -2.1484e-02,  ..., -3.2471e-02,\n",
      "           1.7578e-01,  5.1270e-02],\n",
      "         [-5.9570e-02, -2.3828e-01, -2.3438e-01,  ..., -1.9043e-02,\n",
      "           7.2754e-02,  1.5234e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0498,  0.3203, -0.0071,  ...,  0.5000,  0.2832,  0.1128],\n",
      "         [-0.0732,  0.0884,  0.1001,  ...,  0.0466, -0.1128,  0.0027],\n",
      "         [-0.2178,  0.0684, -0.0601,  ...,  0.3125,  0.0645, -0.0674],\n",
      "         ...,\n",
      "         [-0.0198,  0.0181,  0.0327,  ..., -0.0361,  0.1108, -0.1270],\n",
      "         [ 0.0625, -0.2021, -0.0206,  ..., -0.0267,  0.1006, -0.0200],\n",
      "         [-0.0659, -0.2012, -0.1562,  ..., -0.0498,  0.0304,  0.0674]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0513,  0.3027, -0.0513,  ...,  0.4297,  0.2734,  0.1201],\n",
      "         [-0.0576,  0.0894,  0.0840,  ...,  0.0200, -0.0806,  0.0757],\n",
      "         [-0.2676,  0.0698, -0.0796,  ...,  0.2988,  0.0508, -0.1074],\n",
      "         ...,\n",
      "         [-0.0427,  0.0840,  0.1377,  ..., -0.1562,  0.1270, -0.1143],\n",
      "         [ 0.0574, -0.1494, -0.0133,  ..., -0.1226,  0.1143,  0.0334],\n",
      "         [ 0.0228, -0.1523, -0.1006,  ..., -0.1367,  0.0080,  0.1514]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0786,  0.2637, -0.0410,  ...,  0.4316,  0.3125,  0.0366],\n",
      "         [-0.0613,  0.0757,  0.0781,  ..., -0.0286, -0.1152,  0.0216],\n",
      "         [-0.3027, -0.0150, -0.0571,  ...,  0.2793,  0.0066, -0.1328],\n",
      "         ...,\n",
      "         [-0.1177,  0.0403,  0.0078,  ..., -0.1582,  0.1660, -0.0757],\n",
      "         [ 0.1108, -0.1025, -0.0518,  ..., -0.0566,  0.0732,  0.0645],\n",
      "         [-0.0087, -0.0547, -0.1895,  ..., -0.1514, -0.0703,  0.1689]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0544,  0.2891, -0.0781,  ...,  0.3945,  0.3125,  0.0537],\n",
      "         [-0.1777,  0.1299,  0.1699,  ..., -0.0957, -0.1328, -0.0254],\n",
      "         [-0.2969, -0.0092, -0.0874,  ...,  0.2852,  0.0040, -0.0532],\n",
      "         ...,\n",
      "         [-0.0332, -0.0830, -0.0146,  ..., -0.1309,  0.1182, -0.1445],\n",
      "         [ 0.2871, -0.2334, -0.1084,  ..., -0.0623,  0.1582,  0.0552],\n",
      "         [-0.0275, -0.2441, -0.1118,  ..., -0.0869, -0.1787,  0.2393]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1084,  0.1533, -0.0107,  ...,  0.3164,  0.3516,  0.0146],\n",
      "         [-0.0933,  0.0957,  0.2090,  ..., -0.0786, -0.1592, -0.0439],\n",
      "         [-0.3340, -0.2090, -0.0728,  ...,  0.1729,  0.0425, -0.0559],\n",
      "         ...,\n",
      "         [ 0.0352,  0.0752,  0.0078,  ..., -0.2197,  0.1250, -0.0391],\n",
      "         [ 0.3398, -0.1436,  0.0869,  ..., -0.1504,  0.2227,  0.1758],\n",
      "         [ 0.0089, -0.1816,  0.0505,  ..., -0.1426, -0.1006,  0.2520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1216,  0.1523, -0.0012,  ...,  0.3047,  0.3730,  0.0009],\n",
      "         [-0.0737,  0.0449,  0.2246,  ..., -0.1562, -0.1094, -0.0554],\n",
      "         [-0.3457, -0.2578, -0.0259,  ...,  0.1553,  0.0010, -0.0288],\n",
      "         ...,\n",
      "         [-0.0386,  0.0073,  0.0571,  ..., -0.1875,  0.2266, -0.1758],\n",
      "         [ 0.2871, -0.0806,  0.0420,  ..., -0.1602,  0.2100,  0.0171],\n",
      "         [ 0.0383, -0.1689,  0.0281,  ..., -0.2061, -0.0352,  0.1245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1514,  0.1123,  0.0046,  ...,  0.2930,  0.4668, -0.0225],\n",
      "         [-0.1289,  0.0767,  0.1914,  ..., -0.2148, -0.1562, -0.1465],\n",
      "         [-0.3359, -0.2891,  0.0591,  ...,  0.1455,  0.0060, -0.1143],\n",
      "         ...,\n",
      "         [-0.1787, -0.0400,  0.0640,  ..., -0.1836,  0.2432, -0.1094],\n",
      "         [ 0.1279, -0.1328, -0.0117,  ..., -0.2012,  0.2754, -0.0352],\n",
      "         [-0.1865, -0.1270, -0.0486,  ..., -0.2500, -0.0283,  0.1182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1680,  0.1079,  0.0198,  ...,  0.3047,  0.4023, -0.0493],\n",
      "         [-0.1875,  0.1279,  0.1865,  ..., -0.2598, -0.1309, -0.1738],\n",
      "         [-0.3320, -0.2002,  0.0317,  ...,  0.1196, -0.1484, -0.0938],\n",
      "         ...,\n",
      "         [-0.2363,  0.0181,  0.0908,  ..., -0.2578,  0.1631, -0.0938],\n",
      "         [ 0.0449, -0.1377,  0.0527,  ..., -0.1035,  0.2539, -0.0198],\n",
      "         [-0.1484, -0.1426,  0.0144,  ..., -0.2598, -0.0469,  0.1514]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1709,  0.0635,  0.0085,  ...,  0.2676,  0.5078, -0.0007],\n",
      "         [-0.1855,  0.2070,  0.1875,  ..., -0.2969, -0.0625, -0.3047],\n",
      "         [-0.3613, -0.2617,  0.0156,  ...,  0.0840, -0.1094, -0.0396],\n",
      "         ...,\n",
      "         [-0.2734, -0.2363,  0.3008,  ..., -0.4785,  0.1416, -0.1221],\n",
      "         [ 0.0801, -0.2129,  0.2578,  ..., -0.2754,  0.2559, -0.0581],\n",
      "         [-0.3281, -0.1699,  0.0835,  ..., -0.4922, -0.1001,  0.2832]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797,  0.0669,  0.0342,  ...,  0.2734,  0.5078,  0.0496],\n",
      "         [-0.1104,  0.1816,  0.2266,  ..., -0.3066, -0.0850, -0.1602],\n",
      "         [-0.2578, -0.2490,  0.1328,  ...,  0.1602, -0.2021, -0.1484],\n",
      "         ...,\n",
      "         [-0.2656, -0.2676,  0.2871,  ..., -0.4199,  0.1973, -0.1709],\n",
      "         [ 0.1162, -0.1855,  0.1396,  ..., -0.3320,  0.2168, -0.0229],\n",
      "         [-0.2656, -0.1641,  0.0554,  ..., -0.4199,  0.0171,  0.3516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1699,  0.0461,  0.0283,  ...,  0.2578,  0.4551,  0.0605],\n",
      "         [-0.0479,  0.1445,  0.2148,  ..., -0.2715, -0.1367, -0.2432],\n",
      "         [-0.0625, -0.2656,  0.1177,  ...,  0.1406, -0.3242, -0.2285],\n",
      "         ...,\n",
      "         [-0.2148, -0.1953,  0.1328,  ..., -0.4648,  0.1172, -0.2793],\n",
      "         [ 0.0613, -0.1855,  0.0530,  ..., -0.3516,  0.3223, -0.1279],\n",
      "         [-0.2422, -0.1436, -0.0386,  ..., -0.2773,  0.1240,  0.3496]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.5430e-01,  3.3447e-02,  5.2246e-02,  ...,  2.8516e-01,\n",
      "           4.5312e-01,  6.8848e-02],\n",
      "         [-1.7578e-01,  1.9531e-03,  1.5332e-01,  ..., -2.9883e-01,\n",
      "          -5.1270e-02, -5.2344e-01],\n",
      "         [ 1.0352e-01, -2.5586e-01,  5.7129e-02,  ...,  1.5137e-01,\n",
      "          -4.5117e-01, -3.7695e-01],\n",
      "         ...,\n",
      "         [-2.9492e-01, -1.3672e-01,  2.3047e-01,  ..., -4.8438e-01,\n",
      "          -3.2227e-02, -2.7148e-01],\n",
      "         [-2.5757e-02, -1.9336e-01,  4.8828e-04,  ..., -3.8086e-01,\n",
      "           2.0508e-01, -1.0254e-01],\n",
      "         [-2.5000e-01, -8.4961e-02, -6.5918e-02,  ..., -3.6328e-01,\n",
      "           4.0039e-02,  3.1445e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1235,  0.0214,  0.0481,  ...,  0.3008,  0.4609,  0.0654],\n",
      "         [-0.2520,  0.1709,  0.2090,  ..., -0.2969, -0.0391, -0.5000],\n",
      "         [ 0.2891, -0.1387, -0.0337,  ...,  0.2070, -0.5859, -0.5273],\n",
      "         ...,\n",
      "         [-0.3242, -0.1660,  0.0928,  ..., -0.5039, -0.0034, -0.2598],\n",
      "         [-0.0557, -0.2891,  0.0109,  ..., -0.3945,  0.2520, -0.1699],\n",
      "         [-0.2344, -0.0049, -0.0869,  ..., -0.3848, -0.0066,  0.3535]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2051, -0.0101, -0.0811,  ...,  0.2539,  0.4355,  0.1777],\n",
      "         [-0.2891,  0.3516, -0.0032,  ..., -0.3770, -0.0947, -0.5000],\n",
      "         [ 0.1709, -0.1924, -0.1514,  ...,  0.1582, -0.6211, -0.4160],\n",
      "         ...,\n",
      "         [-0.0352,  0.0200, -0.2002,  ..., -0.6367, -0.0244, -0.3867],\n",
      "         [ 0.4727, -0.5312,  0.1367,  ..., -0.4062,  0.4277, -0.5312],\n",
      "         [-0.0693,  0.0430,  0.0039,  ..., -0.4277,  0.2168,  0.1152]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2100, -0.0232, -0.1250,  ...,  0.2363,  0.3887,  0.1562],\n",
      "         [-0.2676,  0.3496,  0.0630,  ..., -0.4727, -0.0776, -0.4473],\n",
      "         [ 0.2412, -0.1650, -0.2080,  ..., -0.0157, -0.6367, -0.5625],\n",
      "         ...,\n",
      "         [-0.0291, -0.0214, -0.2041,  ..., -0.6055,  0.0557, -0.3398],\n",
      "         [ 0.5898, -0.5352,  0.0588,  ..., -0.3984,  0.4102, -0.5391],\n",
      "         [-0.1406,  0.0048,  0.0243,  ..., -0.3926,  0.2070,  0.2012]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934, -0.0330, -0.1245,  ...,  0.2285,  0.3730,  0.1514],\n",
      "         [-0.2471,  0.4766,  0.0066,  ..., -0.3086, -0.1582, -0.4570],\n",
      "         [ 0.1836, -0.1807, -0.2559,  ..., -0.0601, -0.6953, -0.7734],\n",
      "         ...,\n",
      "         [-0.0781,  0.0527, -0.2520,  ..., -0.6328, -0.0544, -0.3262],\n",
      "         [ 0.5703, -0.4043,  0.0146,  ..., -0.3398,  0.4355, -0.4297],\n",
      "         [-0.3613, -0.0481,  0.0425,  ..., -0.3965,  0.1367,  0.2266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2012, -0.0522, -0.1631,  ...,  0.2422,  0.3496,  0.1562],\n",
      "         [-0.2520,  0.4648,  0.0164,  ..., -0.4688, -0.1128, -0.4688],\n",
      "         [ 0.1562, -0.1484, -0.4609,  ..., -0.0820, -0.7656, -0.8828],\n",
      "         ...,\n",
      "         [-0.1426,  0.1201, -0.3203,  ..., -0.7734, -0.0264, -0.2598],\n",
      "         [ 0.6289, -0.4102, -0.0432,  ..., -0.2969,  0.4707, -0.4160],\n",
      "         [-0.4180,  0.1279,  0.1709,  ..., -0.2852,  0.2080,  0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797, -0.0381, -0.1934,  ...,  0.2246,  0.3789,  0.1436],\n",
      "         [-0.2891,  0.5039,  0.0835,  ..., -0.4902, -0.1396, -0.4180],\n",
      "         [-0.0222, -0.1099, -0.6719,  ..., -0.2012, -0.7148, -0.8672],\n",
      "         ...,\n",
      "         [-0.2188,  0.2539, -0.4102,  ..., -0.6172, -0.0903, -0.3203],\n",
      "         [ 0.5352, -0.4277, -0.1230,  ..., -0.2832,  0.4629, -0.5117],\n",
      "         [-0.5781,  0.0830,  0.4023,  ..., -0.2871,  0.0508,  0.2559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1167, -0.0493, -0.3184,  ...,  0.1045,  0.5312,  0.0952],\n",
      "         [-0.1650,  0.6133, -0.1943,  ..., -0.0859, -0.2070, -0.3086],\n",
      "         [ 0.0781, -0.0991, -0.7812,  ..., -0.2480, -0.6406, -1.0000],\n",
      "         ...,\n",
      "         [-0.0986,  0.2676,  0.0820,  ..., -1.5625, -0.0146, -0.3164],\n",
      "         [ 1.0156, -0.2910,  0.1475,  ..., -0.3203,  0.2148, -0.5234],\n",
      "         [ 0.0039,  0.2148,  0.3438,  ..., -0.4512, -0.1562,  0.2754]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0938e-01, -2.9297e-02, -2.7148e-01,  ...,  1.4062e-01,\n",
      "           5.7812e-01,  1.1572e-01],\n",
      "         [-3.4180e-01,  7.8125e-01,  8.7891e-03,  ..., -1.4355e-01,\n",
      "          -3.9062e-01, -3.7500e-01],\n",
      "         [-3.8086e-02,  8.7891e-02, -9.8047e-01,  ..., -1.9141e-01,\n",
      "          -8.0859e-01, -1.1094e+00],\n",
      "         ...,\n",
      "         [-3.7109e-01,  3.7891e-01, -9.7656e-04,  ..., -1.6953e+00,\n",
      "          -1.6504e-01, -3.5352e-01],\n",
      "         [ 1.1094e+00, -4.5703e-01,  7.6660e-02,  ..., -4.1211e-01,\n",
      "           2.6367e-01, -3.7891e-01],\n",
      "         [-2.4512e-01, -7.8125e-02,  1.7871e-01,  ..., -3.6133e-01,\n",
      "          -2.1875e-01,  2.1094e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0708,  0.0259, -0.2949,  ...,  0.1562,  0.5742,  0.1426],\n",
      "         [-0.4238,  0.5234,  0.1172,  ..., -0.3418, -0.4023, -0.3340],\n",
      "         [ 0.0122,  0.2227, -1.0547,  ...,  0.0791, -0.8672, -1.2344],\n",
      "         ...,\n",
      "         [-0.5469,  0.0098, -0.0425,  ..., -1.6797, -0.1621, -0.3066],\n",
      "         [ 0.9531, -0.7266,  0.1089,  ..., -0.6016,  0.4688, -0.3652],\n",
      "         [-0.1973, -0.1455, -0.0205,  ..., -0.1768, -0.0547, -0.0322]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0396,  0.0908, -0.1641,  ...,  0.1050,  0.4062,  0.1338],\n",
      "         [-0.1230,  0.6250, -0.0068,  ..., -0.6289, -0.5781, -0.4883],\n",
      "         [-0.0547,  0.2012, -1.1719,  ...,  0.1738, -1.0469, -1.4453],\n",
      "         ...,\n",
      "         [-0.7266, -0.1494,  0.0938,  ..., -1.6562, -0.2129, -0.5742],\n",
      "         [ 0.9414, -0.9141, -0.1953,  ..., -0.6484,  0.3789, -0.2334],\n",
      "         [-0.1406, -0.3691, -0.1738,  ..., -0.1777, -0.4512, -0.2285]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 2.4414e-04,  2.5586e-01,  1.0742e-01,  ...,  1.5039e-01,\n",
      "          -2.0898e-01,  3.2812e-01],\n",
      "         [-5.6250e-01,  5.0781e-01, -3.2031e-01,  ..., -8.9844e-01,\n",
      "          -6.5234e-01, -1.0547e+00],\n",
      "         [ 3.3984e-01,  6.2500e-01, -1.7422e+00,  ...,  3.2812e-01,\n",
      "          -1.5469e+00, -1.8359e+00],\n",
      "         ...,\n",
      "         [-1.0469e+00,  7.9102e-02,  8.9844e-02,  ..., -1.9766e+00,\n",
      "           1.5234e-01, -9.7266e-01],\n",
      "         [ 6.4062e-01, -1.2109e+00, -1.6895e-01,  ..., -9.8047e-01,\n",
      "           4.3750e-01, -7.8125e-01],\n",
      "         [-2.3145e-01, -1.2793e-01, -9.9609e-02,  ..., -3.5156e-01,\n",
      "          -6.0156e-01, -6.2891e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 2.3535e-01,  8.3594e-01,  2.3828e-01,  ...,  3.3984e-01,\n",
      "           1.8359e-01,  7.5391e-01],\n",
      "         [-7.6562e-01,  3.3594e-01, -1.4648e-03,  ..., -4.6875e-01,\n",
      "          -7.4219e-01, -1.7188e+00],\n",
      "         [ 7.6562e-01,  5.8203e-01, -1.8594e+00,  ...,  6.9922e-01,\n",
      "          -1.4922e+00, -1.4062e+00],\n",
      "         ...,\n",
      "         [-1.1016e+00, -4.2969e-02, -3.1445e-01,  ..., -1.9844e+00,\n",
      "           3.1641e-01, -1.0156e+00],\n",
      "         [ 7.2656e-01, -1.4844e+00, -4.6875e-01,  ..., -1.2344e+00,\n",
      "           4.0234e-01, -7.5000e-01],\n",
      "         [-5.9766e-01, -2.0605e-01,  6.8359e-02,  ..., -1.0303e-01,\n",
      "          -4.2578e-01, -7.3438e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4961,  2.6094,  0.9453,  ..., -0.9727,  1.9453,  2.7656],\n",
      "         [-1.1094,  1.9609,  0.2559,  ..., -0.5859,  1.2656, -1.0312],\n",
      "         [ 0.5469,  1.3750,  0.0156,  ..., -1.5625,  1.3359, -2.8125],\n",
      "         ...,\n",
      "         [-0.6953,  0.4453, -0.9141,  ..., -1.0078,  0.1914, -1.1328],\n",
      "         [ 0.5078, -1.0156, -0.3516,  ..., -0.1562,  1.0000, -0.3867],\n",
      "         [-0.7695,  0.0229, -0.1494,  ...,  0.7344, -0.7930, -0.7539]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0083, -0.0043,  0.0107,  ...,  0.0042,  0.0022, -0.0014]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0089, -0.0103, -0.0007,  ...,  0.0145, -0.0007, -0.0089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0082, -0.0239, -0.0123,  ...,  0.0007,  0.0033, -0.0259]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0275, -0.0334, -0.0046,  ..., -0.0649,  0.0101, -0.0288]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-5.3711e-02, -1.8555e-02,  6.1035e-05,  ..., -6.3965e-02,\n",
      "           2.7222e-02, -4.5898e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0771,  0.0161,  0.0054,  ..., -0.0236, -0.0133, -0.0199]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0417,  0.1089,  0.0120,  ...,  0.0002, -0.0028, -0.0947]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0154,  0.1230,  0.0342,  ...,  0.0398, -0.0654, -0.0103]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0286,  0.2109, -0.1309,  ...,  0.0361, -0.0767,  0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0240,  0.1875, -0.1719,  ..., -0.0110, -0.0742,  0.0708]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0031,  0.0933, -0.0850,  ..., -0.0449, -0.1299, -0.0105]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0200,  0.0889, -0.0571,  ...,  0.0281,  0.0254, -0.0386]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0244,  0.0303, -0.1680,  ...,  0.0322,  0.0483,  0.0374]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0566,  0.1240, -0.1777,  ...,  0.0918, -0.0410, -0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0386,  0.1543, -0.1846,  ...,  0.0527,  0.0356, -0.0723]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0284,  0.1592, -0.1982,  ...,  0.0674, -0.0435, -0.1416]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0094,  0.0425, -0.0527,  ..., -0.0342, -0.0359,  0.1108]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0742,  0.0203, -0.0103,  ...,  0.0435,  0.0209,  0.0588]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1914,  0.0503, -0.0410,  ...,  0.2012, -0.1582,  0.0330]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1592,  0.0309,  0.0635,  ...,  0.1797, -0.1475,  0.2119]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2422,  0.0630,  0.0972,  ...,  0.1514, -0.2559,  0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1543,  0.0879,  0.0854,  ...,  0.1475, -0.2197,  0.1309]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1030,  0.1826,  0.2148,  ...,  0.1631, -0.1445,  0.2119]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1030,  0.1738,  0.1484,  ..., -0.0010, -0.0449, -0.0039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0571,  0.2344,  0.1904,  ..., -0.0356,  0.0483,  0.0630]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0645,  0.2139,  0.1553,  ..., -0.0198,  0.0703,  0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0243,  0.2295,  0.2617,  ..., -0.1016,  0.1309,  0.0579]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0198,  0.1934,  0.2910,  ..., -0.1299,  0.1436,  0.0291]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2207,  0.3945,  0.2227,  ..., -0.2773,  0.3301,  0.0576]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3672,  0.4473,  0.2773,  ..., -0.2891,  0.2246,  0.0811]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2500,  0.4316,  0.3711,  ..., -0.1826,  0.1318, -0.0066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2256,  0.4824,  0.5000,  ..., -0.1924,  0.0693,  0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0889,  0.4258,  0.5625,  ..., -0.2354,  0.0776,  0.2158]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0320,  0.8633,  0.5234,  ..., -0.5156, -0.5430,  0.8867]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0142,  0.5781,  0.2012,  ..., -0.6484, -0.7227,  0.7266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1309,  0.5039,  0.1025,  ..., -0.8125, -0.5117,  0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0312, -0.1445,  0.0122,  ..., -0.8398, -0.6562,  0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2754, -0.4062, -0.4004,  ..., -0.7539, -0.9062,  0.7109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1836, -0.7734, -0.3672,  ..., -0.4316, -0.6641,  0.7656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7852, -0.4336,  0.2109,  ...,  0.7734, -0.6016,  1.5156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0198,  0.0063, -0.0067,  ..., -0.0396, -0.0173,  0.0003]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0091,  0.0027, -0.0099,  ..., -0.0393, -0.0020, -0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0002,  0.0188, -0.0135,  ..., -0.0527, -0.0244, -0.0086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0096,  0.0101, -0.0728,  ..., -0.1084, -0.0056, -0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0016, -0.0064, -0.0547,  ..., -0.0938,  0.0162, -0.0679]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0194, -0.0018, -0.0206,  ..., -0.0596,  0.0122, -0.0300]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0413,  0.0112, -0.0430,  ..., -0.0684, -0.0195, -0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0654,  0.0256, -0.0432,  ..., -0.0840, -0.0087, -0.0135]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0542,  0.0332, -0.1562,  ..., -0.0986,  0.0684,  0.0229]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0645,  0.1123, -0.1416,  ..., -0.1113, -0.0122,  0.0664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0212,  0.0879, -0.0459,  ..., -0.1387, -0.0376, -0.0129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0337,  0.0571, -0.0449,  ..., -0.1514,  0.0698, -0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0625,  0.0962, -0.1250,  ..., -0.1514,  0.0933,  0.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1035,  0.0510, -0.1631,  ..., -0.0029, -0.0791, -0.0400]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1504,  0.0474, -0.0693,  ..., -0.0474, -0.0515, -0.0245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0532,  0.1289, -0.0693,  ..., -0.0598, -0.1001, -0.0569]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0234,  0.1143,  0.0065,  ..., -0.0928,  0.0088,  0.1162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0586,  0.1133,  0.0112,  ..., -0.0574,  0.0007,  0.1260]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1094, -0.0371, -0.0275,  ..., -0.0664, -0.1250,  0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1025, -0.1201, -0.0081,  ...,  0.0205, -0.0786,  0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1582,  0.0107, -0.1118,  ..., -0.0549, -0.1719,  0.0986]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1914,  0.0090, -0.1523,  ..., -0.0630, -0.1113,  0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1992,  0.0684,  0.0273,  ...,  0.0566, -0.0952,  0.2305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0576,  0.2969,  0.1328,  ..., -0.0947,  0.2129,  0.1279]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1143,  0.2949,  0.3047,  ..., -0.1318,  0.2988,  0.0161]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0693,  0.3359,  0.1191,  ..., -0.0698,  0.2559,  0.1396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0610,  0.3418,  0.1533,  ..., -0.1562,  0.3906,  0.0845]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1387,  0.4023,  0.0762,  ..., -0.1738,  0.1992,  0.0815]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.4414,  0.2793, -0.2539,  ..., -0.6992,  0.4062, -0.1572]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6250,  0.1338, -0.2275,  ..., -0.7109,  0.5508, -0.2441]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5195,  0.1641, -0.2246,  ..., -0.6133,  0.7031, -0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3867,  0.2695, -0.1846,  ..., -0.5508,  0.7148, -0.3535]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4688,  0.4531, -0.1211,  ..., -0.4941,  1.0391, -0.1738]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0488,  0.7266, -0.6367,  ..., -0.8594,  1.3125,  0.0498]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0076,  0.6445, -0.5195,  ..., -0.7695,  1.0859,  0.0986]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5742,  1.2500, -0.6016,  ..., -0.8086,  1.0938, -0.3008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2539,  0.8203, -0.8828,  ..., -0.7812,  0.9219, -0.3926]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5938,  0.7773, -0.8672,  ..., -1.2500,  1.2656, -0.5391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5234,  0.6406, -0.8320,  ..., -1.0547,  1.5938, -0.3828]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.4609,  2.4062, -1.6562,  ..., -1.7344,  0.7148,  1.4531]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0261, -0.0005,  0.0093,  ...,  0.0079,  0.0142,  0.0099]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0332,  0.0143,  0.0123,  ..., -0.0044,  0.0247,  0.0063]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0140,  0.0479,  0.0151,  ...,  0.0005,  0.0120,  0.0249]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0135,  0.0757,  0.0064,  ..., -0.0129,  0.0150,  0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0025,  0.0613,  0.0298,  ...,  0.0400,  0.0361,  0.0083]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0280,  0.0825,  0.0272,  ...,  0.0703, -0.0112,  0.0272]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0299,  0.0835,  0.0332,  ...,  0.0613,  0.0198,  0.0388]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0320,  0.0908,  0.0039,  ...,  0.0845, -0.0150,  0.0447]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0016,  0.0503, -0.0420,  ...,  0.1099, -0.0486,  0.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0087,  0.0115, -0.0011,  ..., -0.0049, -0.0525,  0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0298,  0.0723,  0.0703,  ..., -0.0265, -0.0476,  0.1079]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0371,  0.0513,  0.0435,  ..., -0.0209,  0.0204,  0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0210,  0.0305, -0.0649,  ..., -0.0129,  0.0688,  0.0869]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0076,  0.0272, -0.1367,  ...,  0.1104, -0.0977,  0.1309]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0303,  0.0293, -0.1836,  ...,  0.1025, -0.0298,  0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1221,  0.1396, -0.0708,  ...,  0.1279, -0.0996,  0.0010]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1465,  0.1494,  0.0159,  ...,  0.1377, -0.0381,  0.1221]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1562,  0.2109,  0.0518,  ...,  0.1553,  0.0133,  0.1416]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1250,  0.1055,  0.0085,  ...,  0.1660,  0.0047, -0.0166]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1299,  0.1133,  0.0123,  ...,  0.1523,  0.0344,  0.0522]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1514,  0.1973,  0.0016,  ...,  0.0000, -0.0007,  0.1045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2051,  0.1108, -0.0527,  ...,  0.0708,  0.1206,  0.0420]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2314,  0.1196,  0.0718,  ...,  0.1934,  0.1826,  0.1826]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4531,  0.0059, -0.1445,  ...,  0.2041,  0.2178, -0.2793]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5625, -0.0708,  0.0488,  ...,  0.1943,  0.3418, -0.2793]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5273,  0.0010,  0.0020,  ...,  0.2520,  0.1885, -0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6211,  0.0581,  0.1377,  ...,  0.1367,  0.3398, -0.1465]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4570,  0.1982,  0.1787,  ...,  0.1309,  0.1387, -0.2559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0684,  0.2754, -0.1143,  ...,  0.2656,  0.5078, -0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2354,  0.2129, -0.0039,  ...,  0.2227,  0.3301, -0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1719,  0.0938,  0.0608,  ...,  0.2012,  0.4023, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2256,  0.1670,  0.0217,  ...,  0.0771,  0.4180, -0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3223,  0.1748, -0.0815,  ...,  0.2656,  0.5469, -0.7578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2305,  0.1748,  0.0063,  ..., -0.3867,  1.0000, -0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1484,  0.1660, -0.2617,  ..., -0.5703,  0.7891, -0.2637]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0518,  0.1016, -0.5469,  ..., -0.5352,  1.1172, -0.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1299, -0.2656, -0.9688,  ..., -0.1865,  0.8320, -0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3242, -0.6445, -1.5312,  ..., -0.9375,  0.9688, -0.3340]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1367, -0.7422, -1.4922,  ..., -1.0312,  1.0469, -0.1035]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6055, -0.7227, -2.6250,  ..., -1.3359,  1.1719,  0.6797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0082,  0.0134,  0.0066,  ...,  0.0208,  0.0245, -0.0077]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0057,  0.0236,  0.0037,  ..., -0.0021,  0.0096, -0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0175,  0.0369,  0.0109,  ...,  0.0386,  0.0142, -0.0126]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0381,  0.0654, -0.0096,  ...,  0.0244,  0.0280, -0.0204]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0469,  0.0415,  0.0050,  ...,  0.0233,  0.0266, -0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0415,  0.0603,  0.0147,  ...,  0.0320, -0.0017, -0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0742, 0.0811, 0.0186,  ..., 0.0298, 0.0159, 0.0264]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0474,  0.0576,  0.0173,  ...,  0.0654, -0.0310,  0.0195]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0320,  0.0620, -0.0562,  ...,  0.1270,  0.0084,  0.0258]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0615, 0.0249, 0.0366,  ..., 0.0151, 0.0133, 0.0277]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0044,  0.1006,  0.0591,  ..., -0.0183, -0.0513,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0002, 0.0635, 0.0991,  ..., 0.0444, 0.0085, 0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0100,  0.0630,  0.0488,  ..., -0.0193,  0.0486,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0391,  0.0640,  0.0859,  ...,  0.0684, -0.0488, -0.0032]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0038,  0.0771,  0.0674,  ...,  0.0225,  0.0996, -0.0016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0466,  0.1455,  0.0674,  ...,  0.0898, -0.0049, -0.0408]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0623,  0.1729,  0.1367,  ...,  0.1299, -0.0271,  0.0559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0063, 0.2969, 0.1289,  ..., 0.0957, 0.0723, 0.1025]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0181,  0.1455,  0.1182,  ...,  0.1221,  0.0393,  0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0381,  0.2070,  0.1226,  ...,  0.1260,  0.0413,  0.0889]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0869,  0.5195,  0.2734,  ...,  0.0273, -0.0243,  0.1758]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0154,  0.5547,  0.1934,  ..., -0.0018,  0.1016,  0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0791,  0.5625,  0.2891,  ...,  0.1064,  0.0640,  0.3906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3047,  0.3906,  0.2812,  ...,  0.2207,  0.1445,  0.1572]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3652,  0.3887,  0.4023,  ...,  0.2715,  0.1250, -0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3809,  0.4570,  0.3652,  ...,  0.3906, -0.0200, -0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5781,  0.4863,  0.2754,  ...,  0.1270,  0.0923, -0.3320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5234,  0.3770,  0.3184,  ...,  0.1738,  0.1436, -0.4062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1855,  0.1562,  0.5078,  ...,  0.1709,  0.4316, -0.2500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1436,  0.1475,  0.4023,  ...,  0.2373,  0.2754, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1162,  0.1348,  0.4609,  ...,  0.2168,  0.6250, -0.2656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0435,  0.2500,  0.4355,  ...,  0.2383,  0.7773, -0.2754]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1250,  0.2188,  0.7617,  ...,  0.2617,  0.8242, -0.3145]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0659,  0.3340,  0.8555,  ...,  0.3730,  0.9609, -0.1182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2832,  0.1035,  0.8594,  ...,  0.3555,  0.7344, -0.0349]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8750, -0.2461,  1.4922,  ...,  0.1484,  0.5859, -0.0398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0391, -0.6797,  1.3984,  ...,  0.3359,  0.1475, -0.0076]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.6250, -1.0234,  1.0859,  ...,  0.4961,  0.4160, -0.0437]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.3828, -1.1172,  1.1328,  ...,  0.6367,  0.4512,  0.3457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.3438, -1.2656,  0.0898,  ...,  1.8906,  0.3828,  0.3828]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0058,  0.0080,  0.0010,  ..., -0.0005, -0.0037,  0.0081]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0084, -0.0011,  0.0010,  ..., -0.0249, -0.0192,  0.0026]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0064,  0.0031,  0.0016,  ...,  0.0027, -0.0266,  0.0057]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0197,  0.0010,  0.0220,  ..., -0.0771, -0.0396,  0.0096]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0325, -0.0030,  0.0261,  ..., -0.0415, -0.0113, -0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0217, -0.0023,  0.0356,  ..., -0.0277,  0.0156,  0.0220]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0206,  0.0459, -0.0095,  ..., -0.0220, -0.0388,  0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0339,  0.0369,  0.0143,  ...,  0.0208, -0.0439,  0.0884]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0287,  0.0101, -0.0547,  ...,  0.0801, -0.0271,  0.1914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0732,  0.0708, -0.0342,  ...,  0.0564, -0.0723,  0.2021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0176, -0.0315, -0.0043,  ..., -0.0103, -0.0767,  0.0913]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0281,  0.0239,  0.0142,  ..., -0.0310, -0.0122,  0.0820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0144,  0.0237,  0.0254,  ..., -0.0742, -0.0208,  0.0967]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1143, -0.0107,  0.0093,  ..., -0.0042, -0.0007,  0.1157]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0879,  0.0811,  0.0654,  ..., -0.0479, -0.0239,  0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0967,  0.0962,  0.0391,  ..., -0.0415, -0.0425,  0.0422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0110,  0.1895,  0.1318,  ...,  0.0116, -0.0347,  0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-2.6855e-03,  5.3711e-02,  9.6680e-02,  ...,  4.5654e-02,\n",
      "          -1.2207e-04,  2.3242e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0403,  0.1543,  0.1240,  ...,  0.0425, -0.1504,  0.1475]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0703,  0.1357,  0.2363,  ...,  0.0635, -0.0215,  0.2070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0229,  0.2129,  0.0854,  ..., -0.0459, -0.1895,  0.1011]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0684,  0.1357,  0.0781,  ..., -0.0654, -0.0703,  0.1270]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0449,  0.2285,  0.1660,  ..., -0.1260, -0.0520,  0.1934]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0486,  0.1152,  0.1348,  ..., -0.2354, -0.0476,  0.0869]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0098, -0.0679,  0.1201,  ..., -0.2539, -0.1807, -0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0618,  0.0159,  0.1553,  ..., -0.2598, -0.2656,  0.0339]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0601,  0.1797,  0.0825,  ..., -0.4023, -0.1875,  0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0498,  0.3301,  0.2139,  ..., -0.4043, -0.2441,  0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2080,  0.2734,  0.3906,  ..., -0.4551,  0.0791,  0.2412]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2168,  0.1426,  0.3770,  ..., -0.3848,  0.0132,  0.3984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1631,  0.3086,  0.3574,  ..., -0.5508, -0.0264,  0.4570]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1562,  0.3320,  0.5273,  ..., -0.6094, -0.0742,  0.3926]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2217,  0.4004,  0.4395,  ..., -0.5938, -0.3555,  0.5664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.5625,  0.5156,  0.2080,  ..., -0.4473, -0.4688,  0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4492,  0.6055,  0.2656,  ..., -0.4102, -0.5195,  0.6641]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0078,  0.5664,  0.5391,  ..., -0.2910, -0.5156,  0.7734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1201,  0.2793,  0.7461,  ..., -0.0498, -0.6445,  0.7656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2520, -0.0410,  0.6797,  ..., -0.2539, -0.3828,  0.4375]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1060, -0.1816,  0.5312,  ..., -0.1943, -0.4844,  0.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3672, -0.3555,  0.8477,  ...,  1.0156, -0.0820,  0.8555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0144,  0.0066, -0.0139,  ...,  0.0103, -0.0043, -0.0057]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0054,  0.0056, -0.0102,  ..., -0.0131, -0.0065,  0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0133,  0.0096, -0.0133,  ..., -0.0199,  0.0036,  0.0137]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0156,  0.0031, -0.0014,  ..., -0.0615, -0.0359, -0.0062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0327, -0.0254,  0.0308,  ..., -0.0540, -0.0088, -0.0002]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0540, -0.0569,  0.0576,  ..., -0.0693,  0.0383,  0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0605, -0.0527, -0.0085,  ..., -0.0762,  0.0201,  0.0532]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0811, -0.0659,  0.0063,  ...,  0.0181, -0.0154,  0.0608]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1143, -0.1367, -0.0938,  ...,  0.1133,  0.0181,  0.0649]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0938, -0.0771, -0.0620,  ...,  0.0508, -0.0188,  0.0564]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0776, -0.0601, -0.0610,  ...,  0.0178, -0.0220,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1211, -0.0435,  0.0134,  ...,  0.1128,  0.0119,  0.0684]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1494, -0.0022,  0.0190,  ...,  0.0005,  0.0175,  0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1748, -0.1260, -0.0010,  ...,  0.0361, -0.0229,  0.0200]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0933, -0.0874, -0.0532,  ...,  0.0137,  0.1172,  0.0025]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0337,  0.0132, -0.0513,  ..., -0.0104,  0.0596, -0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0076,  0.1270,  0.0393,  ..., -0.0034,  0.0223,  0.0408]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0200,  0.0791,  0.0422,  ..., -0.1104,  0.0364,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1270, -0.0215, -0.0713,  ..., -0.2344, -0.0312,  0.0869]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0557,  0.0040,  0.0703,  ..., -0.2207, -0.0104,  0.1777]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0437,  0.0549,  0.1309,  ..., -0.2930, -0.0552,  0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0918,  0.0889,  0.1729,  ..., -0.4316,  0.0352,  0.2168]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0100,  0.2090,  0.1699,  ..., -0.4004,  0.2227,  0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0021,  0.0791,  0.2051,  ..., -0.5312,  0.2090,  0.2969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0383,  0.1455,  0.1875,  ..., -0.6445,  0.2754,  0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2119,  0.2617,  0.2119,  ..., -0.6133,  0.4727,  0.2598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1348,  0.2852,  0.3711,  ..., -0.7812,  0.6172,  0.2695]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0303,  0.3750,  0.4570,  ..., -0.9375,  0.6094,  0.0820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0562,  0.4082,  0.5508,  ..., -1.2031,  0.7422,  0.2070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1875,  0.3652,  0.5000,  ..., -1.2109,  0.4590,  0.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0869,  0.4414,  0.5742,  ..., -1.2891,  0.5781,  0.2930]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0557,  0.5938,  0.6367,  ..., -1.2109,  0.6406,  0.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0371,  0.7617,  0.6914,  ..., -1.0938,  0.6367,  0.0483]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3242,  1.0000,  0.6641,  ..., -1.1094,  0.6836,  0.2139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2383,  0.9609,  0.7109,  ..., -1.1719,  0.7930,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3848,  0.9531,  0.8945,  ..., -1.1875,  0.7734,  0.2852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2266,  0.4961,  0.7891,  ..., -0.8945,  0.5508,  0.1475]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1855,  0.2422,  0.7344,  ..., -1.3047,  0.7500, -0.3770]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1826,  0.2441,  0.6367,  ..., -1.1172,  0.7734, -0.1089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7969,  0.3711,  1.0156,  ..., -0.0156,  0.4727,  0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0265, -0.0028,  0.0107,  ..., -0.0095, -0.0225, -0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0454,  0.0195,  0.0216,  ..., -0.0383, -0.0264,  0.0023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0742,  0.0227,  0.0437,  ..., -0.0947, -0.0082, -0.0023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0327, -0.0070,  0.0383,  ..., -0.1069, -0.0012,  0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0337, -0.0308,  0.0361,  ..., -0.1621, -0.0308,  0.0226]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1133, -0.0674,  0.0569,  ..., -0.1133, -0.0703,  0.0581]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0796, -0.0151,  0.0664,  ..., -0.1221, -0.0154,  0.0083]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1484,  0.0146,  0.0107,  ..., -0.1572, -0.0801,  0.0304]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1777,  0.0874, -0.0815,  ..., -0.0146,  0.0620,  0.0933]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1465,  0.0674, -0.0806,  ..., -0.0110,  0.0708,  0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1592,  0.1436,  0.0212,  ..., -0.0620, -0.0938,  0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1914,  0.0981,  0.0610,  ..., -0.0190,  0.0498,  0.0708]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1406,  0.0356,  0.0435,  ..., -0.0317,  0.0059,  0.0564]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1865,  0.0100,  0.0309,  ...,  0.0469,  0.0564,  0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1533,  0.1250, -0.0078,  ..., -0.0022,  0.1445,  0.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1367,  0.0352,  0.0309,  ..., -0.0505,  0.0859,  0.1118]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0181,  0.1064, -0.0115,  ..., -0.0781, -0.0830,  0.1553]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0227,  0.1670,  0.0569,  ..., -0.1611,  0.0239,  0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0625,  0.2451,  0.0654,  ..., -0.2988, -0.0464,  0.1484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0947,  0.2344,  0.1777,  ..., -0.3145,  0.0074,  0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0574,  0.2227,  0.0537,  ..., -0.2793, -0.1621,  0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2148,  0.2871,  0.2148,  ..., -0.3301, -0.1030,  0.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1348,  0.2871,  0.1836,  ..., -0.3555, -0.0413,  0.4414]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0859,  0.2490,  0.0986,  ..., -0.3867,  0.0535,  0.3945]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1650,  0.2373,  0.1406,  ..., -0.4062,  0.1475,  0.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2100,  0.2891,  0.2617,  ..., -0.3633,  0.1475,  0.4844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2461,  0.3008,  0.3535,  ..., -0.3867,  0.1572,  0.6367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3477,  0.5664,  0.5273,  ..., -0.3652,  0.0610,  0.9883]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3008,  0.6094,  0.6172,  ..., -0.3984, -0.0352,  0.9570]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5156,  0.5117,  0.7695,  ..., -0.3555, -0.2129,  0.9297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5352,  0.5273,  0.7773,  ..., -0.3145, -0.2266,  0.8086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6992,  0.8281,  0.6328,  ..., -0.3867, -0.3438,  0.7852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.7031,  0.9297,  0.5469,  ..., -0.3750, -0.1943,  0.9492]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.5625,  1.0000,  0.5430,  ..., -0.6250, -0.1196,  0.9336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.7773,  0.9141,  0.4922,  ..., -0.6367,  0.0469,  0.7617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.7148,  0.9219,  0.5977,  ..., -0.5664, -0.1396,  0.8906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.9609,  0.6055,  0.6523,  ..., -0.4297, -0.6289,  0.9648]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 1.0703,  0.6953,  0.1875,  ..., -0.5469, -0.7031,  0.8359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 1.0391,  0.6133,  0.0410,  ..., -0.7812, -0.7852,  0.8750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.9453,  0.5078, -0.5898,  ..., -0.0781, -0.1777,  0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0142, -0.0052, -0.0106,  ..., -0.0243, -0.0189,  0.0031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0212, -0.0212, -0.0156,  ..., -0.0256, -0.0227, -0.0083]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0635, -0.0282,  0.0085,  ..., -0.0260, -0.0420,  0.0138]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0361, -0.0317,  0.0082,  ..., -0.0469, -0.0276,  0.0111]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0562, -0.0605, -0.0217,  ..., -0.0361, -0.0403, -0.0243]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0830, -0.0674, -0.0260,  ...,  0.0216, -0.0483,  0.0200]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0447, -0.0586,  0.0159,  ...,  0.0405, -0.0796, -0.0124]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1133, -0.0298, -0.0034,  ...,  0.0410, -0.0923,  0.0115]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2158,  0.0087,  0.0085,  ...,  0.1177, -0.0459,  0.0332]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1758,  0.0967,  0.0708,  ...,  0.1162, -0.0031,  0.0483]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1611,  0.0791,  0.1157,  ...,  0.0645, -0.1084,  0.0674]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2402,  0.0732,  0.1245,  ...,  0.1357,  0.0166,  0.0623]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1436,  0.0225,  0.0913,  ...,  0.0752, -0.0022,  0.0215]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2129,  0.0586,  0.0757,  ..., -0.0361, -0.1855, -0.0283]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1494,  0.0879,  0.0187,  ..., -0.1055, -0.0439, -0.0206]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0586,  0.0161,  0.0076,  ..., -0.0603, -0.0967,  0.0547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0129,  0.0386,  0.0302,  ..., -0.0659, -0.2051,  0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0107,  0.0801,  0.0021,  ..., -0.0171, -0.1250,  0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0188,  0.1338,  0.0078,  ..., -0.0986, -0.4453,  0.2617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0864,  0.2334,  0.0767,  ...,  0.0024, -0.3301,  0.2451]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0205,  0.1475,  0.0466,  ..., -0.0120, -0.2773,  0.2012]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0771,  0.1689,  0.1250,  ...,  0.0117, -0.1465,  0.3398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0410,  0.2520,  0.0527,  ..., -0.0349, -0.0537,  0.3770]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0410,  0.2393,  0.0054,  ..., -0.0967,  0.1270,  0.3691]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0593,  0.2139,  0.0564,  ..., -0.1299,  0.0811,  0.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0459,  0.1445,  0.0449,  ..., -0.0640,  0.1240,  0.4160]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1201,  0.0879,  0.0928,  ..., -0.1035,  0.0010,  0.4922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2539,  0.1904,  0.1895,  ..., -0.1553, -0.0267,  0.3262]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.4492, -0.0859,  0.1963,  ..., -0.2969, -0.1562,  0.5312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5078, -0.0732,  0.0996,  ..., -0.3945, -0.2246,  0.4648]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4082,  0.0325,  0.0942,  ..., -0.4238, -0.0771,  0.5039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4883,  0.0820,  0.2422,  ..., -0.2871, -0.0522,  0.4473]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6094,  0.2734,  0.2852,  ..., -0.2500,  0.0598,  0.3242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0273,  0.7031,  0.2188,  ..., -0.4609,  0.4492,  0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0635,  0.5664,  0.0762,  ..., -0.5742,  0.3867,  0.5195]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2930,  0.6445,  0.2354,  ..., -0.4512,  0.2891,  0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5000,  0.4512,  0.2695,  ..., -0.6484,  0.2129,  0.7891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6992, -0.0625, -0.0176,  ..., -0.8203,  0.2578,  0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.1562, -0.0786, -0.0884,  ..., -0.6641, -0.2070,  0.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9258,  0.2930,  0.3770,  ...,  0.6484, -0.0293,  0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0148,  0.0046,  0.0035,  ..., -0.0004,  0.0015,  0.0123]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0012, -0.0074, -0.0159,  ...,  0.0155, -0.0078,  0.0028]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0444,  0.0178, -0.0079,  ...,  0.0025, -0.0048, -0.0041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0167,  0.0135, -0.0020,  ..., -0.0024,  0.0095, -0.0383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0752, -0.0444, -0.0479,  ...,  0.0303, -0.0284, -0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0771, -0.0649, -0.0481,  ...,  0.0942, -0.0366, -0.0308]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0486, -0.0454, -0.0039,  ...,  0.0811, -0.0549, -0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1030, -0.0337, -0.0361,  ...,  0.0635, -0.0703, -0.0554]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1270,  0.0151, -0.1758,  ...,  0.1367, -0.0957, -0.1426]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1670,  0.0042, -0.1514,  ...,  0.1660, -0.0518, -0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1426, -0.0205, -0.0688,  ...,  0.1611, -0.1025, -0.0566]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1030, -0.0403,  0.0322,  ...,  0.1602,  0.0024, -0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0306, -0.0693,  0.0237,  ...,  0.0586,  0.0332, -0.0688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0074, -0.2129, -0.0031,  ..., -0.0645, -0.0610, -0.1689]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0566, -0.0918, -0.0369,  ..., -0.1045,  0.0593, -0.1426]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0596, -0.0708, -0.0254,  ..., -0.0938, -0.0664, -0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0537, -0.0347,  0.0601,  ..., -0.0588, -0.0933, -0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0073, -0.0513,  0.1133,  ..., -0.1152,  0.0352,  0.1196]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0552,  0.0474,  0.1484,  ..., -0.1797, -0.1533,  0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0527,  0.1875,  0.2031,  ..., -0.0312, -0.3320,  0.2256]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0645,  0.1758,  0.0591,  ...,  0.1074, -0.3574,  0.2109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1660,  0.1069,  0.1348,  ...,  0.0820, -0.2461,  0.2412]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1143,  0.1592,  0.0596,  ...,  0.0503, -0.1875,  0.2236]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2344,  0.0703,  0.1865,  ..., -0.0786, -0.0098,  0.0947]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3594,  0.0918,  0.2285,  ..., -0.1099, -0.0391, -0.0508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3105,  0.2578,  0.2520,  ..., -0.2119, -0.0442, -0.0898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1660,  0.2363,  0.3535,  ..., -0.2188, -0.0298, -0.0640]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1855,  0.2891,  0.5039,  ..., -0.2676, -0.0103, -0.1118]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1201,  0.3438,  0.5195,  ..., -0.5859,  0.1768, -0.2021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0439,  0.3145,  0.3711,  ..., -0.5430,  0.0371,  0.0049]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0024,  0.2891,  0.3457,  ..., -0.4492, -0.0142, -0.0427]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1060,  0.1895,  0.4141,  ..., -0.3066,  0.0176, -0.0430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1309,  0.2227,  0.5859,  ..., -0.2676,  0.1758,  0.1309]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4004,  0.0469,  0.5664,  ...,  0.0098,  0.1328,  0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4004,  0.0693,  0.3828,  ..., -0.0040,  0.1387, -0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5625,  0.1133,  0.3789,  ..., -0.0425,  0.2949,  0.0459]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5391, -0.1260,  0.4727,  ...,  0.0732,  0.0801,  0.2285]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1562, -0.3066,  0.2539,  ..., -0.0029,  0.2480,  0.2275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.2500, -0.4805,  0.1582,  ..., -0.0566,  0.5156,  0.4570]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9102, -0.2178, -0.2695,  ...,  0.3066,  0.6406,  0.2617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0058,  0.0004,  0.0079,  ..., -0.0127, -0.0081,  0.0065]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0086, -0.0081,  0.0031,  ..., -0.0094, -0.0071, -0.0021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0161,  0.0126,  0.0193,  ...,  0.0060, -0.0035, -0.0151]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0083,  0.0181,  0.0206,  ..., -0.0347,  0.0118, -0.0080]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0249, -0.0228,  0.0220,  ..., -0.0469,  0.0078, -0.0420]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0117, -0.0217, -0.0258,  ..., -0.0500, -0.0060, -0.0066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0133, -0.0101,  0.0317,  ..., -0.0693, -0.0425,  0.0225]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0273, -0.0151,  0.0017,  ..., -0.0776, -0.0227,  0.0204]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0469, -0.0825, -0.0767,  ..., -0.0015,  0.1338,  0.1357]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0811,  0.0010, -0.0454,  ..., -0.0210,  0.1289,  0.2129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0593, -0.0356, -0.0129,  ..., -0.0527,  0.0215,  0.1748]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0623, -0.0435, -0.0356,  ..., -0.0247,  0.0518,  0.0400]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0170, -0.0133, -0.0586,  ..., -0.0713,  0.0293,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1816, -0.1816,  0.0105,  ..., -0.0815,  0.0061, -0.0239]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1196, -0.0762, -0.0282,  ..., -0.1455,  0.0408, -0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0903, -0.0164, -0.0688,  ..., -0.1045, -0.0063, -0.0337]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0488,  0.1167,  0.0684,  ..., -0.1191, -0.0566,  0.0598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1152,  0.0098,  0.0947,  ..., -0.1582, -0.0138,  0.1963]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0498, -0.0835,  0.1992,  ..., -0.2227, -0.2314,  0.2451]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0210, -0.0388,  0.2070,  ..., -0.1670, -0.4219,  0.2520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0154,  0.1021,  0.0596,  ..., -0.0728, -0.3906,  0.2500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0820,  0.0176,  0.0869,  ..., -0.0029, -0.3047,  0.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0640,  0.0723,  0.0898,  ..., -0.0247, -0.2617,  0.1973]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0073, -0.0488, -0.0352,  ...,  0.0576, -0.1943,  0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0104,  0.0486,  0.0242,  ...,  0.0645, -0.1680, -0.0244]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0991,  0.0767,  0.0073,  ..., -0.0229, -0.1484, -0.0510]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1045,  0.1226,  0.0179,  ..., -0.0293, -0.1904, -0.0129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0146,  0.0811,  0.1670,  ..., -0.1914, -0.1113, -0.1138]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1641,  0.4531,  0.5547,  ..., -0.3125, -0.0972, -0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0557,  0.5078,  0.4863,  ..., -0.2402, -0.0835,  0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0282,  0.5820,  0.5469,  ..., -0.2539, -0.0635, -0.1035]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0625,  0.5977,  0.5664,  ..., -0.2344, -0.1113, -0.0903]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0603,  0.6445,  0.5312,  ..., -0.2168, -0.0432, -0.0124]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3066,  0.5234,  0.7812,  ..., -0.2012, -0.2275, -0.1553]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2129,  0.5156,  0.8555,  ..., -0.4453, -0.0815, -0.2324]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2061,  0.6445,  1.0000,  ..., -0.3418,  0.0190, -0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4043,  0.5938,  1.0469,  ..., -0.5703, -0.1641, -0.1836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7070,  0.2812,  0.6406,  ..., -0.6953, -0.2031,  0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.3281, -0.0273,  0.2500,  ..., -0.9844, -0.5039,  0.4883]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7891,  0.2119,  0.6992,  ..., -0.2227, -0.4922,  0.5039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0088, -0.0396,  0.0031,  ...,  0.0199, -0.0068, -0.0036]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0137, -0.0376,  0.0098,  ..., -0.0046, -0.0132, -0.0186]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0342,  0.0015,  0.0596,  ...,  0.0400, -0.0327, -0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0493,  0.0630,  0.0034,  ..., -0.0017, -0.0203, -0.0242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0776,  0.0332,  0.0315,  ..., -0.0028, -0.0210, -0.0513]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0542,  0.0366,  0.0182,  ...,  0.0449, -0.0087, -0.0271]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0679,  0.0679,  0.0415,  ...,  0.0388, -0.0342, -0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1030,  0.0044, -0.0427,  ...,  0.0126, -0.0518,  0.0000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1377, -0.0410, -0.0986,  ...,  0.1221,  0.0811,  0.0825]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1309, -0.0306, -0.0219,  ...,  0.0566,  0.0859,  0.0251]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0088, -0.0439,  0.0850,  ...,  0.0176, -0.0234,  0.0032]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0231, -0.0432,  0.0525,  ...,  0.0269,  0.0444, -0.0330]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0471, -0.0684,  0.0537,  ..., -0.0139,  0.0063,  0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1055, -0.1426,  0.0559,  ..., -0.0393, -0.0306, -0.0498]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1279, -0.1816, -0.0015,  ..., -0.0830, -0.0304, -0.0859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1426, -0.1245, -0.0603,  ..., -0.0212,  0.0178, -0.0400]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1230, -0.1006, -0.0081,  ..., -0.0525, -0.0417,  0.0728]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1006, -0.0522,  0.0271,  ..., -0.1128,  0.0081,  0.1611]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0322, -0.1455,  0.2148,  ..., -0.0801, -0.2090,  0.2139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0378, -0.0986,  0.1445,  ..., -0.0598, -0.2773,  0.2988]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0596, -0.0649,  0.0840,  ...,  0.0190, -0.3242,  0.2832]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0176, -0.0933,  0.0718,  ...,  0.0039, -0.1348,  0.2354]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0449,  0.0752,  0.0242,  ..., -0.1729, -0.0430,  0.2793]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0471, -0.0879, -0.0977,  ..., -0.0352,  0.1211,  0.2354]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0623,  0.0669, -0.0757,  ...,  0.1050,  0.1348,  0.0527]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1211,  0.0762, -0.1235,  ...,  0.0820,  0.2109,  0.1621]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0564,  0.0569, -0.0977,  ...,  0.1816,  0.1885,  0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1104,  0.1553, -0.1084,  ...,  0.3047,  0.0273, -0.0605]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1133,  0.0449,  0.1543,  ...,  0.2432, -0.0596, -0.0217]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1260,  0.0840,  0.0635,  ...,  0.3320, -0.0488,  0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0967,  0.0488, -0.0762,  ...,  0.3750,  0.0400,  0.2617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0391,  0.0608, -0.1709,  ...,  0.4453,  0.0723,  0.4492]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0625,  0.0153, -0.1611,  ...,  0.5664,  0.1157,  0.6016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2402,  0.0508,  0.2061,  ..., -0.0742,  0.1426,  0.3984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3789, -0.0137,  0.1299,  ..., -0.0845,  0.0654,  0.1289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6328, -0.3555,  0.1045,  ...,  0.0396, -0.2148,  0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5469, -0.9297, -0.2148,  ...,  0.2559, -0.5977,  0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1953, -1.2344, -0.8594,  ...,  0.3047, -0.8359,  0.4180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.3906, -1.5234, -0.6055,  ...,  0.6328, -0.5156,  0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9961, -0.6016, -0.5703,  ...,  1.2188, -0.3438,  0.3301]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0025, -0.0082, -0.0005,  ..., -0.0293, -0.0104,  0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0029, -0.0048, -0.0063,  ..., -0.0415, -0.0222, -0.0018]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0410,  0.0001,  0.0122,  ..., -0.0391, -0.0215, -0.0186]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0498,  0.0115,  0.0322,  ..., -0.0615, -0.0271, -0.0066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0874, -0.0278,  0.0201,  ..., -0.0515, -0.0245,  0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0674,  0.0027,  0.0173,  ..., -0.0303,  0.0131,  0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0781, -0.0155,  0.0056,  ..., -0.0986,  0.0258,  0.0240]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1191, -0.0144, -0.0713,  ..., -0.0430, -0.0176,  0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2002, -0.0232, -0.0469,  ...,  0.0111, -0.0369,  0.1128]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1445,  0.0188, -0.0327,  ..., -0.0145, -0.0273,  0.1279]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0156,  0.0269,  0.0188,  ..., -0.0801, -0.0986,  0.0679]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0415, -0.0498, -0.0001,  ..., -0.0332, -0.0283,  0.0605]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0620, -0.0811,  0.0265,  ..., -0.0464,  0.0166,  0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1602, -0.1562, -0.0146,  ..., -0.0591, -0.0830, -0.0586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1309, -0.0674, -0.0298,  ..., -0.1221, -0.1133, -0.1182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1504, -0.0586, -0.1016,  ..., -0.0439, -0.1475, -0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1631,  0.0273, -0.0718,  ..., -0.1147, -0.1602, -0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1836,  0.0344, -0.1230,  ..., -0.2295, -0.0146,  0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2246,  0.1064, -0.1406,  ..., -0.1562, -0.1367,  0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2109,  0.0444, -0.0752,  ..., -0.2539, -0.1523,  0.0153]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2715,  0.0840, -0.1250,  ..., -0.2021, -0.0679,  0.0547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2656, -0.0347, -0.1216,  ..., -0.2002,  0.0981,  0.1377]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3340,  0.0459,  0.0195,  ..., -0.2656,  0.0251,  0.2109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4434, -0.1680, -0.0522,  ..., -0.1895, -0.0508,  0.0605]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3242, -0.0854,  0.0566,  ..., -0.2197,  0.0605,  0.0791]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4414,  0.0913,  0.0420,  ..., -0.1953,  0.0806,  0.1172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5195, -0.0103,  0.1235,  ..., -0.3066,  0.0203,  0.1260]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5273, -0.1006,  0.1328,  ..., -0.1426, -0.0674,  0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3789, -0.1074,  0.3086,  ...,  0.0928, -0.0110,  0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3711,  0.1719,  0.2539,  ...,  0.1064, -0.0820,  0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3203,  0.1279,  0.1523,  ...,  0.1680, -0.3066,  0.2715]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3125,  0.1387, -0.0093,  ...,  0.1001, -0.3887,  0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5039,  0.2051,  0.2305,  ...,  0.2324, -0.5156,  0.3672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6562,  0.5469,  0.2139,  ...,  0.1162, -0.8086,  0.6484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0625,  0.4746,  0.0498,  ..., -0.0742, -0.7734,  0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9297,  0.5156, -0.1182,  ..., -0.2471, -0.3926,  0.6367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.2188,  0.2031, -0.0610,  ...,  0.0137, -0.5586,  1.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9141, -0.5078, -0.3320,  ..., -0.0225, -0.4805,  1.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.4766, -0.7891, -0.0107,  ...,  0.2871, -0.8359,  1.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0625, -0.0273,  0.5547,  ...,  0.4062, -0.1250,  1.2422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0017,  0.0106, -0.0003,  ..., -0.0349,  0.0068,  0.0123]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0291,  0.0078,  0.0089,  ..., -0.0557,  0.0035,  0.0114]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0122,  0.0009,  0.0113,  ..., -0.0654,  0.0060,  0.0005]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0020, -0.0007,  0.0240,  ..., -0.0991, -0.0061,  0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0031, -0.0178, -0.0120,  ..., -0.0884,  0.0048,  0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0065, -0.0110, -0.0096,  ..., -0.0693, -0.0132,  0.0649]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0251, -0.0337, -0.0215,  ..., -0.1709, -0.0267,  0.0142]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0723, -0.0513, -0.0077,  ..., -0.1216, -0.0085, -0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0732, -0.1621, -0.0723,  ..., -0.0200,  0.0576, -0.0332]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0537, -0.0693,  0.0137,  ...,  0.0010, -0.0073, -0.0092]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0144, -0.1338,  0.0820,  ..., -0.1289, -0.0206, -0.0515]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0029, -0.2207,  0.1338,  ..., -0.0850, -0.0151, -0.0327]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0376, -0.2217,  0.1328,  ..., -0.0684, -0.0085,  0.0032]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2168, -0.3906,  0.0962,  ...,  0.0493, -0.0396, -0.0021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0623, -0.2910,  0.1289,  ...,  0.0078,  0.0388, -0.0153]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0420, -0.2676,  0.0151,  ..., -0.0090,  0.0159, -0.0718]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0193, -0.1167,  0.0698,  ..., -0.1138, -0.0162, -0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0869, -0.0688,  0.0059,  ..., -0.0625,  0.0103,  0.0349]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1465, -0.0967,  0.0811,  ..., -0.1074,  0.0278,  0.3086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1680, -0.0757,  0.0938,  ..., -0.1748, -0.0337,  0.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2676,  0.0374,  0.0664,  ..., -0.0947, -0.0625,  0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2236, -0.0303, -0.0156,  ..., -0.0918,  0.1094,  0.1523]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0547, -0.1836, -0.0339,  ..., -0.0500,  0.1377,  0.2490]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0771, -0.2363, -0.0481,  ..., -0.0181,  0.2266,  0.1963]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0176, -0.2100, -0.0337,  ..., -0.0535,  0.2100,  0.2012]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1738, -0.1875, -0.0309,  ..., -0.1768,  0.3594,  0.1514]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1836, -0.3848, -0.0500,  ..., -0.3105,  0.1807,  0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1895, -0.5547, -0.1465,  ..., -0.2617,  0.1689,  0.1152]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0068, -0.5312,  0.0420,  ..., -0.4707,  0.4922, -0.0303]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0540, -0.4062, -0.0898,  ..., -0.4590,  0.3438,  0.0131]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0554, -0.3457, -0.0957,  ..., -0.4238,  0.3164, -0.1069]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1406, -0.3242, -0.1416,  ..., -0.3320,  0.5156, -0.0815]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1069, -0.2363, -0.0747,  ..., -0.3594,  0.6367, -0.0598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1182, -0.3516,  0.3105,  ..., -0.1543,  0.5312, -0.0232]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0449, -0.6680, -0.0605,  ..., -0.2637,  0.7031, -0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0488, -0.7891, -0.1128,  ..., -0.1777,  0.7812,  0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1289, -1.2344, -0.0918,  ..., -0.4375,  0.6055,  0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0063, -1.6406, -0.8750,  ..., -0.5117,  0.7266,  0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0771, -1.8750, -1.1172,  ..., -0.7031,  0.6016,  0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2256, -1.5625, -0.6875,  ..., -0.3984,  1.1250,  0.3691]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0080, -0.0010,  0.0127,  ..., -0.0242, -0.0254,  0.0198]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0092, -0.0182, -0.0007,  ..., -0.0488, -0.0216,  0.0101]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0508, -0.0239,  0.0240,  ..., -0.0630, -0.0215,  0.0171]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0986, -0.0371, -0.0396,  ..., -0.1133, -0.0415,  0.0459]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0850, -0.0601, -0.0469,  ..., -0.1299, -0.0425,  0.0315]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0967, -0.0466, -0.1118,  ..., -0.0437, -0.0320, -0.0176]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1025, -0.0100, -0.0684,  ..., -0.0469, -0.0522, -0.0045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0894, -0.0525, -0.0483,  ..., -0.0007, -0.0415, -0.0182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0034, -0.1533, -0.1504,  ..., -0.0282,  0.0081,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0284, -0.0469, -0.0664,  ..., -0.0186,  0.0157,  0.0752]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0322, -0.1562, -0.0518,  ..., -0.0447, -0.0212, -0.0076]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0082, -0.1768, -0.0083,  ..., -0.0168,  0.0251, -0.0101]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0251, -0.1436, -0.0354,  ..., -0.0457,  0.0466,  0.0010]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1367, -0.2285, -0.0747,  ...,  0.0923, -0.0503, -0.0227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0093, -0.1475, -0.0535,  ...,  0.1138, -0.0171, -0.0217]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0466, -0.1211, -0.0654,  ..., -0.0120, -0.0100, -0.0713]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0110, -0.0645,  0.0293,  ..., -0.0767, -0.0811,  0.0649]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0171, -0.0121, -0.0508,  ..., -0.1328, -0.1074,  0.1348]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0269, -0.1455,  0.0732,  ..., -0.1914, -0.2578,  0.3320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0035, -0.0630,  0.1621,  ..., -0.1348, -0.2070,  0.3145]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0845, -0.1089,  0.1611,  ..., -0.0674, -0.1924,  0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0381, -0.1064,  0.1377,  ..., -0.0579, -0.0459,  0.3496]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0271, -0.2070,  0.1040,  ..., -0.0120, -0.0674,  0.4102]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2676, -0.4043, -0.0107,  ..., -0.0576, -0.0215,  0.2207]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1875, -0.4434, -0.0098,  ..., -0.0564,  0.0249,  0.1934]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2051, -0.3945, -0.0013,  ..., -0.0723,  0.0337,  0.1416]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1982, -0.3320,  0.0806,  ..., -0.1719,  0.0361,  0.1245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1963, -0.2598,  0.1777,  ..., -0.0737,  0.0204,  0.0830]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1904, -0.5703,  0.3711,  ..., -0.0208,  0.1953,  0.0452]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2168, -0.3145,  0.2793,  ..., -0.0033,  0.0649,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2969, -0.1426,  0.2363,  ..., -0.0991,  0.0305,  0.1172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3418, -0.0527,  0.1157,  ..., -0.1787,  0.1729,  0.0864]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4434,  0.1001,  0.1748,  ..., -0.3418,  0.1816,  0.2676]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8281,  0.0957,  0.2656,  ..., -0.0312,  0.5234,  0.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8008, -0.0698,  0.2070,  ...,  0.1504,  0.4980,  0.4121]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7227, -0.0889,  0.0146,  ...,  0.1064,  0.4648,  0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5820, -0.3125, -0.4199,  ...,  0.0278,  0.0000,  0.4062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8242, -0.2334, -0.7109,  ...,  0.0688, -0.3242,  0.3262]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.0391, -0.4258, -0.2363,  ...,  0.4297,  0.0508,  0.2061]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8672,  0.3594,  0.0723,  ...,  1.0703, -0.8477,  0.4629]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0007, -0.0018,  0.0096,  ...,  0.0023,  0.0005,  0.0037]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0059, -0.0033, -0.0027,  ...,  0.0140, -0.0079,  0.0006]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0034,  0.0015,  0.0050,  ...,  0.0317, -0.0205, -0.0016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0067, -0.0095, -0.0208,  ...,  0.0186,  0.0101, -0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0302, -0.0166, -0.0149,  ...,  0.0281, -0.0081, -0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0457, -0.0334, -0.0356,  ...,  0.1221, -0.0093, -0.0432]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0625, -0.0371,  0.0195,  ...,  0.0645,  0.0108, -0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0933, -0.0059,  0.0152,  ...,  0.0427,  0.0044, -0.0298]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1001, -0.0603, -0.1514,  ...,  0.1074, -0.0002, -0.0134]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0889,  0.0249, -0.0137,  ...,  0.0457,  0.0284,  0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1162,  0.0415, -0.0049,  ..., -0.0046, -0.0552,  0.0201]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0476,  0.0194,  0.0075,  ...,  0.0962,  0.0317, -0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0437, -0.0366, -0.0156,  ...,  0.0081,  0.0234,  0.0210]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1270, -0.0488,  0.0112,  ...,  0.2119, -0.0459, -0.0898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0962, -0.0256,  0.0141,  ...,  0.1777, -0.0109, -0.0859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1230, -0.0454, -0.0244,  ...,  0.0840,  0.0164, -0.1982]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1230,  0.0093,  0.1270,  ..., -0.0508, -0.0566,  0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1133, -0.0234,  0.0601,  ..., -0.1318, -0.0095,  0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0288, -0.0776,  0.1240,  ..., -0.2773, -0.1240,  0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0781,  0.0300,  0.1914,  ..., -0.2500, -0.1123,  0.0806]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0732,  0.0728,  0.2207,  ..., -0.2168, -0.2246,  0.0481]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0088,  0.0056,  0.1465,  ..., -0.1592, -0.2236, -0.0308]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0776,  0.0522,  0.2139,  ..., -0.1357, -0.1152,  0.0464]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2324,  0.0381,  0.2041,  ..., -0.3848, -0.1147,  0.0889]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2324,  0.0403,  0.2715,  ..., -0.3750, -0.1147,  0.0059]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3652, -0.0654,  0.2695,  ..., -0.4141, -0.0471,  0.0918]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3105, -0.0315,  0.2676,  ..., -0.5234, -0.0293,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1338, -0.0386,  0.1689,  ..., -0.4609, -0.1367,  0.1001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2617, -0.2490, -0.1914,  ..., -0.6094, -0.0396,  0.4629]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0889, -0.0410, -0.2715,  ..., -0.6172, -0.2305,  0.4688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1357,  0.2949, -0.1934,  ..., -0.5977, -0.1602,  0.5195]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0654,  0.4531, -0.1729,  ..., -0.5078, -0.1904,  0.6484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1562,  0.5391, -0.0557,  ..., -0.4805, -0.1191,  0.7461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6719,  0.9492,  0.3496,  ..., -0.6250, -0.5586,  1.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8828,  0.7539,  0.3203,  ..., -0.5430, -0.7344,  1.1094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9727,  0.4805,  0.2031,  ..., -0.5938, -0.6641,  1.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8750,  0.4082, -0.1553,  ..., -0.4824, -1.1719,  1.3203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.2109,  0.3789, -0.4824,  ..., -0.1748, -1.7969,  0.7188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.1562,  0.1406, -0.2617,  ...,  0.1079, -1.5781,  0.8477]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.4219,  0.7734,  0.0361,  ...,  0.9688, -2.0625,  0.8164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0081,  0.0068, -0.0055,  ..., -0.0315, -0.0120, -0.0002]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0012, -0.0043, -0.0052,  ..., -0.0291,  0.0046, -0.0055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0090,  0.0123,  0.0243,  ..., -0.0601, -0.0190, -0.0171]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0026, -0.0022, -0.0153,  ..., -0.1104,  0.0015, -0.0131]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0190, -0.0206, -0.0027,  ..., -0.1064,  0.0071, -0.0398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0332, -0.0254,  0.0045,  ..., -0.0496,  0.0186,  0.0024]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0127, -0.0190, -0.0237,  ..., -0.0305,  0.0277,  0.0051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0598,  0.0269, -0.0493,  ..., -0.0228, -0.0262,  0.0076]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0938, -0.0312, -0.1533,  ..., -0.0046,  0.0864,  0.0574]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0942,  0.0435, -0.0801,  ...,  0.0020,  0.0430,  0.1289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0447,  0.0449,  0.0193,  ..., -0.0183, -0.0461,  0.0728]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0732,  0.0193,  0.0117,  ..., -0.0005,  0.0542, -0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1133,  0.0403, -0.1572,  ..., -0.0259,  0.0825,  0.0295]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2695,  0.0156, -0.0938,  ...,  0.0415, -0.1836, -0.0056]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2070, -0.0388, -0.0703,  ..., -0.0352, -0.0303,  0.0101]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1777,  0.0023, -0.0747,  ..., -0.0991, -0.0457, -0.0249]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1348,  0.0728,  0.0664,  ..., -0.1328, -0.0898,  0.1094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0247,  0.1206,  0.1328,  ..., -0.0811, -0.1270,  0.1177]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0435, -0.0576,  0.2109,  ..., -0.0630, -0.3125,  0.0244]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0308, -0.0415,  0.1216,  ..., -0.0198, -0.2461, -0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0635,  0.0142,  0.0530,  ..., -0.1201, -0.3066,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0869, -0.0249,  0.0466,  ..., -0.0664, -0.2207,  0.0525]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0220, -0.0255,  0.0918,  ...,  0.0459, -0.1260,  0.2139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0825,  0.0986,  0.2930,  ...,  0.1523,  0.0063,  0.3789]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1758, -0.0166,  0.2637,  ...,  0.0530,  0.0996,  0.3066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0693,  0.0222,  0.1582,  ...,  0.1406,  0.1641,  0.3867]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0498,  0.0378,  0.1128,  ...,  0.0225,  0.1455,  0.3379]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0737,  0.0278, -0.0635,  ...,  0.0464,  0.0396,  0.2871]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3008,  0.0583, -0.4277,  ..., -0.2539,  0.2656,  0.2061]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4453,  0.0366, -0.4824,  ..., -0.2539,  0.3848,  0.2090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3418,  0.2695, -0.3926,  ..., -0.0928,  0.5703,  0.5117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3086,  0.2988, -0.4102,  ..., -0.1357,  0.6523,  0.3828]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3496,  0.4141, -0.5352,  ..., -0.2432,  0.8984,  0.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0254,  0.8594, -0.4453,  ..., -0.7188,  1.0625,  0.5742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0361,  0.7695, -0.2070,  ..., -0.5938,  0.9062,  0.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3359,  0.9336, -0.2402,  ..., -0.6172,  1.0000,  0.0273]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0273,  0.5391, -0.6172,  ..., -0.5469,  0.8008, -0.2090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1670,  0.5938, -0.7031,  ..., -0.8125,  1.0078, -0.3457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0056,  0.6797, -0.7305,  ..., -0.7422,  1.1875, -0.2598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4316,  1.9375, -0.5977,  ..., -1.3672,  0.9219,  0.6523]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0135,  0.0138,  0.0012,  ..., -0.0150,  0.0058,  0.0030]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0076,  0.0046, -0.0107,  ..., -0.0239,  0.0084, -0.0028]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0033,  0.0172, -0.0156,  ..., -0.0483,  0.0103, -0.0178]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0105,  0.0126, -0.0356,  ..., -0.0459,  0.0188, -0.0160]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0139,  0.0311,  0.0088,  ..., -0.0132,  0.0464, -0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0134,  0.0299,  0.0148,  ...,  0.0214,  0.0461, -0.0188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0908,  0.0249, -0.0057,  ...,  0.0330,  0.0173, -0.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0820,  0.0420, -0.0361,  ...,  0.0386, -0.0371, -0.0405]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1016, -0.0625, -0.0623,  ...,  0.0728,  0.0276, -0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0342, -0.0854, -0.0576,  ...,  0.0654,  0.0327,  0.0090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0137, -0.0549,  0.0444,  ...,  0.0393, -0.0457, -0.0190]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0273, -0.0664,  0.1055,  ...,  0.0209,  0.0168, -0.0203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0894, -0.0664,  0.0225,  ..., -0.0227,  0.0547, -0.0420]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0452, -0.1143,  0.0713,  ...,  0.0214,  0.0269, -0.0767]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0376, -0.1348, -0.0112,  ...,  0.0183,  0.1133, -0.0247]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0791, -0.0684, -0.0055,  ..., -0.0486,  0.0698, -0.0439]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0742, -0.0288,  0.0996,  ..., -0.1187,  0.0679,  0.0425]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1089,  0.0664,  0.2305,  ..., -0.1396,  0.0403,  0.0027]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0889,  0.0063,  0.2295,  ..., -0.2148, -0.1719, -0.0618]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1289, -0.0059,  0.2246,  ..., -0.1494, -0.0859,  0.0586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1553,  0.0264,  0.1128,  ..., -0.2734, -0.0059,  0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2617, -0.0139,  0.1152,  ..., -0.1992,  0.0579,  0.1348]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1289, -0.0576,  0.0713,  ..., -0.3125, -0.0249,  0.1143]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0391, -0.2285,  0.2129,  ..., -0.4961,  0.0286, -0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0053, -0.1270,  0.2031,  ..., -0.7227,  0.0664, -0.2275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0569, -0.1348,  0.1270,  ..., -0.7969,  0.1787, -0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0044, -0.1377,  0.1133,  ..., -0.8477,  0.3945, -0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0327, -0.0073,  0.0703,  ..., -0.8320,  0.3848, -0.3477]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1162, -0.0894,  0.1270,  ..., -0.8828,  0.3594, -0.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3164, -0.0674,  0.0532,  ..., -0.9414,  0.3418, -0.1904]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2734,  0.1973,  0.1230,  ..., -1.0078,  0.3711, -0.2598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2988,  0.1660, -0.0762,  ..., -1.0391,  0.5078, -0.3398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1816, -0.0430, -0.0859,  ..., -1.0312,  0.5547, -0.2451]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1514,  0.0474, -0.1108,  ..., -1.1094,  0.6562, -0.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2773, -0.1406, -0.2148,  ..., -1.0312,  0.3633,  0.1123]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1025, -0.1211,  0.0195,  ..., -1.2031,  0.3340,  0.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0415, -0.3516, -0.3281,  ..., -1.3359,  0.0088,  0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2734, -0.7305, -0.7344,  ..., -1.7422,  0.4453,  0.1475]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2051, -0.6641, -0.8438,  ..., -1.8594,  0.7266,  0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3359, -0.8633, -0.8438,  ..., -1.3984,  0.8633,  1.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0106, -0.0044,  0.0356,  ..., -0.0322, -0.0084, -0.0026]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0273, -0.0080,  0.0361,  ..., -0.0410, -0.0137, -0.0155]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0048,  0.0119,  0.0271,  ..., -0.0503, -0.0332, -0.0084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0109,  0.0245,  0.0322,  ..., -0.0620, -0.0282, -0.0095]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0308,  0.0022,  0.0078,  ..., -0.0884, -0.0081, -0.0044]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0410,  0.0203,  0.0037,  ..., -0.0674, -0.0430, -0.0554]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1045,  0.0381,  0.0027,  ..., -0.0300,  0.0117, -0.1001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0483, -0.0094, -0.0352,  ...,  0.0014, -0.0742, -0.0229]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0149, -0.0322, -0.0771,  ..., -0.0116,  0.1338, -0.0461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0272, -0.0635, -0.0540,  ..., -0.0210,  0.0947,  0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0096,  0.0400, -0.0144,  ...,  0.0009, -0.0449,  0.0304]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0525, -0.0166,  0.0091,  ..., -0.0003, -0.0007,  0.0139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0508, -0.0089, -0.0359,  ..., -0.0420,  0.0197, -0.0123]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0488, -0.0042, -0.0752,  ..., -0.0277, -0.1001, -0.0297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0352, -0.0020, -0.1196,  ..., -0.0070,  0.0518,  0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1211, -0.0923, -0.0620,  ..., -0.0498,  0.0034, -0.0205]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1699, -0.0347, -0.0269,  ..., -0.1494,  0.0082,  0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1738, -0.0190,  0.1064,  ..., -0.1289, -0.0024, -0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3203,  0.0004,  0.0654,  ..., -0.1816, -0.1133, -0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1934,  0.0228,  0.1953,  ..., -0.1279, -0.0049, -0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2402,  0.0723,  0.0781,  ..., -0.1787, -0.0186, -0.0752]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2227,  0.1299,  0.1484,  ..., -0.1484,  0.0181, -0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0718,  0.0981,  0.2500,  ..., -0.2207,  0.0120, -0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1133,  0.1064,  0.3438,  ..., -0.2598,  0.2617, -0.0903]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0645,  0.3125,  0.3984,  ..., -0.4531,  0.2500, -0.0913]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0410,  0.2412,  0.2178,  ..., -0.6016,  0.1992, -0.0483]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0028,  0.1660,  0.4297,  ..., -0.7188,  0.1680,  0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0591,  0.2031,  0.3555,  ..., -0.6953,  0.1270,  0.0337]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0381,  0.3145,  0.2852,  ..., -1.0312,  0.0327,  0.0576]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0737,  0.0928,  0.3457,  ..., -1.1797,  0.0559,  0.0854]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0034, -0.0195,  0.2969,  ..., -1.1797,  0.2070,  0.2090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1602,  0.2158,  0.3359,  ..., -1.1875,  0.4336,  0.1934]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1523,  0.2520,  0.2461,  ..., -1.4219,  0.5820,  0.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2168,  0.4258,  0.3203,  ..., -1.7812,  0.2109,  0.2988]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0586,  0.2617,  0.4453,  ..., -1.7578,  0.0420,  0.3555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3105,  0.1855,  0.6523,  ..., -1.8672, -0.2461,  0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4375, -0.1221,  0.2793,  ..., -1.9531, -0.4297,  0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8125, -0.2578, -0.5273,  ..., -2.1250, -0.4395,  0.3613]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7969, -0.3320, -0.8242,  ..., -2.0625, -0.1973,  0.2637]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2031,  0.4492, -0.3555,  ..., -2.0938,  0.0322, -0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0040,  0.0100,  0.0225,  ...,  0.0212, -0.0177,  0.0066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0109,  0.0165,  0.0262,  ...,  0.0121, -0.0134,  0.0322]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0038,  0.0444,  0.0500,  ...,  0.0216, -0.0393,  0.0188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0016,  0.0132,  0.0830,  ..., -0.0056, -0.0339,  0.0201]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0330,  0.0020,  0.0742,  ..., -0.0295, -0.0405, -0.0056]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0425,  0.0179,  0.0869,  ...,  0.0459, -0.0305, -0.0160]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0498, -0.0029,  0.1270,  ...,  0.1240, -0.0173, -0.0057]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0084,  0.0090,  0.0278,  ...,  0.1504, -0.0654,  0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1162,  0.0042, -0.0286,  ...,  0.1797,  0.0864,  0.1348]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1357,  0.0034, -0.0571,  ...,  0.1807,  0.0747,  0.1602]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0889,  0.0312,  0.0181,  ...,  0.1406, -0.0281,  0.1191]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0610, -0.0081,  0.1172,  ...,  0.1729, -0.0013,  0.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0928, -0.0815,  0.0742,  ...,  0.1299, -0.0289,  0.0562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1011, -0.1895,  0.0977,  ...,  0.1270, -0.1206, -0.0039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0027, -0.0947, -0.0085,  ...,  0.1836, -0.0579, -0.0374]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1279, -0.0854,  0.0159,  ...,  0.0659, -0.0884,  0.0024]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1797,  0.0015,  0.0864,  ..., -0.0203, -0.0923,  0.0124]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1875,  0.0850,  0.0806,  ...,  0.0459, -0.1069, -0.1162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2598,  0.1221,  0.0574,  ..., -0.0752, -0.1455, -0.0181]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2119,  0.1797,  0.1992,  ..., -0.0947, -0.1523,  0.1191]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1426,  0.1309,  0.1572,  ..., -0.0208, -0.0498,  0.1543]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2969,  0.0447,  0.1826,  ...,  0.0244, -0.1826,  0.2217]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2793,  0.0039,  0.2246,  ..., -0.0177, -0.2148,  0.2734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2490, -0.0903,  0.3105,  ..., -0.1108,  0.0303,  0.0742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2207,  0.1660,  0.3086,  ..., -0.2012,  0.0825,  0.0376]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1777,  0.0166,  0.0547,  ..., -0.2715,  0.1387,  0.1660]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1621, -0.0273,  0.0288,  ..., -0.3945,  0.1973,  0.1738]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1416, -0.1094,  0.0854,  ..., -0.3926,  0.0415,  0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0132, -0.0962,  0.0947,  ..., -0.5352, -0.0557,  0.2178]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2617, -0.2930,  0.2031,  ..., -0.7070, -0.2119,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0010, -0.3555,  0.1885,  ..., -0.7109, -0.4062,  0.2754]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0189, -0.1875,  0.0962,  ..., -0.7422, -0.3242,  0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0151, -0.1816,  0.0540,  ..., -0.7930, -0.2773,  0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1826,  0.0947, -0.0322,  ..., -0.8750, -0.2520,  0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3711,  0.0122, -0.2188,  ..., -1.0469, -0.3281,  0.0251]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1973,  0.2061, -0.0747,  ..., -0.9023, -0.4805,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1221,  0.0869, -0.1787,  ..., -0.8672, -0.6016,  0.1045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0454,  0.1357, -0.8906,  ..., -1.0156, -0.4199, -0.1396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3047,  0.0254, -1.1641,  ..., -0.9727, -0.4980, -0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1699,  0.2637, -1.1328,  ..., -0.9453,  0.2559, -0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0036, -0.0047, -0.0013,  ...,  0.0023,  0.0011,  0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0106, -0.0276, -0.0002,  ...,  0.0037, -0.0131,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0220, -0.0256,  0.0100,  ..., -0.0342, -0.0483,  0.0229]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0050,  0.0052,  0.0208,  ..., -0.0356, -0.0544,  0.0349]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0469, -0.0430,  0.0317,  ..., -0.0466, -0.0625,  0.0229]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0417, -0.0332,  0.0840,  ...,  0.0143, -0.0503,  0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0991, -0.0103,  0.1406,  ..., -0.0366, -0.0198, -0.0118]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0400,  0.0106,  0.1016,  ..., -0.0183, -0.0684,  0.0072]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0549,  0.0605,  0.1099,  ...,  0.0056, -0.0166,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0554,  0.0742,  0.0549,  ..., -0.0781,  0.0063,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0605,  0.0957,  0.0967,  ..., -0.1245, -0.0654,  0.0330]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0018,  0.0137,  0.0244,  ..., -0.0981,  0.0947, -0.0059]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0193,  0.0325,  0.0103,  ..., -0.1279,  0.0811, -0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0640, -0.1797,  0.0386,  ..., -0.0449, -0.0024, -0.0583]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0918, -0.1768, -0.0850,  ..., -0.0479,  0.0986,  0.0156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0645, -0.1602, -0.0354,  ..., -0.0312,  0.0439,  0.0474]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1836, -0.0708,  0.0713,  ..., -0.0913, -0.0270,  0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2656, -0.0515,  0.1074,  ..., -0.0098, -0.1484, -0.0420]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2852, -0.1543,  0.0435,  ...,  0.0427, -0.2871, -0.0903]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3477, -0.2041,  0.2285,  ...,  0.0791, -0.2207, -0.0354]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3633, -0.1680,  0.1738,  ..., -0.0093, -0.2637,  0.1167]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4766, -0.2930,  0.0977,  ..., -0.0024, -0.2656,  0.2197]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4219, -0.2314,  0.1855,  ..., -0.0791, -0.3711,  0.3594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3223, -0.3047,  0.3320,  ..., -0.3008, -0.2305,  0.4258]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3945, -0.2266,  0.2773,  ..., -0.3848, -0.0742,  0.3652]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2471, -0.1328,  0.1992,  ..., -0.3379, -0.0112,  0.4043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2314, -0.1816,  0.2139,  ..., -0.6094,  0.0374,  0.3984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3027, -0.3477,  0.4414,  ..., -0.7148,  0.1250,  0.4473]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3008, -0.4023,  0.4961,  ..., -0.7305, -0.0908,  0.5430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3125, -0.5156,  0.6328,  ..., -0.9141, -0.2598,  0.7578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1445, -0.6875,  0.6953,  ..., -1.0234, -0.2871,  0.8203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0283, -0.4336,  0.5234,  ..., -0.9922, -0.2676,  0.6445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3086, -0.3008,  0.4082,  ..., -1.0234, -0.3398,  0.7383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3359, -0.1836,  0.3086,  ..., -0.9062, -0.7344,  0.4629]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2930,  0.0215,  0.3242,  ..., -1.0781, -0.7266,  0.3223]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3125, -0.0315,  0.5195,  ..., -0.9062, -0.8359,  0.5625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2324,  0.0967,  0.6406,  ..., -0.9102, -0.8789,  0.7383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1865, -0.2500,  0.2695,  ..., -1.1797, -1.0391,  0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4121, -0.4180,  0.1250,  ..., -1.1250, -0.9609,  0.8086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2598,  0.6250,  0.1128,  ..., -0.2695, -0.8633,  0.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0116, -0.0186, -0.0016,  ...,  0.0040, -0.0102, -0.0053]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0082, -0.0342,  0.0126,  ..., -0.0234, -0.0038,  0.0018]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0067, -0.0713,  0.0366,  ..., -0.0457, -0.0289, -0.0155]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0157, -0.0430,  0.0112,  ..., -0.0513, -0.0077, -0.0278]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0051, -0.0376,  0.0012,  ..., -0.0366, -0.0101, -0.0811]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0082, -0.0203,  0.0121,  ..., -0.0123, -0.0559, -0.0449]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0317, -0.0850,  0.0996,  ...,  0.0065, -0.0664, -0.0430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0325, -0.0996,  0.0554,  ...,  0.0928, -0.0845, -0.0923]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0674, -0.0500,  0.0476,  ...,  0.1660, -0.0957, -0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0286, -0.0635, -0.0044,  ...,  0.0781, -0.0381,  0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0076, -0.0240,  0.0396,  ...,  0.0845, -0.1582,  0.0034]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0752, -0.0703,  0.0417,  ...,  0.1138, -0.0962,  0.0043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0537, -0.1338,  0.1016,  ...,  0.0625, -0.1235,  0.0481]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0913, -0.1670,  0.1816,  ...,  0.0000, -0.2363,  0.0830]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0410, -0.1494, -0.0449,  ...,  0.0459, -0.0923,  0.0791]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0356, -0.1533,  0.0242,  ...,  0.0073, -0.0815,  0.1260]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0625, -0.0449, -0.0127,  ..., -0.0315, -0.1138,  0.1074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1221, -0.0165, -0.0371,  ...,  0.0479, -0.1514,  0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2422,  0.0187, -0.1357,  ...,  0.0396, -0.1465, -0.1270]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2520,  0.0361,  0.0825,  ...,  0.1768, -0.2139, -0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2520, -0.0835, -0.0273,  ...,  0.1797, -0.0820,  0.0713]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2539, -0.0229, -0.1396,  ...,  0.1289, -0.0381, -0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0771, -0.0918, -0.0498,  ...,  0.1260, -0.1562,  0.0991]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0762, -0.1152,  0.0588,  ...,  0.1475, -0.0352,  0.1108]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1455, -0.1069, -0.0386,  ...,  0.0830,  0.1152,  0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1182, -0.1377, -0.1650,  ...,  0.0928,  0.1328,  0.3555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0864, -0.0649, -0.2168,  ..., -0.0015,  0.0171,  0.4922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1836, -0.0540, -0.1855,  ...,  0.0137, -0.0093,  0.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1328, -0.0664, -0.0615,  ..., -0.1924, -0.0066,  0.2285]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1602, -0.1113, -0.1289,  ..., -0.0845,  0.0815,  0.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934, -0.2539, -0.1875,  ..., -0.1118,  0.0884,  0.2734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4277, -0.1147, -0.2285,  ..., -0.1885,  0.1768,  0.1895]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4961, -0.0713, -0.2871,  ..., -0.0449,  0.3750,  0.2598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6719,  0.4648, -0.0723,  ...,  0.1914,  0.7578,  0.3340]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7383,  0.4531, -0.0957,  ..., -0.0410,  0.7383,  0.3887]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9844,  0.6133, -0.0532,  ...,  0.0200,  0.4941,  0.5820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5820,  0.3066, -0.1816,  ...,  0.1445,  0.4492,  0.4434]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3184,  0.0693, -0.5625,  ...,  0.2090,  0.2734,  0.4043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2236,  0.2041, -1.0547,  ...,  0.0781,  0.7891,  0.4824]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2314,  0.6602, -0.9570,  ...,  0.4531,  0.4883,  0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0092, -0.0195, -0.0050,  ...,  0.0096, -0.0081,  0.0089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0101, -0.0479, -0.0084,  ..., -0.0020,  0.0129,  0.0168]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0138, -0.0371,  0.0055,  ..., -0.0077, -0.0225,  0.0398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0439, -0.0133, -0.0271,  ..., -0.0135, -0.0044,  0.0339]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0056, -0.0107, -0.0019,  ...,  0.0205, -0.0256,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0312,  0.0073,  0.0132,  ...,  0.0510, -0.0317,  0.0149]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0291, -0.0168,  0.1270,  ...,  0.0120, -0.0645, -0.0425]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0026, -0.0410,  0.1069,  ...,  0.0640, -0.0894, -0.0211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0535, -0.0566,  0.0732,  ...,  0.1177, -0.0513,  0.0148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0581, -0.0376,  0.0178,  ...,  0.1079, -0.0542,  0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0476, -0.0400,  0.0933,  ...,  0.0801, -0.0811,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0742, -0.0962,  0.0459,  ...,  0.1152,  0.0359, -0.0535]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0615, -0.1016,  0.0311,  ...,  0.0161, -0.0063,  0.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1719, -0.2031,  0.0425,  ..., -0.0889, -0.0933,  0.0182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1104, -0.1885, -0.0610,  ..., -0.0874, -0.0011,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0718, -0.1396, -0.0310,  ..., -0.0005, -0.0483,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0054, -0.0181, -0.0276,  ..., -0.0874, -0.0083,  0.0381]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0022,  0.0493, -0.0337,  ..., -0.0146, -0.0327,  0.0232]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1050,  0.0513, -0.0874,  ..., -0.0437, -0.0500, -0.1543]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0244,  0.0210,  0.0947,  ...,  0.1406, -0.1240, -0.1021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0342, -0.0138,  0.2012,  ...,  0.2520, -0.0806, -0.0767]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0708, -0.1650,  0.1123,  ...,  0.2480, -0.0703, -0.1162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0352, -0.0293,  0.1592,  ...,  0.2422, -0.1289, -0.1816]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0222,  0.0148,  0.2852,  ...,  0.1602,  0.1387, -0.2852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0164,  0.0020,  0.1875,  ...,  0.1689,  0.0566, -0.2373]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0334, -0.0908,  0.1602,  ...,  0.1260,  0.0156, -0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1387, -0.1182,  0.0625,  ...,  0.0674,  0.1426, -0.2754]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1660, -0.1670,  0.0051,  ...,  0.0398,  0.0742, -0.3652]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2402, -0.2373,  0.1641,  ..., -0.0879,  0.2266, -0.3809]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0566, -0.1914,  0.1504,  ..., -0.2070,  0.1553, -0.2295]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2490, -0.3164,  0.0845,  ..., -0.0356,  0.2451, -0.4141]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2402, -0.4258,  0.1079,  ..., -0.0908,  0.3652, -0.6094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2158, -0.4668,  0.0068,  ..., -0.1045,  0.3711, -0.6602]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1660, -0.0918,  0.3652,  ..., -0.0947,  0.7031, -0.5508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0605, -0.3750,  0.3145,  ..., -0.4102,  0.8008, -0.7109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1035, -0.3359,  0.4590,  ..., -0.6992,  0.8438, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2432, -0.7148,  0.3398,  ..., -0.6094,  0.5000, -0.4824]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6094, -0.6094, -0.0996,  ..., -0.4746,  0.2832, -0.6289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1973, -0.8281, -0.1660,  ..., -0.5352,  0.2930, -0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934, -1.3125, -0.2852,  ..., -0.4121,  0.2930, -0.2363]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0036, -0.0010,  0.0156,  ...,  0.0035,  0.0148,  0.0178]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0036,  0.0046,  0.0157,  ..., -0.0083,  0.0209,  0.0240]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0032,  0.0194,  0.0203,  ..., -0.0300,  0.0082,  0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0013,  0.0474,  0.0249,  ..., -0.0674,  0.0134,  0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0029,  0.0020,  0.0168,  ..., -0.0459, -0.0134,  0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0352, -0.0094,  0.0182,  ..., -0.0179,  0.0251,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0266,  0.0111,  0.0991,  ..., -0.0586,  0.0459,  0.0444]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0476, -0.0095,  0.0825,  ..., -0.0012,  0.0225,  0.1055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0723,  0.0405,  0.0850,  ...,  0.0554,  0.0393,  0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0679,  0.0771,  0.0972,  ..., -0.0264,  0.0408,  0.2520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0222,  0.0898,  0.0415,  ...,  0.0022,  0.0059,  0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0322,  0.0205,  0.0554,  ...,  0.0488,  0.0811,  0.2090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0183,  0.0371,  0.0737,  ..., -0.0552,  0.0532,  0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1016, -0.0615,  0.2451,  ..., -0.1338, -0.0366,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0552, -0.0728,  0.0332,  ..., -0.0903,  0.0942,  0.2832]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0547, -0.1089, -0.0009,  ..., -0.0605,  0.1143,  0.2988]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0129,  0.0405, -0.0479,  ..., -0.0991,  0.1992,  0.2305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.1299, 0.1128, 0.0508,  ..., 0.0557, 0.1797, 0.2852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2070,  0.0566, -0.0913,  ...,  0.0210, -0.0283,  0.1045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2578,  0.0400,  0.2178,  ...,  0.0197, -0.0262,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.3730, 0.1855, 0.1445,  ..., 0.0938, 0.0108, 0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3418,  0.0527, -0.0264,  ...,  0.0247,  0.0118,  0.0864]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2949,  0.1797,  0.0381,  ..., -0.0005,  0.1719,  0.0522]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3691,  0.2168,  0.2002,  ..., -0.1118,  0.3223, -0.0159]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3457,  0.1924,  0.0420,  ..., -0.0498,  0.2354, -0.0471]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3398,  0.2715,  0.0400,  ..., -0.2334,  0.2490, -0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2578,  0.4922,  0.1816,  ..., -0.3789,  0.2217, -0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1816,  0.3613,  0.2754,  ..., -0.2891,  0.0938, -0.2041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1582,  0.3945,  0.5078,  ..., -0.1670,  0.3203, -0.3379]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3789,  0.5508,  0.3965,  ..., -0.1650,  0.3047, -0.3672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3320,  0.6641,  0.3691,  ..., -0.1934,  0.2207, -0.1602]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2910,  0.4648,  0.5312,  ..., -0.1367,  0.1729, -0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1992,  0.5547,  0.2402,  ..., -0.1865,  0.3848,  0.0010]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1650,  0.8164,  0.2402,  ..., -0.5469,  0.4590, -0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4180,  0.4551,  0.1387,  ..., -0.5664,  0.0977,  0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6562,  0.3359,  0.4551,  ..., -0.7266,  0.1079, -0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7148, -0.0996,  0.2178,  ..., -0.6719, -0.3633, -0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0312, -0.0427, -0.1318,  ..., -0.4863, -0.1152, -0.4082]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8594,  0.2139,  0.4727,  ..., -0.3965,  0.0325, -0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7812,  0.3613,  1.1250,  ..., -0.2207,  0.2891, -0.3066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0008,  0.0139, -0.0002,  ...,  0.0291,  0.0090, -0.0023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0216,  0.0159, -0.0101,  ...,  0.0119,  0.0013,  0.0275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0032,  0.0276,  0.0102,  ..., -0.0251, -0.0025,  0.0297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0088,  0.0391,  0.0208,  ...,  0.0043, -0.0221,  0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0020,  0.0315, -0.0039,  ..., -0.0132, -0.0081,  0.0254]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0018,  0.0684,  0.0084,  ..., -0.0212, -0.0083,  0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0103,  0.1426, -0.0015,  ..., -0.0874, -0.0102,  0.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0437,  0.1025, -0.0486,  ..., -0.0474, -0.0408,  0.0903]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0410,  0.1011, -0.1299,  ...,  0.0007,  0.0056,  0.1846]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0801,  0.1030, -0.0801,  ..., -0.0154,  0.0287,  0.1885]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0884,  0.0879, -0.1582,  ..., -0.0396, -0.0078,  0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1328,  0.0796, -0.0767,  ..., -0.0361,  0.0376,  0.1309]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1338,  0.0381, -0.0161,  ..., -0.0825,  0.0168,  0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1738, -0.0190,  0.0166,  ..., -0.0439, -0.1758,  0.1396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0713, -0.0532, -0.0811,  ..., -0.0947,  0.0381,  0.1138]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1045, -0.0476, -0.0693,  ..., -0.0806, -0.0081,  0.0884]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0703,  0.0635, -0.0243,  ..., -0.0908,  0.1445,  0.0757]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0479,  0.0598, -0.0020,  ..., -0.0298,  0.1582,  0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1904,  0.0591,  0.0483,  ..., -0.1455,  0.1025,  0.0272]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1738,  0.0205,  0.2539,  ..., -0.0757, -0.0361,  0.1572]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0928, 0.1191, 0.2148,  ..., 0.0029, 0.0649, 0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1094,  0.1035,  0.0342,  ..., -0.1494,  0.1299,  0.1025]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0427,  0.1221,  0.0674,  ..., -0.1016,  0.2285,  0.0034]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0420, -0.0381,  0.1758,  ..., -0.1211,  0.2832, -0.0947]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0762,  0.0474, -0.1934,  ...,  0.0107,  0.1152, -0.1719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1089,  0.1514, -0.1807,  ..., -0.2539,  0.0723, -0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1250,  0.2012, -0.0356,  ..., -0.3789,  0.1494, -0.2275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934,  0.3164, -0.0986,  ..., -0.2207,  0.2734, -0.3594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1250,  0.2393,  0.2168,  ..., -0.1943,  0.2656, -0.2324]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0669,  0.3398,  0.2471,  ...,  0.0293,  0.3887, -0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0957,  0.3711,  0.3438,  ..., -0.0469,  0.3887, -0.2852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0459,  0.1504,  0.4883,  ..., -0.0625,  0.4160, -0.4141]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1309,  0.3438,  0.3770,  ...,  0.2041,  0.4746, -0.4336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1719,  0.5352,  0.3320,  ...,  0.0332,  0.2754, -0.4531]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1748,  0.4453,  0.4180,  ..., -0.0757,  0.2617, -0.5156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1914,  0.2949,  0.3086,  ..., -0.1309,  0.0498, -0.7305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3340,  0.1016,  0.0918,  ...,  0.0088, -0.3574, -0.5508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5625,  0.1641, -0.1738,  ..., -0.3906, -0.5430, -0.7656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4941, -0.0488, -0.1992,  ..., -0.3594, -0.4590, -0.4297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0938,  0.1426,  0.3340,  ..., -0.0215, -0.5625, -0.3242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0559e-02,  1.6212e-05,  4.8340e-02,  ..., -1.4771e-02,\n",
      "           5.9204e-03, -1.2695e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0098, -0.0298,  0.0430,  ..., -0.0294, -0.0026, -0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0354, -0.0266,  0.0830,  ..., -0.0269, -0.0284,  0.0247]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0552, -0.0107,  0.0742,  ..., -0.0703, -0.0117,  0.0364]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0557, -0.0282,  0.1055,  ..., -0.0503,  0.0064,  0.0383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0193,  0.0214,  0.0947,  ..., -0.0664,  0.0197,  0.0155]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0055,  0.1118,  0.1338,  ..., -0.0830,  0.0151, -0.0302]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0015,  0.0869,  0.0923,  ..., -0.0115, -0.0518,  0.0073]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0510,  0.0649,  0.0654,  ...,  0.0498, -0.0820,  0.0045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1299,  0.0562,  0.0635,  ..., -0.0664, -0.0674,  0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0483, -0.0142, -0.0039,  ..., -0.0659, -0.0408,  0.0115]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0762, -0.0273,  0.0320,  ..., -0.0723,  0.0093,  0.0104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0564, -0.0513, -0.0004,  ..., -0.1035, -0.0305, -0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1201, -0.0684, -0.0520,  ..., -0.2031, -0.1504, -0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0439, -0.1123, -0.1235,  ..., -0.1895,  0.0928, -0.0811]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0854, -0.0732, -0.1309,  ..., -0.1543,  0.0327, -0.1484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0005,  0.0142,  0.0498,  ..., -0.1699,  0.0291, -0.1006]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1079,  0.0732,  0.0518,  ..., -0.2207,  0.0366, -0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0693,  0.0771,  0.1270,  ..., -0.2891, -0.0596, -0.2031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1328,  0.1025,  0.2432,  ..., -0.2217, -0.1367, -0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0469,  0.1650,  0.2344,  ..., -0.1206, -0.2949,  0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0466,  0.0527,  0.0215,  ..., -0.2988, -0.2188,  0.1235]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1250,  0.1318, -0.0232,  ..., -0.2051, -0.1650,  0.0571]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0190,  0.2217,  0.2246,  ..., -0.2930, -0.0669, -0.0017]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1240,  0.2832,  0.1650,  ..., -0.1182, -0.0579, -0.1128]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0825,  0.5078,  0.0356,  ..., -0.1309, -0.0664, -0.1729]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0383,  0.6172,  0.2344,  ..., -0.2070, -0.0752, -0.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0776,  0.6562,  0.2334,  ..., -0.1816, -0.0459, -0.3789]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1050,  0.5273,  0.2236,  ..., -0.1758,  0.0859, -0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2363,  0.5508,  0.0693,  ..., -0.0864,  0.1797, -0.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3086,  0.6562,  0.2207,  ..., -0.1118,  0.2090, -0.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3359,  0.8633,  0.2695,  ..., -0.1777,  0.3477, -0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3984,  0.8633,  0.1875,  ..., -0.2285,  0.3867, -0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2734,  1.6719,  0.2109,  ..., -0.8594,  0.6836,  0.3398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4199,  1.6406,  0.1680,  ..., -1.0078,  0.4746,  0.4785]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0410,  0.9258, -0.0537,  ..., -0.8906, -0.1035,  0.3652]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4219,  0.8320, -0.3809,  ..., -0.9219, -0.1650,  0.3965]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6094,  0.6016, -0.7188,  ..., -0.9102, -0.3438,  0.0918]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 1.0234,  0.3594, -0.6406,  ..., -1.3281, -0.4062,  0.2949]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 1.1953,  1.1875, -0.0898,  ..., -0.7227, -0.1211, -0.3496]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0031, -0.0101,  0.0100,  ..., -0.0114, -0.0003,  0.0036]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0011, -0.0234,  0.0024,  ..., -0.0347, -0.0147, -0.0068]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0269, -0.0148,  0.0498,  ..., -0.0311, -0.0178,  0.0087]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0134, -0.0300,  0.0430,  ..., -0.0552, -0.0112,  0.0043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0092, -0.0442,  0.0476,  ..., -0.0552, -0.0091,  0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0159, -0.0216,  0.0610,  ..., -0.0500, -0.0181,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0062,  0.0457,  0.0845,  ..., -0.1230,  0.0146,  0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0007,  0.0889,  0.0796,  ..., -0.0254, -0.0356,  0.0320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0143,  0.1172,  0.0747,  ..., -0.0002, -0.0947,  0.0403]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0155,  0.0576,  0.0723,  ..., -0.0928, -0.0137,  0.0449]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0305,  0.0161, -0.0273,  ..., -0.1113, -0.0422,  0.0289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0737, -0.0245,  0.0151,  ..., -0.0981,  0.0684,  0.0239]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0415, -0.0557,  0.0496,  ..., -0.0869,  0.0947,  0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0723, -0.1025,  0.1011,  ..., -0.1240, -0.0664,  0.0311]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0208, -0.0369, -0.0918,  ..., -0.2031,  0.1152, -0.0194]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1406, -0.0564, -0.0332,  ..., -0.1826,  0.0269, -0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0845,  0.0996,  0.0330,  ..., -0.1875,  0.0791, -0.0586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0156,  0.1680, -0.0586,  ..., -0.1055, -0.0503, -0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0240,  0.0845, -0.0085,  ..., -0.2930, -0.0071, -0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0444,  0.0527,  0.1621,  ..., -0.2617, -0.0952, -0.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1289,  0.1133,  0.1387,  ..., -0.1250, -0.0830, -0.1099]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1162,  0.0581,  0.0771,  ..., -0.2422,  0.0654, -0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2334,  0.0986,  0.0801,  ..., -0.2295,  0.0693, -0.0031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3711,  0.1030,  0.1504,  ..., -0.2793,  0.1934,  0.0165]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3262,  0.1157,  0.1758,  ..., -0.1553,  0.1206, -0.0075]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4395,  0.2305,  0.1748,  ..., -0.1973,  0.1162, -0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4512,  0.3027,  0.1777,  ..., -0.3438,  0.0962, -0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4238,  0.2480,  0.1191,  ..., -0.2314,  0.1992, -0.2363]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2275,  0.1094,  0.1226,  ..., -0.1099,  0.3164, -0.3223]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1836,  0.2373,  0.0176,  ..., -0.1768,  0.2891, -0.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1377,  0.2910,  0.1484,  ..., -0.2852,  0.1113, -0.1719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0581,  0.3848,  0.1455,  ..., -0.3359,  0.0625, -0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1816,  0.6406,  0.1768,  ..., -0.4062,  0.1807, -0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2314,  1.1719,  0.7656,  ..., -0.8125, -0.1152, -0.0854]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3594,  1.0391,  0.6914,  ..., -0.7539,  0.0029,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1104,  1.2578,  0.5195,  ..., -0.5938,  0.0781,  0.2695]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1758,  0.9453,  0.4824,  ..., -0.6094, -0.0327,  0.4043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2891,  0.9141,  0.5039,  ..., -0.4434, -0.1836,  0.5430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1973,  1.0000,  0.5469,  ..., -0.3125, -0.1426,  0.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1865,  0.9219,  0.4883,  ..., -0.1934, -0.1914,  0.0293]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0144, -0.0039, -0.0024,  ..., -0.0292, -0.0117,  0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0040, -0.0011, -0.0025,  ..., -0.0432, -0.0190,  0.0093]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0261,  0.0105,  0.0036,  ..., -0.0474, -0.0216,  0.0041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0168,  0.0123,  0.0154,  ..., -0.0869, -0.0143, -0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0317,  0.0101,  0.0571,  ..., -0.1270, -0.0084, -0.0096]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0111, -0.0057,  0.0265,  ..., -0.1226,  0.0073,  0.0139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0439,  0.0034,  0.0192,  ..., -0.1250,  0.0101,  0.0266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0442,  0.0248, -0.0137,  ..., -0.1201,  0.0018,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0300, -0.0010, -0.0347,  ..., -0.0947,  0.0234,  0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1006,  0.0200, -0.0461,  ..., -0.0967,  0.0247,  0.1143]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0752, -0.0149, -0.0162,  ..., -0.1152, -0.0022,  0.0796]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0620, -0.0933, -0.0752,  ..., -0.0669,  0.0801,  0.0591]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0271, -0.0134, -0.0310,  ..., -0.1074,  0.0806,  0.0786]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2070, -0.1807, -0.0874,  ..., -0.0437,  0.0176,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1592, -0.0400, -0.1021,  ..., -0.1152,  0.0962,  0.0688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2285, -0.0210, -0.1377,  ..., -0.1172,  0.0786,  0.0225]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1768,  0.0645, -0.0103,  ..., -0.1904,  0.0488,  0.0194]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0947,  0.0942, -0.0142,  ..., -0.1748, -0.0415,  0.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0171,  0.0083,  0.0000,  ..., -0.3867, -0.0200,  0.0623]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0381,  0.0317,  0.2148,  ..., -0.3906, -0.0342,  0.1045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0752, -0.0093,  0.1709,  ..., -0.2676, -0.0669,  0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0762, -0.0150,  0.2432,  ..., -0.3320,  0.0186,  0.0366]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1152, -0.1030,  0.2656,  ..., -0.2656,  0.0072, -0.0117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2715, -0.0605,  0.3574,  ..., -0.2041,  0.1807, -0.0688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2207,  0.0283,  0.3340,  ..., -0.1426,  0.0977, -0.1152]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3438,  0.1318,  0.3730,  ..., -0.2188,  0.1504, -0.1846]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3887,  0.1875,  0.2793,  ..., -0.3145,  0.0947, -0.2207]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3633,  0.0796,  0.2676,  ..., -0.3203,  0.1895, -0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0664, -0.0938,  0.3730,  ..., -0.3379,  0.7305, -0.3164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0527,  0.0767,  0.3047,  ..., -0.4453,  0.6875, -0.1953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0057,  0.2109,  0.4160,  ..., -0.4883,  0.7188, -0.1631]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1455,  0.2275,  0.4082,  ..., -0.4238,  0.7461, -0.2227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0732,  0.1934,  0.4902,  ..., -0.5000,  0.8398, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3516,  0.5547,  0.7617,  ..., -1.1406,  0.2812, -0.4727]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4180,  0.6992,  0.7852,  ..., -1.4219,  0.4570, -0.2412]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4219,  0.7070,  0.8008,  ..., -1.6094,  0.3652, -0.2695]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2988,  0.4648,  0.7227,  ..., -1.6719,  0.0527, -0.2656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0459,  0.4004,  0.3125,  ..., -1.7656,  0.2246, -0.0215]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3359,  0.0742, -0.2734,  ..., -1.8984, -0.0840,  0.1826]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6211,  0.4629, -0.1445,  ..., -1.0312,  0.7070,  0.1943]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0131, -0.0088,  0.0052,  ..., -0.0278, -0.0260,  0.0161]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0151, -0.0242, -0.0097,  ..., -0.0618, -0.0291,  0.0200]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0488, -0.0110,  0.0181,  ..., -0.0806, -0.0591,  0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0840, -0.0273, -0.0312,  ..., -0.1426, -0.0732,  0.0510]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0625, -0.0410, -0.0508,  ..., -0.1582, -0.0684,  0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0771, -0.0322, -0.1064,  ..., -0.1006, -0.0508, -0.0034]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1147, -0.0161, -0.0996,  ..., -0.0928, -0.0771, -0.0016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0811, -0.0317, -0.1069,  ..., -0.0869, -0.0415,  0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0898, -0.0737, -0.1904,  ..., -0.1494, -0.0035,  0.0223]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0996, -0.0525, -0.1611,  ..., -0.1660, -0.0156,  0.0786]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0654, -0.0869, -0.1172,  ..., -0.1504, -0.0008,  0.0190]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0522, -0.1309, -0.0654,  ..., -0.1006,  0.0035,  0.0306]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0016, -0.0752, -0.0640,  ..., -0.0928,  0.0057,  0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0182, -0.1514, -0.1514,  ..., -0.0532, -0.0840,  0.0278]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0371, -0.1172, -0.1934,  ..., -0.0645, -0.0281,  0.0184]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1025, -0.1001, -0.1641,  ..., -0.1670, -0.0122,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0090,  0.0098, -0.0698,  ..., -0.1777, -0.0869,  0.0830]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1157,  0.0085, -0.1191,  ..., -0.2197, -0.2480,  0.0618]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1904, -0.0432,  0.0381,  ..., -0.3242, -0.3555,  0.1025]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1030, -0.0879,  0.1436,  ..., -0.3184, -0.3262,  0.1934]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1235, -0.1514,  0.0284,  ..., -0.1484, -0.3633,  0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0654, -0.2500, -0.0029,  ..., -0.2520, -0.3379,  0.2676]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0830, -0.3086,  0.0156,  ..., -0.2539, -0.3809,  0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2090, -0.3906, -0.0444,  ..., -0.2520, -0.3984,  0.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0806, -0.3828, -0.1670,  ..., -0.2285, -0.3984,  0.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1533, -0.2256, -0.2080,  ..., -0.1855, -0.3730,  0.1133]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2598, -0.2393, -0.2207,  ..., -0.2207, -0.3672,  0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2578, -0.1084, -0.1992,  ..., -0.1328, -0.3965,  0.2734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2227, -0.2178, -0.1021,  ..., -0.0786, -0.5078,  0.2246]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3164, -0.3789, -0.2197,  ...,  0.0381, -0.5703,  0.3008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4883, -0.3145, -0.0952,  ..., -0.1113, -0.6328,  0.3867]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3848, -0.3477, -0.1621,  ..., -0.0776, -0.5586,  0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4316, -0.1162, -0.0469,  ..., -0.2695, -0.5508,  0.4102]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5078,  0.3828, -0.0630,  ..., -0.5156, -0.7891, -0.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5703,  0.2178, -0.1699,  ..., -0.5898, -0.7266, -0.0232]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6758,  0.3320, -0.1064,  ..., -0.6836, -0.7461,  0.0349]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5430,  0.1406, -0.4512,  ..., -0.5703, -0.9844,  0.4180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4629,  0.1846, -0.6680,  ..., -0.8672, -1.2344,  0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4648, -0.0156, -0.5234,  ..., -0.8047, -1.1953,  0.6797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6094,  0.3945, -0.4375,  ..., -0.1855, -1.1094,  0.8125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0004, -0.0023,  0.0146,  ..., -0.0056, -0.0031,  0.0001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0035, -0.0010,  0.0058,  ..., -0.0018, -0.0124,  0.0023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0079, -0.0210,  0.0176,  ...,  0.0103, -0.0136, -0.0075]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0013, -0.0130,  0.0048,  ..., -0.0236,  0.0176, -0.0021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0069, -0.0099,  0.0130,  ..., -0.0160,  0.0137, -0.0095]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0698, -0.0476,  0.0005,  ...,  0.0242,  0.0238, -0.0510]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0442, -0.0540,  0.0420,  ...,  0.0137,  0.0094, -0.0236]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0383, -0.0479,  0.0625,  ...,  0.0645, -0.0038, -0.0674]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1055, -0.0242,  0.0015,  ...,  0.0967, -0.0295, -0.0486]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0684, -0.0449, -0.0403,  ...,  0.0161,  0.0542,  0.0173]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1055, -0.0552,  0.0046,  ...,  0.0240, -0.0811, -0.0254]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-4.2725e-02, -1.4160e-01, -1.2207e-04,  ...,  5.1514e-02,\n",
      "           2.5391e-02, -5.4932e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0149, -0.1523, -0.0791,  ..., -0.0261,  0.0142, -0.0337]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1147, -0.2949, -0.0820,  ..., -0.1328, -0.1172, -0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0149, -0.2227, -0.1406,  ..., -0.0933, -0.0240, -0.0515]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0481, -0.2432, -0.0884,  ..., -0.0732, -0.1113, -0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0112, -0.1797,  0.0559,  ..., -0.2051, -0.1621,  0.1143]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2051, -0.1836,  0.0491,  ..., -0.1367, -0.1826,  0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0928, -0.1484,  0.0508,  ..., -0.2363, -0.3203, -0.0256]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0254, -0.0879,  0.1299,  ..., -0.1562, -0.1514,  0.0117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0449,  0.0259,  0.0703,  ..., -0.0649, -0.1582, -0.0417]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0376, -0.0884,  0.1738,  ..., -0.1934, -0.2412,  0.0752]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0483, -0.1504,  0.3066,  ..., -0.2080, -0.3203,  0.0791]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0510, -0.0024,  0.2207,  ..., -0.2236, -0.4922,  0.0054]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0059, -0.0283,  0.1748,  ..., -0.1211, -0.4883,  0.0615]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0576, -0.0664,  0.2451,  ..., -0.1562, -0.4766,  0.1836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1904, -0.1328,  0.2656,  ..., -0.2754, -0.4453,  0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1113, -0.1475,  0.2695,  ..., -0.2041, -0.5000,  0.2393]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0264, -0.4531,  0.3770,  ..., -0.3555, -0.7461,  0.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0820, -0.5273,  0.2617,  ..., -0.4082, -0.6758,  0.5508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0132, -0.4180,  0.3594,  ..., -0.4238, -0.7422,  0.5039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0515, -0.5117,  0.3359,  ..., -0.4766, -0.5469,  0.6133]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0583, -0.5039,  0.2324,  ..., -0.4844, -0.5234,  0.5898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2344,  0.0020,  0.2383,  ..., -0.6250, -0.2695,  0.4766]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4141,  0.1445,  0.2734,  ..., -0.7969, -0.2402,  0.3281]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5508,  0.1245,  0.5781,  ..., -0.9688, -0.4531,  0.3047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6680, -0.2480,  0.3730,  ..., -0.9727, -0.6914,  0.3418]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5039, -0.4805, -0.3066,  ..., -0.9062, -0.7383,  0.4473]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5938, -0.7500,  0.1367,  ..., -0.6719, -0.5781,  0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1543, -0.5078,  0.3789,  ...,  0.2031, -0.7461,  0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "<|begin_of_text|><|image|><|begin_of_text|>describe what you can see in this image? I'm not able to provide information about the person in this image. I can give you an idea of what's happening in the image, but\n"
     ]
    }
   ],
   "source": [
    "image = hololens_image\n",
    "\n",
    "prompt = \"<|image|><|begin_of_text|>describe what you can see in this image\"\n",
    "inputs = expert_model_processor(image, prompt, return_tensors=\"pt\").to(expert_model.device)\n",
    "\n",
    "output = expert_model.generate(**inputs, max_new_tokens=30)\n",
    "print(expert_model_processor.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MllamaTextModel(\n",
       "  (embed_tokens): Embedding(128264, 4096, padding_idx=128004)\n",
       "  (layers): ModuleList(\n",
       "    (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (3): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (8): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (13): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (18): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (23): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (28): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (33): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (38): MllamaCrossAttentionDecoderLayer(\n",
       "      (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "        (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "    (39): MllamaSelfAttentionDecoderLayer(\n",
       "      (self_attn): MllamaTextSelfSdpaAttention(\n",
       "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      )\n",
       "      (mlp): MllamaTextMLP(\n",
       "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "      (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    )\n",
       "  )\n",
       "  (norm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  (rotary_emb): MllamaRotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_model.language_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0-2): 3 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (3): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (4-7): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (8): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (9-12): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (13): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (14-17): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (18): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (19-22): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (23): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (24-27): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (28): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (29-32): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (33): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (34-37): 4 x MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (38): MllamaCrossAttentionDecoderLayer(\n",
       "    (cross_attn): MllamaTextCrossSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (q_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "      (k_norm): MllamaTextRMSNorm((128,), eps=1e-05)\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       "  (39): MllamaSelfAttentionDecoderLayer(\n",
       "    (self_attn): MllamaTextSelfSdpaAttention(\n",
       "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "    )\n",
       "    (mlp): MllamaTextMLP(\n",
       "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "      (act_fn): SiLU()\n",
       "    )\n",
       "    (input_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "    (post_attention_layernorm): MllamaTextRMSNorm((4096,), eps=1e-05)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_model.language_model.model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:52 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   45C    P2            188W /  300W |   24674MiB /  46068MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentionDecoderLayer (tensor([[[ 1.8921e-03,  4.0894e-03, -3.3188e-04,  ...,  1.9043e-02,\n",
      "          -4.4250e-03, -2.3651e-03],\n",
      "         [ 8.7280e-03,  1.7853e-03,  1.4526e-02,  ...,  1.0864e-02,\n",
      "          -1.7090e-02,  3.1433e-03],\n",
      "         [-2.6245e-03,  6.1035e-05,  2.2278e-03,  ...,  2.2705e-02,\n",
      "          -6.8359e-03, -3.2043e-03],\n",
      "         ...,\n",
      "         [-6.7139e-03, -5.3711e-03,  3.3569e-03,  ..., -2.6611e-02,\n",
      "          -2.7313e-03,  1.5564e-02],\n",
      "         [-7.5073e-03,  6.2256e-03, -3.6926e-03,  ..., -8.3008e-03,\n",
      "           6.5308e-03,  5.4932e-04],\n",
      "         [ 6.0425e-03, -1.8066e-02, -6.7139e-03,  ...,  9.0942e-03,\n",
      "          -2.0752e-02,  1.4954e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1128,  0.0574, -0.0723,  ...,  0.8477,  0.1738,  0.1138],\n",
      "         [ 0.0011,  0.0032,  0.0098,  ..., -0.0184, -0.0115,  0.0048],\n",
      "         [-0.1035,  0.0444, -0.0625,  ...,  0.7656,  0.1553,  0.1025],\n",
      "         ...,\n",
      "         [-0.0265,  0.0106,  0.0045,  ..., -0.0603, -0.0107,  0.0232],\n",
      "         [-0.0176,  0.0190,  0.0082,  ..., -0.0171, -0.0278, -0.0067],\n",
      "         [ 0.0220, -0.0197, -0.0062,  ...,  0.0016, -0.0442,  0.0074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1963e-01,  6.2256e-02, -6.6895e-02,  ...,  8.6328e-01,\n",
      "           1.8848e-01,  1.0889e-01],\n",
      "         [ 3.7537e-03,  1.3672e-02,  1.9043e-02,  ..., -5.5542e-03,\n",
      "          -5.4932e-04, -7.6294e-03],\n",
      "         [-1.0791e-01, -5.9509e-04, -7.5684e-02,  ...,  7.1875e-01,\n",
      "           1.8945e-01,  4.5654e-02],\n",
      "         ...,\n",
      "         [ 1.3794e-02,  4.7302e-03, -3.3264e-03,  ..., -6.9824e-02,\n",
      "          -4.6143e-02,  1.7700e-02],\n",
      "         [ 2.0386e-02,  1.2878e-02,  2.7100e-02,  ..., -5.7373e-03,\n",
      "          -4.6631e-02, -1.1475e-02],\n",
      "         [ 7.9346e-03, -4.4434e-02,  7.2632e-03,  ...,  3.5400e-02,\n",
      "          -5.2246e-02, -1.2817e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1201,  0.0588, -0.0630,  ...,  0.8633,  0.1855,  0.1035],\n",
      "         [-0.0282, -0.0233,  0.0135,  ..., -0.0200,  0.0121,  0.0138],\n",
      "         [-0.0889,  0.0282, -0.0796,  ...,  0.6758,  0.1670,  0.0386],\n",
      "         ...,\n",
      "         [ 0.0361,  0.0210,  0.0312,  ..., -0.1025, -0.0469, -0.0256],\n",
      "         [ 0.0625, -0.0162,  0.0055,  ..., -0.0026, -0.0684, -0.0083],\n",
      "         [ 0.0210, -0.0649, -0.0566,  ..., -0.0015, -0.0767, -0.0162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-9.9121e-02,  7.2266e-02, -4.3213e-02,  ...,  9.1406e-01,\n",
      "           1.8555e-01,  1.0400e-01],\n",
      "         [-2.3193e-02, -1.7334e-02,  1.3672e-02,  ...,  3.6621e-03,\n",
      "           1.9165e-02,  3.9062e-03],\n",
      "         [-1.3672e-02,  8.6426e-02, -6.3477e-02,  ...,  7.7734e-01,\n",
      "           1.5625e-01,  2.2583e-02],\n",
      "         ...,\n",
      "         [ 3.5156e-02,  1.1597e-02,  5.9082e-02,  ..., -4.5654e-02,\n",
      "          -3.6865e-02,  1.5381e-02],\n",
      "         [ 6.2988e-02, -1.2207e-04,  5.5176e-02,  ..., -4.7363e-02,\n",
      "          -9.0820e-02,  4.4434e-02],\n",
      "         [ 8.9111e-03, -9.0820e-02, -5.1758e-02,  ..., -5.2979e-02,\n",
      "          -4.8828e-02, -4.9805e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1226,  0.0718, -0.0417,  ...,  0.8906,  0.2197,  0.1138],\n",
      "         [ 0.0035, -0.0161,  0.0312,  ...,  0.0496,  0.0157,  0.0107],\n",
      "         [-0.0220,  0.0962, -0.1050,  ...,  0.7461,  0.1885,  0.0225],\n",
      "         ...,\n",
      "         [ 0.0276,  0.0737,  0.0457,  ..., -0.0405, -0.0165,  0.0280],\n",
      "         [ 0.0317,  0.0270,  0.0825,  ..., -0.0571, -0.0859,  0.0747],\n",
      "         [ 0.0466, -0.0894, -0.0684,  ..., -0.0669, -0.0659, -0.0149]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1157,  0.0781, -0.0281,  ...,  0.8828,  0.2363,  0.1206],\n",
      "         [ 0.0035, -0.0079,  0.0024,  ...,  0.0129,  0.0229,  0.0146],\n",
      "         [-0.0240,  0.1182, -0.1230,  ...,  0.7109,  0.1729,  0.0229],\n",
      "         ...,\n",
      "         [ 0.0654,  0.0623,  0.0515,  ..., -0.0544,  0.0080,  0.0106],\n",
      "         [ 0.1396,  0.0684,  0.1177,  ..., -0.1128, -0.0776,  0.0256],\n",
      "         [ 0.0017, -0.0127, -0.0271,  ..., -0.1152, -0.0238, -0.0315]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1221,  0.0981, -0.0195,  ...,  0.8398,  0.2451,  0.1167],\n",
      "         [ 0.0046, -0.0028,  0.0148,  ...,  0.0530,  0.0181,  0.0383],\n",
      "         [-0.0427,  0.1553, -0.0996,  ...,  0.6094,  0.1709,  0.0383],\n",
      "         ...,\n",
      "         [ 0.0708,  0.0718,  0.0593,  ...,  0.0161,  0.0869,  0.0566],\n",
      "         [ 0.1943,  0.0762,  0.0718,  ..., -0.0801, -0.0537,  0.0077],\n",
      "         [ 0.0286, -0.0219,  0.0031,  ..., -0.1104, -0.0569,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0884,  0.1069,  0.0081,  ...,  0.8633,  0.2266,  0.0830],\n",
      "         [-0.0962,  0.0014,  0.1230,  ...,  0.0244, -0.0244,  0.0664],\n",
      "         [-0.0096,  0.1504, -0.0586,  ...,  0.6406,  0.1484,  0.0096],\n",
      "         ...,\n",
      "         [ 0.0386,  0.2441,  0.1001,  ...,  0.0106,  0.0376,  0.1138],\n",
      "         [ 0.1777,  0.1270,  0.1123,  ..., -0.1660, -0.0791,  0.0396],\n",
      "         [-0.0840,  0.0737, -0.1123,  ..., -0.1484,  0.0654,  0.0605]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0728,  0.1650, -0.0155,  ...,  0.8008,  0.2207,  0.1104],\n",
      "         [-0.0957,  0.0142,  0.0898,  ..., -0.0403, -0.0527,  0.0464],\n",
      "         [-0.0215,  0.1226, -0.0199,  ...,  0.5625,  0.1240, -0.0010],\n",
      "         ...,\n",
      "         [ 0.0155,  0.1904,  0.1260,  ..., -0.0066,  0.0273,  0.1406],\n",
      "         [ 0.1738,  0.1069,  0.1650,  ..., -0.1250, -0.0684,  0.0269],\n",
      "         [-0.0508, -0.0615, -0.1206,  ..., -0.1807,  0.0122,  0.0405]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0806,  0.1982, -0.0013,  ...,  0.7344,  0.2305,  0.1157],\n",
      "         [-0.0820,  0.0859,  0.0728,  ...,  0.0281, -0.0247, -0.0236],\n",
      "         [-0.0459,  0.1152, -0.0189,  ...,  0.5273,  0.1191, -0.0391],\n",
      "         ...,\n",
      "         [ 0.0192,  0.2041,  0.1455,  ..., -0.0053,  0.0178,  0.1270],\n",
      "         [ 0.1572,  0.1455,  0.1367,  ..., -0.0698, -0.0664,  0.0051],\n",
      "         [-0.0056, -0.1104, -0.0928,  ..., -0.2197, -0.0181,  0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0684,  0.2217, -0.0308,  ...,  0.6875,  0.2500,  0.1465],\n",
      "         [-0.1001,  0.1514,  0.0598,  ..., -0.0332, -0.0164, -0.0034],\n",
      "         [-0.0261,  0.1953, -0.0400,  ...,  0.5195,  0.0386, -0.0713],\n",
      "         ...,\n",
      "         [ 0.0204,  0.1221,  0.0732,  ...,  0.0225, -0.0413,  0.1475],\n",
      "         [ 0.1050,  0.0591,  0.1260,  ..., -0.0415, -0.0593,  0.0061],\n",
      "         [ 0.0459, -0.1709, -0.1260,  ..., -0.1729,  0.0325,  0.1074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-2.6855e-03,  3.0469e-01, -1.7822e-02,  ...,  5.7031e-01,\n",
      "           2.8125e-01,  1.6309e-01],\n",
      "         [-7.6172e-02,  1.4941e-01,  2.7344e-02,  ..., -3.2715e-02,\n",
      "          -3.8330e-02, -1.3428e-02],\n",
      "         [-5.2246e-02,  2.0312e-01, -4.2969e-02,  ...,  3.5742e-01,\n",
      "           4.3701e-02, -3.5400e-02],\n",
      "         ...,\n",
      "         [ 7.1289e-02,  1.0693e-01,  3.9307e-02,  ...,  9.5825e-03,\n",
      "          -6.5918e-03,  2.2461e-01],\n",
      "         [ 1.0352e-01,  3.1006e-02,  9.2285e-02,  ..., -1.6211e-01,\n",
      "           1.4160e-02,  3.5156e-02],\n",
      "         [-4.8828e-04, -1.7090e-01, -8.9355e-02,  ..., -2.0410e-01,\n",
      "          -1.2207e-03,  8.7891e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0503,  0.2734,  0.0208,  ...,  0.5938,  0.2891,  0.1240],\n",
      "         [-0.1523,  0.0508,  0.0674,  ...,  0.1172, -0.0767,  0.1055],\n",
      "         [-0.1484,  0.1426,  0.0079,  ...,  0.3594,  0.0859, -0.0737],\n",
      "         ...,\n",
      "         [ 0.0679,  0.1348,  0.0239,  ...,  0.0840, -0.0525,  0.2949],\n",
      "         [ 0.1011,  0.0493,  0.0732,  ..., -0.0879,  0.0781,  0.1226],\n",
      "         [ 0.0012, -0.1582, -0.0903,  ..., -0.2070,  0.0732,  0.1299]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0613,  0.3008, -0.0217,  ...,  0.5703,  0.2988,  0.1226],\n",
      "         [-0.1318,  0.0723,  0.0840,  ...,  0.0664, -0.0698,  0.0403],\n",
      "         [-0.2168,  0.0947, -0.0170,  ...,  0.3535,  0.0957, -0.0913],\n",
      "         ...,\n",
      "         [ 0.1504,  0.2188,  0.0771,  ...,  0.0039, -0.0052,  0.2930],\n",
      "         [ 0.1650,  0.1514,  0.1187,  ..., -0.1797,  0.1191,  0.0649],\n",
      "         [ 0.0031, -0.0732, -0.0342,  ..., -0.2734,  0.0603,  0.0811]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0498,  0.3203, -0.0071,  ...,  0.5000,  0.2832,  0.1128],\n",
      "         [-0.0732,  0.0884,  0.1001,  ...,  0.0466, -0.1128,  0.0027],\n",
      "         [-0.2178,  0.0684, -0.0601,  ...,  0.3125,  0.0645, -0.0674],\n",
      "         ...,\n",
      "         [ 0.2158,  0.2139,  0.0403,  ...,  0.0693, -0.0374,  0.3359],\n",
      "         [ 0.1904,  0.1650,  0.1143,  ..., -0.0913,  0.0981,  0.1045],\n",
      "         [-0.0177, -0.0986, -0.0139,  ..., -0.1758,  0.0864,  0.1011]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0513,  0.3027, -0.0513,  ...,  0.4297,  0.2734,  0.1201],\n",
      "         [-0.0576,  0.0894,  0.0840,  ...,  0.0200, -0.0806,  0.0757],\n",
      "         [-0.2676,  0.0698, -0.0796,  ...,  0.2988,  0.0508, -0.1074],\n",
      "         ...,\n",
      "         [ 0.2041,  0.2061,  0.0322,  ...,  0.0332,  0.0420,  0.1973],\n",
      "         [ 0.2207,  0.1592,  0.0579,  ..., -0.0996,  0.1235,  0.0432],\n",
      "         [ 0.0393, -0.0781,  0.0410,  ..., -0.0942,  0.1514, -0.0386]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0786,  0.2637, -0.0410,  ...,  0.4316,  0.3125,  0.0366],\n",
      "         [-0.0613,  0.0757,  0.0781,  ..., -0.0286, -0.1152,  0.0216],\n",
      "         [-0.3027, -0.0150, -0.0571,  ...,  0.2793,  0.0066, -0.1328],\n",
      "         ...,\n",
      "         [ 0.0786,  0.2949, -0.0608,  ...,  0.0630, -0.0317,  0.1069],\n",
      "         [ 0.1543,  0.2930,  0.0204,  ..., -0.0889,  0.0369, -0.0684],\n",
      "         [-0.0242,  0.0352, -0.1680,  ..., -0.1602,  0.0476, -0.0752]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0544,  0.2891, -0.0781,  ...,  0.3945,  0.3125,  0.0537],\n",
      "         [-0.1777,  0.1299,  0.1699,  ..., -0.0957, -0.1328, -0.0254],\n",
      "         [-0.2969, -0.0092, -0.0874,  ...,  0.2852,  0.0040, -0.0532],\n",
      "         ...,\n",
      "         [ 0.0352,  0.2891, -0.1699,  ...,  0.1992, -0.1680,  0.2139],\n",
      "         [ 0.1108,  0.3047, -0.0566,  ...,  0.0269, -0.1079, -0.1138],\n",
      "         [-0.0491,  0.0327, -0.1973,  ...,  0.0107, -0.0205, -0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1084,  0.1533, -0.0107,  ...,  0.3164,  0.3516,  0.0146],\n",
      "         [-0.0933,  0.0957,  0.2090,  ..., -0.0786, -0.1592, -0.0439],\n",
      "         [-0.3340, -0.2090, -0.0728,  ...,  0.1729,  0.0425, -0.0559],\n",
      "         ...,\n",
      "         [ 0.1367,  0.1045, -0.0615,  ...,  0.2412, -0.1084,  0.3418],\n",
      "         [ 0.2002,  0.1260,  0.0601,  ...,  0.0050, -0.0063,  0.0459],\n",
      "         [-0.0242, -0.0115, -0.1328,  ..., -0.1484, -0.0253, -0.0527]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1216,  0.1523, -0.0012,  ...,  0.3047,  0.3730,  0.0009],\n",
      "         [-0.0737,  0.0449,  0.2246,  ..., -0.1562, -0.1094, -0.0554],\n",
      "         [-0.3457, -0.2578, -0.0259,  ...,  0.1553,  0.0010, -0.0288],\n",
      "         ...,\n",
      "         [ 0.0669,  0.0034, -0.0991,  ...,  0.2617, -0.0386,  0.3789],\n",
      "         [ 0.1572,  0.1089,  0.1250,  ..., -0.0610,  0.1133,  0.0786],\n",
      "         [-0.0220, -0.0187, -0.1001,  ..., -0.2168, -0.0447,  0.0237]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1514,  0.1123,  0.0046,  ...,  0.2930,  0.4668, -0.0225],\n",
      "         [-0.1289,  0.0767,  0.1914,  ..., -0.2148, -0.1562, -0.1465],\n",
      "         [-0.3359, -0.2891,  0.0591,  ...,  0.1455,  0.0060, -0.1143],\n",
      "         ...,\n",
      "         [-0.0503,  0.0518,  0.0229,  ...,  0.2539, -0.0400,  0.3398],\n",
      "         [ 0.0674,  0.1055,  0.0444,  ..., -0.1973,  0.1406, -0.0481],\n",
      "         [-0.2090,  0.1855, -0.1025,  ..., -0.3047,  0.0742, -0.0898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1680,  0.1079,  0.0198,  ...,  0.3047,  0.4023, -0.0493],\n",
      "         [-0.1875,  0.1279,  0.1865,  ..., -0.2598, -0.1309, -0.1738],\n",
      "         [-0.3320, -0.2002,  0.0317,  ...,  0.1196, -0.1484, -0.0938],\n",
      "         ...,\n",
      "         [-0.0654, -0.0015, -0.1157,  ...,  0.2598,  0.1465,  0.3516],\n",
      "         [ 0.0649,  0.0806,  0.0454,  ..., -0.3125,  0.2617, -0.0889],\n",
      "         [-0.2637,  0.1660, -0.1504,  ..., -0.3438,  0.1797, -0.0977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1709,  0.0635,  0.0085,  ...,  0.2676,  0.5078, -0.0007],\n",
      "         [-0.1855,  0.2070,  0.1875,  ..., -0.2969, -0.0625, -0.3047],\n",
      "         [-0.3613, -0.2617,  0.0156,  ...,  0.0840, -0.1094, -0.0396],\n",
      "         ...,\n",
      "         [-0.0381, -0.0874, -0.1128,  ...,  0.2080,  0.2480,  0.3066],\n",
      "         [ 0.1328,  0.1055,  0.0310,  ..., -0.3203,  0.2891, -0.0791],\n",
      "         [-0.3008,  0.1260, -0.0737,  ..., -0.4473,  0.1475, -0.0151]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797,  0.0669,  0.0342,  ...,  0.2734,  0.5078,  0.0496],\n",
      "         [-0.1104,  0.1816,  0.2266,  ..., -0.3066, -0.0850, -0.1602],\n",
      "         [-0.2578, -0.2490,  0.1328,  ...,  0.1602, -0.2021, -0.1484],\n",
      "         ...,\n",
      "         [-0.0283,  0.0503, -0.2402,  ...,  0.1523,  0.1797,  0.2656],\n",
      "         [ 0.2051,  0.1011, -0.0752,  ..., -0.4492,  0.1406, -0.1377],\n",
      "         [-0.2891,  0.2188, -0.1562,  ..., -0.5234,  0.1719, -0.1758]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1699,  0.0461,  0.0283,  ...,  0.2578,  0.4551,  0.0605],\n",
      "         [-0.0479,  0.1445,  0.2148,  ..., -0.2715, -0.1367, -0.2432],\n",
      "         [-0.0625, -0.2656,  0.1177,  ...,  0.1406, -0.3242, -0.2285],\n",
      "         ...,\n",
      "         [-0.0430, -0.1230, -0.0898,  ...,  0.0161,  0.2412,  0.3008],\n",
      "         [ 0.2422,  0.1128, -0.1011,  ..., -0.6406,  0.1465, -0.1543],\n",
      "         [-0.4258,  0.2412, -0.1426,  ..., -0.6211,  0.1377, -0.2344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1543,  0.0334,  0.0522,  ...,  0.2852,  0.4531,  0.0688],\n",
      "         [-0.1758,  0.0020,  0.1533,  ..., -0.2988, -0.0513, -0.5234],\n",
      "         [ 0.1035, -0.2559,  0.0571,  ...,  0.1514, -0.4512, -0.3770],\n",
      "         ...,\n",
      "         [-0.2168, -0.3359, -0.1157,  ..., -0.1553,  0.3789,  0.3203],\n",
      "         [ 0.1465,  0.0479, -0.0540,  ..., -0.7812,  0.3945, -0.1426],\n",
      "         [-0.3887,  0.1455, -0.2480,  ..., -0.5820,  0.2520, -0.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1235,  0.0214,  0.0481,  ...,  0.3008,  0.4609,  0.0654],\n",
      "         [-0.2520,  0.1709,  0.2090,  ..., -0.2969, -0.0391, -0.5000],\n",
      "         [ 0.2891, -0.1387, -0.0337,  ...,  0.2070, -0.5859, -0.5273],\n",
      "         ...,\n",
      "         [ 0.0615, -0.2061, -0.1914,  ..., -0.2441,  0.3574,  0.4062],\n",
      "         [ 0.3242,  0.2773, -0.1299,  ..., -0.9570,  0.4160, -0.2324],\n",
      "         [-0.3340,  0.1729, -0.3477,  ..., -0.5547,  0.2070, -0.6211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2051, -0.0101, -0.0811,  ...,  0.2539,  0.4355,  0.1777],\n",
      "         [-0.2891,  0.3516, -0.0032,  ..., -0.3770, -0.0947, -0.5000],\n",
      "         [ 0.1709, -0.1924, -0.1514,  ...,  0.1582, -0.6211, -0.4160],\n",
      "         ...,\n",
      "         [ 0.2383, -0.1006, -0.3984,  ..., -0.3730,  0.1309,  0.3633],\n",
      "         [ 0.4863,  0.4297, -0.2891,  ..., -1.1016,  0.3730, -0.2363],\n",
      "         [-0.2891,  0.2734, -0.4824,  ..., -0.5430,  0.1543, -0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2100, -0.0232, -0.1250,  ...,  0.2363,  0.3887,  0.1562],\n",
      "         [-0.2676,  0.3496,  0.0630,  ..., -0.4727, -0.0776, -0.4473],\n",
      "         [ 0.2412, -0.1650, -0.2080,  ..., -0.0157, -0.6367, -0.5625],\n",
      "         ...,\n",
      "         [ 0.3594, -0.0967, -0.5156,  ..., -0.2520,  0.1582,  0.5000],\n",
      "         [ 0.6367,  0.4707, -0.3594,  ..., -1.0938,  0.4883,  0.0498],\n",
      "         [-0.2197,  0.4062, -0.4473,  ..., -0.7773,  0.1885, -0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934, -0.0330, -0.1245,  ...,  0.2285,  0.3730,  0.1514],\n",
      "         [-0.2471,  0.4766,  0.0066,  ..., -0.3086, -0.1582, -0.4570],\n",
      "         [ 0.1836, -0.1807, -0.2559,  ..., -0.0601, -0.6953, -0.7734],\n",
      "         ...,\n",
      "         [ 0.2305, -0.0486, -0.6016,  ..., -0.2754,  0.1523,  0.4141],\n",
      "         [ 0.5820,  0.5742, -0.2500,  ..., -1.2500,  0.5156,  0.1387],\n",
      "         [-0.2969,  0.3145, -0.4980,  ..., -0.7148,  0.1172, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2012, -0.0522, -0.1631,  ...,  0.2422,  0.3496,  0.1562],\n",
      "         [-0.2520,  0.4648,  0.0164,  ..., -0.4688, -0.1128, -0.4688],\n",
      "         [ 0.1562, -0.1484, -0.4609,  ..., -0.0820, -0.7656, -0.8828],\n",
      "         ...,\n",
      "         [ 0.2793,  0.1187, -0.6797,  ..., -0.2402,  0.2168,  0.3418],\n",
      "         [ 0.5938,  0.8516, -0.4844,  ..., -1.3359,  0.5312,  0.0364],\n",
      "         [-0.3145,  0.2070, -0.5273,  ..., -0.8281,  0.3125, -0.7461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797, -0.0381, -0.1934,  ...,  0.2246,  0.3789,  0.1436],\n",
      "         [-0.2891,  0.5039,  0.0835,  ..., -0.4902, -0.1396, -0.4180],\n",
      "         [-0.0222, -0.1099, -0.6719,  ..., -0.2012, -0.7148, -0.8672],\n",
      "         ...,\n",
      "         [ 0.3574,  0.1108, -0.6445,  ..., -0.0684,  0.1094,  0.3594],\n",
      "         [ 0.5039,  0.8008, -0.3477,  ..., -1.2734,  0.4863, -0.0342],\n",
      "         [-0.4512,  0.1553, -0.5703,  ..., -0.7305,  0.1934, -0.8516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1167, -0.0493, -0.3184,  ...,  0.1045,  0.5312,  0.0952],\n",
      "         [-0.1650,  0.6133, -0.1943,  ..., -0.0859, -0.2070, -0.3086],\n",
      "         [ 0.0781, -0.0991, -0.7812,  ..., -0.2480, -0.6406, -1.0000],\n",
      "         ...,\n",
      "         [ 0.6250,  0.3555, -0.7500,  ..., -0.0154,  0.1465,  0.3965],\n",
      "         [ 0.7305,  1.1250, -0.3008,  ..., -1.1250,  0.4688,  0.0197],\n",
      "         [-0.2598,  0.5859, -0.7188,  ..., -0.4805,  0.2676, -0.8711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1094, -0.0293, -0.2715,  ...,  0.1406,  0.5781,  0.1157],\n",
      "         [-0.3418,  0.7812,  0.0088,  ..., -0.1436, -0.3906, -0.3750],\n",
      "         [-0.0381,  0.0879, -0.9805,  ..., -0.1914, -0.8086, -1.1094],\n",
      "         ...,\n",
      "         [ 0.6562,  0.1875, -0.7422,  ..., -0.1709,  0.2051,  0.4434],\n",
      "         [ 0.6914,  0.9766, -0.2480,  ..., -1.3281,  0.5703, -0.0562],\n",
      "         [-0.3301,  0.4941, -0.6406,  ..., -0.5703,  0.1738, -0.8906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0708,  0.0259, -0.2949,  ...,  0.1562,  0.5742,  0.1426],\n",
      "         [-0.4238,  0.5234,  0.1172,  ..., -0.3418, -0.4023, -0.3340],\n",
      "         [ 0.0122,  0.2227, -1.0547,  ...,  0.0791, -0.8672, -1.2344],\n",
      "         ...,\n",
      "         [ 0.1875, -0.1777, -1.0078,  ..., -0.1338,  0.2275,  0.4023],\n",
      "         [ 0.4727,  0.8906, -0.3672,  ..., -1.5312,  0.6172, -0.1406],\n",
      "         [-0.4844,  0.3906, -0.7500,  ..., -0.7148,  0.3086, -0.7734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0396,  0.0908, -0.1641,  ...,  0.1050,  0.4062,  0.1338],\n",
      "         [-0.1230,  0.6250, -0.0068,  ..., -0.6289, -0.5781, -0.4883],\n",
      "         [-0.0547,  0.2012, -1.1719,  ...,  0.1738, -1.0469, -1.4453],\n",
      "         ...,\n",
      "         [ 0.1641, -0.4023, -1.2031,  ..., -0.1094,  0.3301,  0.3359],\n",
      "         [ 0.7383,  0.9297, -0.4668,  ..., -1.5234,  0.9297, -0.2402],\n",
      "         [-0.2188,  0.2119, -0.7148,  ..., -0.4805,  0.3789, -0.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 2.4414e-04,  2.5586e-01,  1.0742e-01,  ...,  1.5039e-01,\n",
      "          -2.0898e-01,  3.2812e-01],\n",
      "         [-5.6250e-01,  5.0781e-01, -3.2031e-01,  ..., -8.9844e-01,\n",
      "          -6.5234e-01, -1.0547e+00],\n",
      "         [ 3.3984e-01,  6.2500e-01, -1.7422e+00,  ...,  3.2812e-01,\n",
      "          -1.5469e+00, -1.8359e+00],\n",
      "         ...,\n",
      "         [ 1.7676e-01, -6.2891e-01, -1.2656e+00,  ..., -1.9141e-01,\n",
      "           2.6367e-02, -1.3281e-01],\n",
      "         [ 7.8906e-01,  7.9688e-01, -1.9141e-01,  ..., -1.9375e+00,\n",
      "           1.0938e+00, -4.2578e-01],\n",
      "         [-3.0273e-01,  3.0469e-01, -8.0078e-01,  ..., -5.8594e-01,\n",
      "           7.0312e-01, -1.1328e+00]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 2.3535e-01,  8.3594e-01,  2.3828e-01,  ...,  3.3984e-01,\n",
      "           1.8359e-01,  7.5391e-01],\n",
      "         [-7.6562e-01,  3.3594e-01, -1.4648e-03,  ..., -4.6875e-01,\n",
      "          -7.4219e-01, -1.7188e+00],\n",
      "         [ 7.6562e-01,  5.8203e-01, -1.8594e+00,  ...,  6.9922e-01,\n",
      "          -1.4922e+00, -1.4062e+00],\n",
      "         ...,\n",
      "         [ 5.0781e-01, -7.9297e-01, -1.2969e+00,  ...,  6.2109e-01,\n",
      "           3.8281e-01,  1.3672e-01],\n",
      "         [ 7.7344e-01,  7.6562e-01, -7.2656e-01,  ..., -1.2188e+00,\n",
      "           1.4531e+00,  5.7617e-02],\n",
      "         [-5.5859e-01, -1.5625e-02, -4.0625e-01,  ..., -7.6562e-01,\n",
      "           5.0781e-01, -1.0312e+00]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4961,  2.6094,  0.9453,  ..., -0.9727,  1.9453,  2.7656],\n",
      "         [-1.1094,  1.9609,  0.2559,  ..., -0.5859,  1.2656, -1.0312],\n",
      "         [ 0.5469,  1.3750,  0.0156,  ..., -1.5625,  1.3359, -2.8125],\n",
      "         ...,\n",
      "         [-0.5312, -0.3281, -0.4492,  ...,  1.2266, -1.2188,  1.1250],\n",
      "         [-0.4844,  0.9492,  0.3008,  ..., -1.4141, -0.3984,  1.2891],\n",
      "         [-0.8047,  0.4434, -0.4375,  ..., -1.1562, -0.1582, -1.2422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0029, -0.0071, -0.0012,  ..., -0.0046,  0.0016,  0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0096, -0.0276, -0.0009,  ..., -0.0078, -0.0084, -0.0009]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0009, -0.0513,  0.0405,  ..., -0.0178, -0.0074, -0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0197, -0.0786,  0.0776,  ..., -0.0325, -0.0062, -0.0432]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0103, -0.0703,  0.0786,  ..., -0.0684,  0.0033, -0.0366]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0415, -0.0352,  0.0317,  ..., -0.0596, -0.0796, -0.0103]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0247,  0.0371, -0.0186,  ..., -0.1465, -0.0684,  0.0110]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0278,  0.0381,  0.0283,  ..., -0.1011, -0.0552, -0.0249]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0205,  0.0811,  0.0898,  ..., -0.2148,  0.0337, -0.0620]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0091,  0.0269,  0.0352,  ..., -0.1572,  0.0081, -0.0742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0146,  0.0098, -0.0037,  ..., -0.1094, -0.0090, -0.0278]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0503, -0.0242, -0.0708,  ..., -0.0840,  0.0376,  0.0378]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0288, -0.0962, -0.0166,  ..., -0.1045,  0.0845,  0.0767]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0200, -0.1934, -0.0283,  ..., -0.0233,  0.1016,  0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0437, -0.0520, -0.0156,  ...,  0.0203,  0.0898,  0.0820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0378, -0.0432, -0.0215,  ...,  0.0339,  0.0574,  0.0461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0630,  0.0012,  0.0605,  ...,  0.0225,  0.1484, -0.0062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0615, -0.0242, -0.1006,  ..., -0.0293, -0.0015, -0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0352, -0.0742, -0.0591,  ...,  0.0742, -0.0400, -0.0121]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0425, -0.0684,  0.0596,  ..., -0.0518, -0.1445, -0.0159]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0427, -0.0527,  0.0190,  ..., -0.0698,  0.0234,  0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1387, -0.0300,  0.0378,  ..., -0.0918,  0.0449,  0.0044]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1973, -0.0182,  0.0004,  ..., -0.2246,  0.0889, -0.1089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1250,  0.0171,  0.1221,  ..., -0.3672,  0.1226, -0.1123]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1641,  0.1230,  0.0088,  ..., -0.3379,  0.0024, -0.2402]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4199,  0.2207, -0.0366,  ..., -0.4531, -0.0688, -0.3340]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5742,  0.0615, -0.2227,  ..., -0.3906,  0.0679, -0.6914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6172,  0.0898, -0.2334,  ..., -0.3750,  0.0449, -0.6953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5508,  0.2539, -0.3477,  ..., -0.4355,  0.1602, -0.6602]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7148,  0.1846, -0.3711,  ..., -0.6445,  0.2988, -0.6406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7695,  0.2119, -0.5156,  ..., -0.6562,  0.2617, -0.8516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9453,  0.2012, -0.5508,  ..., -0.8672,  0.3594, -0.7266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1016,  0.2793, -0.3691,  ..., -0.8281,  0.4453, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8438,  0.3672, -0.5742,  ..., -0.7539,  0.5820, -0.6211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1797,  0.4316, -0.5859,  ..., -0.8789,  0.5586, -0.7266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.3203,  0.1924, -0.6484,  ..., -1.0703,  0.5195, -0.6250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.5312, -0.0610, -0.4922,  ..., -0.9766,  0.5703, -0.8125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-2.2500,  0.0547, -0.8906,  ..., -1.2031,  1.1250, -0.7852]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-2.0938, -0.5156, -1.0312,  ..., -0.8906,  1.0938, -0.5391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.7812,  0.6211, -1.5312,  ..., -1.1719,  1.1641, -1.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0105, -0.0039, -0.0007,  ..., -0.0249, -0.0096,  0.0143]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0013,  0.0064, -0.0028,  ..., -0.0016, -0.0155,  0.0162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0013,  0.0095, -0.0064,  ..., -0.0222, -0.0286,  0.0090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0127,  0.0159,  0.0035,  ..., -0.0737, -0.0260, -0.0251]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0189,  0.0150,  0.0557,  ..., -0.0957, -0.0271, -0.0381]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0110,  0.0203,  0.0300,  ..., -0.0566, -0.0444, -0.0005]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0339,  0.0247,  0.0123,  ..., -0.1128, -0.0608,  0.0026]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0371,  0.0291,  0.0006,  ..., -0.1074, -0.0498,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0903,  0.0520, -0.0330,  ..., -0.1719, -0.0430,  0.1147]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0342,  0.0479,  0.0186,  ..., -0.0620, -0.0510,  0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0337, -0.0422, -0.0249,  ..., -0.0845, -0.0277,  0.1514]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0527, -0.0591, -0.1216,  ..., -0.0981, -0.0211,  0.1709]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0020, -0.0938, -0.1406,  ..., -0.1230,  0.0178,  0.1973]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1934, -0.0957, -0.1943,  ..., -0.1143,  0.0776,  0.2676]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1865, -0.0068, -0.1211,  ..., -0.1113,  0.0850,  0.2266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1426, -0.0464, -0.1260,  ..., -0.0981,  0.0967,  0.1660]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0830, -0.0068, -0.0249,  ..., -0.0918,  0.1279,  0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1455, -0.0586, -0.0986,  ..., -0.0771,  0.0073,  0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1602, -0.1211, -0.0698,  ..., -0.0239, -0.0378,  0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1680, -0.1816,  0.0894,  ..., -0.1084, -0.0967,  0.0967]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1289, -0.1250, -0.0181,  ..., -0.1602,  0.0181,  0.0859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1992, -0.0845,  0.0310,  ..., -0.2285, -0.1177, -0.0439]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2578, -0.0967, -0.0947,  ..., -0.2812, -0.0820, -0.1484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2139, -0.0457, -0.0449,  ..., -0.4434, -0.0074, -0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1621,  0.0430, -0.1494,  ..., -0.4473, -0.0376, -0.2520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4492,  0.1875, -0.2129,  ..., -0.5898, -0.1045, -0.3164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5312,  0.1445, -0.4180,  ..., -0.6758, -0.0732, -0.7031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6055,  0.1123, -0.3398,  ..., -0.6367, -0.1230, -0.8594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5938,  0.2773, -0.4121,  ..., -0.6992, -0.0581, -0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6133,  0.3281, -0.4785,  ..., -0.8320,  0.2207, -0.7617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6953,  0.4922, -0.5273,  ..., -0.8516,  0.2373, -0.9883]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7227,  0.4336, -0.5508,  ..., -0.9688,  0.3320, -0.8672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9453,  0.4023, -0.5000,  ..., -0.9297,  0.4512, -0.7969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7969,  0.5586, -0.6250,  ..., -0.9727,  0.6562, -0.8398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0391,  0.6172, -0.6484,  ..., -1.0625,  0.5508, -0.8867]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0469,  0.5000, -0.6758,  ..., -1.4219,  0.5391, -0.8008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0078,  0.3281, -0.6250,  ..., -1.4766,  0.4922, -0.8398]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.5859,  0.0391, -0.9258,  ..., -1.8828,  0.8516, -1.0000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.5312, -0.0820, -1.0859,  ..., -1.8750,  0.7422, -0.5859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.5547,  0.5273, -0.4258,  ..., -2.0625,  0.3984, -0.4570]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0134,  0.0024,  0.0118,  ..., -0.0249, -0.0122,  0.0055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0106,  0.0025,  0.0024,  ..., -0.0459, -0.0077,  0.0007]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0123, -0.0052,  0.0249,  ..., -0.0532, -0.0004,  0.0205]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0459, -0.0311,  0.0165,  ..., -0.1836, -0.0197, -0.0077]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0059, -0.0469,  0.0265,  ..., -0.1699, -0.0021, -0.0364]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0596, -0.0435,  0.0281,  ..., -0.1123,  0.0013, -0.0422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0156,  0.0239, -0.0181,  ..., -0.1553,  0.0197, -0.0374]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0361,  0.0342, -0.0259,  ..., -0.1099,  0.0247,  0.0131]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0737,  0.0703, -0.0664,  ..., -0.1895,  0.1182,  0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0576,  0.0510, -0.0256,  ..., -0.1660,  0.0718,  0.1279]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0430,  0.0090, -0.0127,  ..., -0.1953,  0.0522,  0.1182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0400, -0.0085, -0.0503,  ..., -0.1582,  0.1016,  0.1289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0640, -0.0471,  0.0015,  ..., -0.1953,  0.0464,  0.1309]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0239, -0.1357, -0.0957,  ..., -0.0376,  0.1348,  0.1709]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0197, -0.1191, -0.0540,  ..., -0.0835,  0.1406,  0.1572]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0640, -0.1167,  0.0143,  ..., -0.0781,  0.1147,  0.1167]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0337,  0.0400,  0.1562,  ..., -0.0623,  0.0884,  0.0957]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0042,  0.0454, -0.0781,  ..., -0.0820, -0.0474,  0.0615]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1299,  0.0520, -0.1660,  ...,  0.0288, -0.1250,  0.1855]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1553,  0.0126,  0.0303,  ..., -0.1094, -0.1816,  0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0977, -0.1045,  0.0243,  ..., -0.1187, -0.1543,  0.2129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0576, -0.0325, -0.2451,  ..., -0.0952, -0.2031,  0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0520,  0.0176, -0.1953,  ..., -0.1270, -0.1099,  0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0178,  0.0151, -0.1514,  ..., -0.1270, -0.1797,  0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0747,  0.0962, -0.1895,  ..., -0.1787, -0.1699,  0.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2246, -0.0420, -0.1270,  ..., -0.2812, -0.1699,  0.0654]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3379, -0.0396, -0.2246,  ..., -0.2891, -0.0674, -0.0840]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4395, -0.1196, -0.2480,  ..., -0.2441, -0.2168, -0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1309, -0.0349, -0.3555,  ..., -0.1914, -0.2969, -0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1748, -0.0170, -0.3164,  ..., -0.0806, -0.2871,  0.0559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2119, -0.0120, -0.2930,  ..., -0.0398, -0.2471, -0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1992,  0.0283, -0.2188,  ..., -0.0581, -0.0674, -0.1289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5781, -0.0256, -0.2773,  ...,  0.0223, -0.0859, -0.1777]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4336,  0.3418, -0.1650,  ...,  0.0679,  0.1396, -0.0747]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6172,  0.1855, -0.1338,  ..., -0.0176,  0.0654, -0.0457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-6.7188e-01,  8.3008e-02, -1.7578e-02,  ..., -1.2402e-01,\n",
      "           8.3008e-02, -4.8828e-04]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4102, -0.0078,  0.1953,  ..., -0.1006, -0.1484, -0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7188, -0.1006,  0.1064,  ..., -0.1011,  0.4961, -0.8008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8086, -0.2422,  0.0508,  ..., -0.0854,  0.2148, -0.8320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8984, -0.6172,  0.6836,  ...,  0.1836,  0.4062, -1.4766]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0007, 0.0014, 0.0015,  ..., 0.0045, 0.0019, 0.0085]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0048,  0.0113, -0.0157,  ...,  0.0111, -0.0042,  0.0124]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0116,  0.0083, -0.0065,  ...,  0.0349, -0.0014, -0.0012]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0120, -0.0161, -0.0075,  ..., -0.0132,  0.0026,  0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0042,  0.0175, -0.0283,  ..., -0.0225, -0.0359,  0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0050,  0.0254,  0.0108,  ...,  0.0879, -0.0212,  0.0135]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0299,  0.0811,  0.0026,  ..., -0.0264,  0.0115,  0.0195]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0262,  0.0820,  0.0293,  ...,  0.0474, -0.0332,  0.0498]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1187,  0.0938, -0.0337,  ..., -0.0420, -0.0124,  0.0532]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0464,  0.0400,  0.0042,  ..., -0.1328,  0.0073,  0.0238]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0170, -0.0232,  0.0215,  ..., -0.1162, -0.0049,  0.0527]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0271, -0.0439,  0.0090,  ..., -0.0283,  0.0610,  0.0654]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0161, -0.0381,  0.0121,  ..., -0.1309,  0.1211,  0.0603]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1099, -0.2031,  0.1064,  ..., -0.0376,  0.2539,  0.0835]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1206, -0.1187,  0.0688,  ..., -0.0618,  0.2598,  0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0898, -0.1416,  0.0654,  ..., -0.1250,  0.1855,  0.0864]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0767, -0.0762,  0.1748,  ..., -0.0247,  0.2109,  0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0376, -0.0098,  0.0068,  ..., -0.0225,  0.2578, -0.0669]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0503, -0.0176,  0.0298,  ...,  0.1777,  0.2246, -0.0059]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0527, -0.0659,  0.1357,  ...,  0.0596, -0.0791, -0.0588]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0403, -0.0571,  0.1167,  ...,  0.0840,  0.0850,  0.0225]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1230, -0.0334,  0.1484,  ..., -0.1138,  0.0283, -0.2207]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1011,  0.0126,  0.0059,  ...,  0.0010,  0.1768, -0.0850]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0347,  0.0498, -0.0063,  ...,  0.0576,  0.1533, -0.1201]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0214,  0.0273, -0.0033,  ...,  0.0850,  0.1299, -0.0303]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1279, -0.0977,  0.1396,  ..., -0.0193,  0.1689, -0.1865]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0093,  0.0146,  0.0688,  ..., -0.0830,  0.2578, -0.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0723,  0.0674,  0.0388,  ..., -0.0923,  0.2695, -0.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1289,  0.1914,  0.0242,  ..., -0.0469,  0.3164, -0.3984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2324,  0.0845,  0.0664,  ..., -0.1738,  0.3691, -0.5508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1235,  0.2754, -0.0732,  ..., -0.0547,  0.3145, -0.7969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1167,  0.2988, -0.0806,  ..., -0.1631,  0.4512, -0.8750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0752,  0.2422, -0.0020,  ..., -0.1934,  0.4902, -1.0156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0703,  0.3945, -0.0820,  ..., -0.0820,  0.5703, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2041,  0.3887, -0.0596,  ..., -0.0869,  0.5078, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4336,  0.1055, -0.1602,  ..., -0.4336,  0.5625, -0.7344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3691, -0.1387, -0.1826,  ..., -0.5234,  0.5078, -1.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7266, -0.1367, -0.5664,  ..., -0.8438,  1.0625, -1.4531]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6758, -0.7656, -0.7422,  ..., -0.8438,  1.1250, -1.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3984, -0.0371, -1.2578,  ..., -0.7422,  0.8633, -1.8438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0081, -0.0064,  0.0093,  ..., -0.0099,  0.0023,  0.0089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0083, -0.0037,  0.0057,  ..., -0.0106, -0.0044,  0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0065,  0.0063,  0.0056,  ...,  0.0046, -0.0122,  0.0131]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0121,  0.0242,  0.0094,  ..., -0.0388, -0.0131, -0.0112]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0046,  0.0259,  0.0217,  ..., -0.0182,  0.0151,  0.0144]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0041, -0.0028, -0.0122,  ...,  0.0215,  0.0437, -0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0303,  0.0239, -0.0330,  ..., -0.0356,  0.1064, -0.0139]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0190,  0.0491, -0.0245,  ..., -0.0270,  0.0840,  0.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0479,  0.1021,  0.0234,  ..., -0.1807,  0.1299,  0.0439]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0400,  0.0908,  0.1338,  ..., -0.2109,  0.1328,  0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0217,  0.0454,  0.0586,  ..., -0.1904,  0.1064, -0.0366]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0203,  0.0977, -0.0037,  ..., -0.1533,  0.1387,  0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0405,  0.1152, -0.0315,  ..., -0.1689,  0.1050,  0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1196,  0.0278, -0.0491,  ..., -0.1226,  0.1128,  0.1221]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0708,  0.0120, -0.0173,  ..., -0.0693,  0.0923,  0.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0569,  0.0293, -0.0518,  ..., -0.1367,  0.0977,  0.0334]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1064, -0.0183,  0.0732,  ..., -0.1270,  0.2119,  0.0275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0879, -0.0036,  0.0146,  ..., -0.1074,  0.1953, -0.0525]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0835,  0.0339,  0.1211,  ..., -0.1074,  0.1089, -0.0623]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1172, -0.0630,  0.2695,  ..., -0.0869, -0.0393, -0.0598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0239, -0.0564,  0.1094,  ..., -0.1201,  0.1318,  0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1079,  0.0115,  0.1631,  ..., -0.2617,  0.0190, -0.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1094, -0.0444,  0.0522,  ..., -0.2715,  0.0889, -0.2354]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0486, -0.0525,  0.0304,  ..., -0.3359,  0.0698, -0.2617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0020,  0.0190,  0.0315,  ..., -0.4141,  0.0281, -0.2285]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0605,  0.0713,  0.0562,  ..., -0.5117,  0.0659, -0.3555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0957,  0.0361,  0.0496,  ..., -0.5625,  0.1196, -0.7188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2021,  0.1367,  0.0713,  ..., -0.5156,  0.0093, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1572,  0.2578,  0.0220,  ..., -0.4805,  0.0605, -0.7070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3203,  0.2559, -0.0679,  ..., -0.6797,  0.3359, -0.8359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3184,  0.4199, -0.1836,  ..., -0.6172,  0.2539, -1.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4160,  0.4160, -0.2207,  ..., -0.6914,  0.3828, -0.9180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2539,  0.4570, -0.1367,  ..., -0.7969,  0.4492, -0.9727]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3242,  0.6250, -0.1650,  ..., -0.8984,  0.5664, -1.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0117,  0.7344, -0.1719,  ..., -0.9219,  0.6367, -1.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1982,  0.6133, -0.2793,  ..., -1.2344,  0.7383, -0.9609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0425,  0.3320, -0.2656,  ..., -1.4453,  0.7070, -1.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5312, -0.1562, -0.5625,  ..., -1.9062,  0.7031, -1.3047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7617, -0.5430, -0.7070,  ..., -1.8594,  0.5312, -0.8594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3906, -0.0742, -0.6445,  ..., -1.6250,  0.0742, -0.8984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0192, -0.0110, -0.0139,  ..., -0.0200, -0.0166,  0.0011]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0166, -0.0150, -0.0298,  ..., -0.0391, -0.0104,  0.0226]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0051,  0.0157, -0.0476,  ...,  0.0046, -0.0190, -0.0151]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0269,  0.0121, -0.0830,  ...,  0.0115, -0.0312, -0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0320, -0.0134, -0.0674,  ...,  0.0317,  0.0071, -0.0957]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0742,  0.0264, -0.0806,  ...,  0.0444,  0.0021, -0.0574]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0518,  0.0281, -0.1611,  ...,  0.0088,  0.0187, -0.0918]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0447,  0.0938, -0.0488,  ...,  0.0396,  0.0488, -0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0344,  0.1553,  0.0264,  ...,  0.0522,  0.0239, -0.0381]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0840,  0.1162,  0.0640,  ..., -0.0068,  0.0422, -0.0203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1279,  0.0879,  0.0747,  ...,  0.0938, -0.0098, -0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1279,  0.1191,  0.0520,  ..., -0.0137, -0.0737, -0.0564]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1396,  0.0698,  0.0295,  ..., -0.0425,  0.0022,  0.0039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2324, -0.0396,  0.0635,  ..., -0.0229,  0.0352,  0.0540]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1147,  0.0208,  0.0776,  ..., -0.0381, -0.0459,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0835,  0.0074,  0.0664,  ..., -0.0815,  0.0352, -0.0059]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0266,  0.0153,  0.1177,  ..., -0.1187,  0.0625, -0.0016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0659,  0.0129,  0.0254,  ..., -0.1230, -0.0079, -0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0164, -0.0054, -0.0122,  ..., -0.1465, -0.1582,  0.0820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0311, -0.1670,  0.0703,  ..., -0.1855, -0.1992,  0.0879]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0454, -0.2236, -0.0166,  ..., -0.2461, -0.0693,  0.0991]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1348, -0.1562,  0.0479,  ..., -0.3672, -0.1128, -0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2500, -0.2266, -0.0096,  ..., -0.3496, -0.0535, -0.1006]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3027, -0.1201, -0.0464,  ..., -0.4336, -0.0781, -0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3906, -0.0942,  0.0347,  ..., -0.4434, -0.1387, -0.1924]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4102, -0.0684,  0.1006,  ..., -0.4922, -0.1309, -0.2695]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3359, -0.0037,  0.1025,  ..., -0.5039, -0.0679, -0.5508]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2832,  0.0850,  0.1533,  ..., -0.3164, -0.3008, -0.7031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1592,  0.3281,  0.0562,  ..., -0.3262, -0.3086, -0.7773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0850,  0.4199,  0.0260,  ..., -0.4609, -0.3242, -1.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0220,  0.5156, -0.0693,  ..., -0.4258, -0.5469, -1.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0135,  0.5703, -0.1064,  ..., -0.6602, -0.2930, -1.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2852,  0.6406, -0.0244,  ..., -0.7188, -0.3555, -1.2969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1689,  0.8516, -0.1553,  ..., -0.7266, -0.3242, -1.4219]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1875,  0.9766,  0.0830,  ..., -0.9531, -0.2344, -1.3203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4062,  0.8047, -0.0698,  ..., -1.3125, -0.1250, -1.5391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3828,  0.7383,  0.0986,  ..., -1.5391, -0.5352, -1.6562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6641,  0.5312,  0.0659,  ..., -2.1250, -0.2070, -2.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7930,  0.0781, -0.0566,  ..., -1.9844, -0.6328, -1.4297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6094,  0.5703,  0.6719,  ..., -1.9531, -0.9961, -1.9844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0106, -0.0232,  0.0048,  ..., -0.0080,  0.0099,  0.0042]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0201, -0.0161, -0.0111,  ..., -0.0227,  0.0043,  0.0182]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0410,  0.0167,  0.0081,  ...,  0.0205, -0.0103,  0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0483,  0.0195, -0.0021,  ...,  0.0302, -0.0237, -0.0104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0049, 0.0099, 0.0337,  ..., 0.0601, 0.0272, 0.0105]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0610,  0.0391,  0.0029,  ...,  0.0610, -0.0153,  0.0008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0801,  0.0767, -0.0500,  ...,  0.0645, -0.0383, -0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0693,  0.0850,  0.0505,  ...,  0.1621, -0.0366,  0.0095]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[0.0205, 0.0957, 0.0674,  ..., 0.1826, 0.0042, 0.0454]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0422, 0.0698, 0.0508,  ..., 0.0791, 0.0630, 0.0188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0188,  0.0601,  0.1074,  ...,  0.1387, -0.0430,  0.0076]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0571,  0.0923,  0.0413,  ...,  0.1045, -0.0532,  0.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0430, 0.0239, 0.0229,  ..., 0.0364, 0.0046, 0.0149]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0742, -0.0732,  0.0466,  ...,  0.1426,  0.0923,  0.0393]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0281,  0.0640,  0.0693,  ...,  0.1738,  0.0928, -0.0286]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0088,  0.0137, -0.0566,  ...,  0.0337,  0.0630, -0.0879]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0229,  0.0835,  0.0198,  ...,  0.0366,  0.0347, -0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0013,  0.0015,  0.0261,  ..., -0.0674, -0.0133, -0.2432]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0164,  0.0933,  0.0015,  ...,  0.0032, -0.0713, -0.1035]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0128, -0.0537,  0.1157,  ..., -0.0771, -0.1396, -0.0023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0168, -0.0427,  0.0254,  ..., -0.0996,  0.0371,  0.1172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2236, -0.0317,  0.1045,  ..., -0.2734, -0.0840, -0.0055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1445, -0.0126,  0.0469,  ..., -0.2871, -0.0447, -0.0330]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1582,  0.1035,  0.0820,  ..., -0.2852,  0.0200, -0.0586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2949,  0.1406,  0.1152,  ..., -0.2012,  0.0269, -0.0520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3633,  0.2109, -0.0420,  ..., -0.2275,  0.0967, -0.0242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4883, -0.1973,  0.0771,  ..., -0.1104,  0.2695, -0.4180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7812, -0.0503,  0.0098,  ..., -0.0659,  0.3457, -0.4688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8320,  0.0305,  0.0305,  ..., -0.0732,  0.4434, -0.5156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8750, -0.0198, -0.0703,  ..., -0.1641,  0.6641, -0.4746]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0312,  0.0317, -0.1455,  ...,  0.0723,  0.4590, -0.5742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1562, -0.1924, -0.1709,  ...,  0.0181,  0.5547, -0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1562, -0.2266, -0.0928,  ..., -0.0771,  0.5938, -0.4844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.1484, -0.1504, -0.2178,  ...,  0.0435,  0.6602, -0.4824]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.2500, -0.1475, -0.1079,  ..., -0.0138,  0.7461, -0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.3594, -0.2461, -0.2500,  ..., -0.1992,  0.8359, -0.5898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.3281, -0.6562, -0.1514,  ..., -0.1289,  0.6992, -0.6797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.8828, -1.0625, -0.5352,  ...,  0.0283,  1.1562, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.9062, -1.5625, -0.5703,  ...,  0.5000,  1.2109, -0.2432]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-2.3125, -0.7773, -1.0391,  ...,  1.5391,  0.5547, -0.4883]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0114, -0.0067, -0.0071,  ..., -0.0220, -0.0067,  0.0062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0040,  0.0007, -0.0194,  ..., -0.0361, -0.0027,  0.0149]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0157,  0.0071, -0.0005,  ..., -0.0264,  0.0016, -0.0080]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0068, -0.0036,  0.0036,  ..., -0.0488, -0.0060, -0.0226]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0002,  0.0168, -0.0057,  ..., -0.0786, -0.0112, -0.0435]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0060, -0.0134,  0.0303,  ..., -0.0815, -0.0532, -0.0571]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0254, -0.0210, -0.0918,  ..., -0.1191, -0.0762, -0.0698]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0486,  0.0190, -0.0635,  ..., -0.0171, -0.0284, -0.0737]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0825,  0.0052, -0.0762,  ..., -0.0153, -0.0060, -0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0571, -0.0005, -0.0049,  ..., -0.0076,  0.0347,  0.0017]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0405, -0.0601,  0.0918,  ...,  0.1270, -0.0085,  0.0094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0260, -0.0347,  0.0488,  ...,  0.0747, -0.0491,  0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0096, -0.0376,  0.1016,  ...,  0.0664, -0.0068,  0.0483]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1406, -0.1123,  0.0327,  ...,  0.1953,  0.0579,  0.0889]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1309,  0.0039,  0.0493,  ...,  0.1982,  0.0908,  0.0593]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0664, -0.0347, -0.0276,  ...,  0.0854,  0.0796,  0.0024]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0698,  0.0056, -0.0236,  ...,  0.0894,  0.0295, -0.0439]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0923, -0.0664, -0.0059,  ..., -0.0112,  0.0552, -0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0889, -0.0312, -0.0713,  ...,  0.0410,  0.0908,  0.0554]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0129, -0.2314, -0.0308,  ..., -0.0137, -0.1060,  0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0200, -0.1211, -0.1060,  ..., -0.0522,  0.0352,  0.2021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1973, -0.0635, -0.0535,  ..., -0.2148, -0.0417,  0.0674]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1641, -0.0850, -0.1221,  ..., -0.2041, -0.0330, -0.0630]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1533, -0.0454, -0.0952,  ..., -0.1992,  0.0396, -0.0815]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1855,  0.0236, -0.0444,  ..., -0.2373,  0.0601, -0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3223,  0.0771, -0.0645,  ..., -0.1660,  0.0786, -0.1494]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2812, -0.2109,  0.1367,  ..., -0.1719,  0.0820, -0.4668]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5078,  0.0254,  0.1396,  ..., -0.2070,  0.0894, -0.5312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5391,  0.0850,  0.2119,  ..., -0.2158,  0.1318, -0.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5547,  0.1943,  0.1787,  ..., -0.3066,  0.3848, -0.5781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6445,  0.2070,  0.1436,  ..., -0.1953,  0.1934, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7422,  0.0850,  0.1172,  ..., -0.2500,  0.2441, -0.5664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6953,  0.0342,  0.2773,  ..., -0.2754,  0.2559, -0.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7578,  0.1133,  0.2334,  ..., -0.2734,  0.3438, -0.5742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7773,  0.0869,  0.2344,  ..., -0.3848,  0.4082, -0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8242,  0.0020,  0.0020,  ..., -0.5781,  0.3867, -0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8086, -0.2500,  0.0859,  ..., -0.5625,  0.2432, -0.7773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1406, -0.8125, -0.3438,  ..., -0.4648,  0.4570, -0.7617]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.2969, -1.1484, -0.5430,  ..., -0.0420,  0.3633, -0.3418]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.3125, -1.1406, -1.1875,  ...,  0.3008, -0.6172, -0.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0067, -0.0084, -0.0162,  ..., -0.0151, -0.0369,  0.0114]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0062, -0.0109, -0.0259,  ..., -0.0330, -0.0474,  0.0062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0195, -0.0060, -0.0430,  ...,  0.0061, -0.0757, -0.0111]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0236, -0.0157, -0.0576,  ..., -0.0178, -0.0894, -0.0181]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0022, -0.0247, -0.0261,  ...,  0.0145, -0.0684,  0.0043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0505, -0.0164, -0.0374,  ...,  0.0481, -0.0908,  0.0427]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0208, -0.0079, -0.1035,  ...,  0.0139, -0.0396,  0.0108]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0225,  0.0244, -0.0962,  ...,  0.0153, -0.0894,  0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0236,  0.0371, -0.0259,  ...,  0.0493, -0.1162,  0.0776]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0938,  0.0386,  0.0302,  ...,  0.0654, -0.0835,  0.0737]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0190,  0.0479,  0.0566,  ...,  0.1406, -0.0972,  0.0332]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0596,  0.0981,  0.1157,  ...,  0.0693, -0.1709,  0.0439]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0513,  0.0168,  0.1426,  ...,  0.0635, -0.0947,  0.1006]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0762, -0.0591,  0.1699,  ...,  0.1113, -0.0547,  0.0889]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0085, -0.0723,  0.1162,  ...,  0.0522, -0.0669,  0.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0532, -0.0435,  0.0967,  ...,  0.0620, -0.1187,  0.1670]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.2207e-04, -3.8086e-02,  1.3086e-01,  ...,  8.7891e-02,\n",
      "          -1.0986e-01,  1.3281e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0488, -0.0947,  0.0576,  ...,  0.0188, -0.2246,  0.1260]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0400, -0.1064,  0.1514,  ...,  0.1152, -0.3516,  0.3184]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0332, -0.3789,  0.1699,  ...,  0.0928, -0.4551,  0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0620, -0.3809, -0.0137,  ..., -0.0107, -0.4414,  0.4004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2324, -0.3379, -0.0574,  ..., -0.1377, -0.5508,  0.1660]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3652, -0.2275, -0.1484,  ..., -0.1045, -0.4629,  0.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3086, -0.1318, -0.1172,  ..., -0.0786, -0.4883,  0.1094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3750, -0.1660, -0.0320,  ..., -0.1104, -0.4688,  0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3047, -0.1914, -0.1387,  ..., -0.2148, -0.2852,  0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2246, -0.3809, -0.0684,  ..., -0.1650, -0.2295, -0.0840]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4512, -0.2539, -0.0623,  ..., -0.0898, -0.2256, -0.1719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3945, -0.1631, -0.1523,  ..., -0.0801, -0.1484, -0.2266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3320, -0.0845, -0.1045,  ..., -0.1094, -0.1553, -0.2354]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5469, -0.0435, -0.2031,  ..., -0.1235, -0.3516, -0.2520]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6055, -0.0566, -0.1797,  ..., -0.2852, -0.2158, -0.2041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7578, -0.0527, -0.2227,  ..., -0.2852, -0.3203, -0.2598]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5938,  0.1660, -0.3359,  ..., -0.0977, -0.2617, -0.2539]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5938,  0.2031, -0.3262,  ..., -0.1016, -0.1660, -0.3691]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8438, -0.2471, -0.3672,  ..., -0.1650, -0.1934, -0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7344, -0.3984, -0.1123,  ..., -0.2695, -0.6328, -0.6367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9102, -0.6992, -0.3516,  ..., -0.4121, -0.2383, -1.0000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7695, -1.0703, -0.3535,  ...,  0.0186, -0.2422, -0.7031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8516,  0.5312, -0.2559,  ...,  0.9609, -1.6875, -1.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0075, -0.0036,  0.0074,  ..., -0.0386, -0.0090, -0.0126]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0107, -0.0071,  0.0008,  ..., -0.0486, -0.0108, -0.0072]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0190,  0.0142,  0.0104,  ..., -0.0664, -0.0400, -0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0142,  0.0417, -0.0254,  ..., -0.0991, -0.0474, -0.0400]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0422, -0.0190, -0.0457,  ..., -0.0796, -0.0464, -0.0493]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0225,  0.0176,  0.0042,  ..., -0.0552, -0.0435, -0.0083]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0435,  0.0288,  0.0024,  ..., -0.0366,  0.0049,  0.0220]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0625,  0.1187, -0.0295,  ..., -0.0233,  0.0243, -0.0075]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0220,  0.2617, -0.0118,  ..., -0.0342,  0.1021, -0.0184]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0325,  0.1523,  0.0250,  ...,  0.0182,  0.0674,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0223, 0.1123, 0.1328,  ..., 0.0488, 0.0110, 0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1035,  0.0947,  0.1006,  ..., -0.0054,  0.0212,  0.0065]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1465,  0.1445,  0.0498,  ..., -0.0264,  0.1074,  0.0181]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0107,  0.0952,  0.0137,  ...,  0.0977,  0.0137,  0.1543]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0383, 0.0850, 0.0098,  ..., 0.0864, 0.0221, 0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0981,  0.0527,  0.0391,  ...,  0.0535, -0.0269,  0.0155]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0293,  0.0352,  0.1836,  ...,  0.0608,  0.0488, -0.0986]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0388,  0.0060,  0.0542,  ..., -0.0493,  0.0067, -0.1157]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0322,  0.0771,  0.0542,  ...,  0.0359, -0.0020,  0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0391, -0.0894,  0.0083,  ..., -0.0137, -0.2295,  0.0386]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1094, -0.1494, -0.1904,  ..., -0.1143, -0.1138,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0771, -0.1152, -0.0859,  ..., -0.1494, -0.1494, -0.0166]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1807,  0.0244, -0.0474,  ...,  0.0273, -0.1006, -0.1455]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1826,  0.0005, -0.0479,  ...,  0.0200, -0.1157, -0.2002]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0527,  0.1074,  0.0032,  ..., -0.0219, -0.0913, -0.0654]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1367, -0.0137, -0.1143,  ..., -0.1807, -0.0889, -0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0908, -0.1040, -0.1484,  ..., -0.2734, -0.0879, -0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0459,  0.1660, -0.1235,  ..., -0.2246, -0.0179, -0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1914,  0.4883, -0.1592,  ..., -0.2119,  0.0240, -0.1064]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2139,  0.4961, -0.0352,  ..., -0.2676,  0.1270,  0.1289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2100,  0.6875,  0.0166,  ..., -0.1133,  0.1602,  0.1709]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3105,  0.5156,  0.2324,  ..., -0.1650,  0.0728,  0.0466]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0566,  0.7070,  0.3242,  ..., -0.3320,  0.0405, -0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1426,  0.7109,  0.2402,  ..., -0.2500,  0.0654, -0.1777]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0762,  0.7852,  0.2090,  ..., -0.1465, -0.1201,  0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2695,  0.5234,  0.1602,  ..., -0.1758, -0.1406,  0.2432]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5977,  0.7500,  0.0645,  ...,  0.0361, -0.1885,  0.2197]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8906,  0.9531, -0.0918,  ..., -0.0134, -0.4141, -0.3945]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.0547,  0.9219, -0.6641,  ...,  0.5156, -0.2773, -0.1855]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.4219,  1.4766, -0.7031,  ...,  0.4043, -1.9766,  1.2422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0148, -0.0035, -0.0153,  ..., -0.0104,  0.0070, -0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0198,  0.0060, -0.0030,  ..., -0.0059,  0.0083, -0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0221,  0.0195,  0.0068,  ...,  0.0128, -0.0042,  0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0583,  0.0386,  0.0071,  ..., -0.0098, -0.0092, -0.0070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0801, 0.0320, 0.0562,  ..., 0.0027, 0.0114, 0.0369]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0898,  0.0269,  0.0811,  ..., -0.0107, -0.0031,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0806,  0.0664,  0.0459,  ..., -0.0103,  0.0120,  0.1143]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0850,  0.0879,  0.0569,  ...,  0.0571, -0.0493,  0.0557]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1328,  0.1377,  0.0042,  ...,  0.1455, -0.0112,  0.0679]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0049, 0.0410, 0.0469,  ..., 0.1367, 0.0352, 0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0011, 0.0508, 0.1426,  ..., 0.1855, 0.0405, 0.1299]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0189,  0.0449,  0.1426,  ...,  0.1631,  0.0068,  0.1270]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0442, 0.0317, 0.0674,  ..., 0.1895, 0.0603, 0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0159,  0.0654, -0.0684,  ...,  0.2910, -0.0020,  0.1660]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0000,  0.0049, -0.0508,  ...,  0.2871,  0.0020,  0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0238, -0.0889,  0.0190,  ...,  0.2148, -0.0542,  0.0498]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0830, -0.1484,  0.0874,  ...,  0.2432, -0.0481,  0.0094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0425, -0.0786, -0.0154,  ...,  0.1895,  0.0181, -0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0271, -0.0972, -0.0801,  ...,  0.2734, -0.0093,  0.0220]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0918, -0.3047,  0.0488,  ...,  0.1602, -0.2266, -0.0859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1416, -0.4609, -0.1196,  ...,  0.1396, -0.0488, -0.0088]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0879, -0.2949, -0.0640,  ...,  0.0610,  0.0273, -0.0525]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2246, -0.1484,  0.0718,  ...,  0.1924, -0.0386, -0.1641]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2637, -0.0020,  0.0471,  ...,  0.3203, -0.0278, -0.1641]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2324,  0.1484, -0.0820,  ...,  0.0986, -0.1387, -0.1348]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3027,  0.1836, -0.0967,  ...,  0.0325, -0.1836, -0.2344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3145,  0.2275,  0.0869,  ...,  0.1953, -0.3008, -0.2295]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797,  0.2500, -0.0249,  ...,  0.0962, -0.2637, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1299,  0.3672, -0.1758,  ...,  0.0195, -0.1406, -0.4219]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2246,  0.2227, -0.2109,  ..., -0.1377, -0.1934, -0.4648]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2695,  0.2871, -0.1230,  ..., -0.0361, -0.1895, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4219,  0.2383, -0.0415,  ..., -0.2227, -0.0767, -0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8438,  0.3535, -0.0825,  ...,  0.0439, -0.2578, -0.6445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6641,  0.6172, -0.4004,  ...,  0.3555, -0.0625, -0.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8047,  0.6016, -0.4180,  ...,  0.4121, -0.0776, -0.4492]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9922,  0.2949, -0.4766,  ...,  0.5586, -0.4336, -0.3848]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0078,  0.1523, -0.5859,  ...,  0.6367, -0.5820, -0.6562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0938,  0.3477, -1.0078,  ...,  0.4961, -0.1602, -0.9062]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.5781,  0.0391, -0.7773,  ...,  0.2734, -0.5703, -0.6289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9336,  1.4609, -0.1191,  ...,  0.6406, -1.8438, -1.4531]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0051, -0.0074,  0.0055,  ...,  0.0104,  0.0148,  0.0070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0128, -0.0060, -0.0051,  ...,  0.0254,  0.0220,  0.0126]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0640,  0.0289,  0.0042,  ...,  0.0854,  0.0128, -0.0142]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0684,  0.0276,  0.0201,  ...,  0.0986,  0.0209, -0.0116]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0640,  0.0129,  0.0043,  ...,  0.1011,  0.0403,  0.0408]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0166,  0.0312, -0.0096,  ...,  0.1177,  0.0330,  0.0610]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0376,  0.0308, -0.0291,  ...,  0.0747,  0.0732,  0.1240]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0291,  0.0466,  0.0208,  ...,  0.0752, -0.0850,  0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0364,  0.0518, -0.0327,  ...,  0.0693, -0.0615, -0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1270, -0.0020,  0.0474,  ...,  0.0132, -0.0996, -0.0190]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0996,  0.0806,  0.0957,  ...,  0.0444, -0.0703, -0.0430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1387,  0.1504,  0.1260,  ..., -0.0388, -0.0776, -0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0898,  0.0623,  0.0991,  ..., -0.0879, -0.0208, -0.0471]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1309,  0.0391,  0.1367,  ..., -0.0889, -0.0042, -0.0171]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0884,  0.0229,  0.1680,  ..., -0.0957,  0.0322, -0.0593]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0952, -0.0347,  0.1562,  ..., -0.0066,  0.0239, -0.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0193, -0.0698,  0.1357,  ...,  0.0815,  0.0393, -0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0164, -0.0117,  0.0713,  ..., -0.0542,  0.0129, -0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0170, -0.0126,  0.0889,  ...,  0.0337, -0.0464, -0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0488, -0.1875,  0.1641,  ..., -0.0240, -0.2754, -0.0054]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0242, -0.3535,  0.0811,  ..., -0.1289, -0.3105, -0.0500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2520, -0.2500,  0.0396,  ..., -0.2578, -0.3477, -0.2256]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3457, -0.0654, -0.0500,  ..., -0.1855, -0.3809, -0.3242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3262,  0.0879, -0.0164,  ..., -0.1855, -0.2695, -0.3633]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3242,  0.0593, -0.0017,  ..., -0.2617, -0.1152, -0.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4277, -0.0376, -0.1533,  ..., -0.3965,  0.0190, -0.3477]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5508, -0.2773, -0.0347,  ..., -0.4512,  0.0623, -0.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7422, -0.1230, -0.0947,  ..., -0.4375,  0.1309, -0.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7305, -0.0281, -0.1445,  ..., -0.4102,  0.2041, -0.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6953, -0.0483, -0.2227,  ..., -0.5703,  0.2754, -0.4824]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7773, -0.0388, -0.1660,  ..., -0.5000,  0.1523, -0.5391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8242, -0.1748, -0.1416,  ..., -0.6719,  0.2793, -0.4375]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9414, -0.0688, -0.0903,  ..., -0.7070,  0.2344, -0.4082]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8438,  0.2051, -0.1885,  ..., -0.5352,  0.3301, -0.3379]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8711,  0.2852, -0.1094,  ..., -0.5078,  0.4043, -0.4043]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9883,  0.0137, -0.2812,  ..., -0.5156,  0.3574, -0.4355]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6875, -0.0248, -0.2305,  ..., -0.6719, -0.0352, -0.6211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8398, -0.0287, -0.6211,  ..., -0.7539,  0.5820, -1.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.9336, -0.4961, -0.5234,  ..., -0.5078,  0.6094, -0.9844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7422,  0.6484, -0.8789,  ...,  0.3613, -0.4180, -1.5391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0016, 0.0034, 0.0031,  ..., 0.0035, 0.0030, 0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0027,  0.0052, -0.0159,  ...,  0.0302, -0.0046,  0.0038]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0009,  0.0125, -0.0154,  ...,  0.0447, -0.0111, -0.0034]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0317, -0.0094, -0.0214,  ...,  0.0073,  0.0075, -0.0249]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0251, -0.0134, -0.0244,  ...,  0.0625,  0.0266, -0.0074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0276, -0.0195,  0.0210,  ...,  0.1309,  0.0508, -0.0035]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0593, -0.0183, -0.0113,  ...,  0.0791,  0.0496, -0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0398,  0.0781,  0.0664,  ...,  0.0938, -0.0085,  0.0505]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0698,  0.1865,  0.1338,  ..., -0.0327,  0.0413,  0.0786]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0659,  0.1680,  0.1387,  ..., -0.0334,  0.0430,  0.0254]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0566,  0.1094,  0.1465,  ..., -0.0256, -0.0566, -0.0280]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0486,  0.1543,  0.0889,  ..., -0.0444,  0.0378,  0.0435]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0386,  0.0732,  0.0210,  ..., -0.1152,  0.0295,  0.1523]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2217, -0.0142, -0.0205,  ..., -0.1758,  0.0396,  0.3086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1543,  0.0182,  0.0549,  ..., -0.2275,  0.0830,  0.2217]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1406, -0.1260,  0.0322,  ..., -0.1621,  0.0981,  0.1631]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0369, -0.1143,  0.1680,  ..., -0.1133,  0.1562,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0767, -0.0569,  0.1699,  ..., -0.2080,  0.1030,  0.0845]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0879,  0.0723,  0.2051,  ..., -0.1494,  0.1602,  0.1807]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1133, -0.0537,  0.2441,  ..., -0.2070,  0.0659,  0.1992]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0288, -0.0679,  0.2227,  ..., -0.2695,  0.1162,  0.2373]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0884,  0.0366,  0.2031,  ..., -0.2305, -0.0664, -0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1211,  0.0889,  0.2061,  ..., -0.2539, -0.1152, -0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0762,  0.2266,  0.2598,  ..., -0.2773, -0.0347, -0.4336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0101,  0.2207,  0.1562,  ..., -0.3691,  0.1367, -0.3203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0156,  0.2256,  0.0791,  ..., -0.4629,  0.0586, -0.3418]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0256,  0.1396,  0.0309,  ..., -0.5703,  0.2207, -0.4980]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0732,  0.2363, -0.0408,  ..., -0.5352,  0.1836, -0.4473]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1250,  0.3164, -0.1484,  ..., -0.5156,  0.3027, -0.5664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1826,  0.3047, -0.1396,  ..., -0.5508,  0.4062, -0.4766]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1797,  0.3301, -0.1367,  ..., -0.5234,  0.3457, -0.6484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1436,  0.3398, -0.0405,  ..., -0.7031,  0.5469, -0.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0659,  0.2207, -0.1016,  ..., -0.7891,  0.4883, -0.6133]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0359,  0.5391, -0.2500,  ..., -0.5547,  0.6172, -0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1914,  0.5273, -0.1768,  ..., -0.5469,  0.4766, -0.7422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2012,  0.1328, -0.1641,  ..., -0.6641,  0.4609, -0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1484,  0.0442, -0.0527,  ..., -0.8750,  0.2559, -0.8086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4492, -0.4492, -0.4531,  ..., -1.1875,  0.8594, -1.4219]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4941, -1.0781, -0.5000,  ..., -0.7266,  1.0312, -1.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4512, -0.1328, -1.0781,  ..., -0.4082,  1.1562, -1.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0085,  0.0089, -0.0061,  ..., -0.0294, -0.0141, -0.0013]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0096, -0.0071, -0.0031,  ..., -0.0344, -0.0074, -0.0033]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0173,  0.0119,  0.0078,  ..., -0.0630, -0.0265, -0.0130]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0166,  0.0150, -0.0269,  ..., -0.1128, -0.0223, -0.0231]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0208, -0.0184, -0.0184,  ..., -0.1011, -0.0016, -0.0381]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0007, -0.0076,  0.0291,  ..., -0.0474, -0.0276, -0.0130]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0008,  0.0131, -0.0349,  ..., -0.0186,  0.0173,  0.0002]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0140,  0.0322,  0.0098,  ..., -0.0282,  0.0508,  0.0183]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0713,  0.1299,  0.0059,  ..., -0.0391,  0.1758,  0.0031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0088,  0.0625,  0.0625,  ..., -0.0327,  0.1299,  0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0256,  0.0154,  0.1230,  ..., -0.0588,  0.1025,  0.0046]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0189,  0.0210,  0.1128,  ..., -0.1289,  0.1465,  0.0205]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0183,  0.0933,  0.0039,  ..., -0.1445,  0.1807,  0.0193]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0474,  0.0732, -0.1221,  ..., -0.0864,  0.1162,  0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0176,  0.1328, -0.0264,  ..., -0.1680,  0.0806,  0.0757]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0562,  0.0830, -0.0164,  ..., -0.0742,  0.0093,  0.1768]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1230,  0.0645,  0.1240,  ...,  0.0018,  0.0903,  0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0972,  0.1309,  0.0820,  ..., -0.0557,  0.0278, -0.0181]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1338,  0.1582,  0.0435,  ..., -0.0339, -0.0068,  0.0154]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0918,  0.0635,  0.1426,  ..., -0.0840,  0.0247, -0.0112]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0266,  0.0566,  0.0669,  ..., -0.1045,  0.0593,  0.1108]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0076,  0.1240,  0.1709,  ..., -0.0088, -0.0173, -0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0249,  0.2598,  0.2070,  ..., -0.1152, -0.0640, -0.2559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0024,  0.2598,  0.2344,  ..., -0.1738,  0.0092, -0.3594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0579,  0.2334,  0.0938,  ..., -0.2637,  0.1289, -0.4141]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0801,  0.1943,  0.1719,  ..., -0.3926,  0.0684, -0.5117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0020,  0.1660,  0.1201,  ..., -0.4082,  0.2500, -0.7344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0481,  0.3828,  0.0645,  ..., -0.3613,  0.3730, -0.6680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1074,  0.6445,  0.0225,  ..., -0.3809,  0.4102, -0.7109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1367,  0.6875,  0.1953,  ..., -0.2988,  0.5234, -0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1035,  0.9023,  0.1689,  ..., -0.3945,  0.6250, -0.5664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1641,  0.8711,  0.3477,  ..., -0.5430,  0.7383, -0.7031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0049,  0.9609,  0.4766,  ..., -0.5430,  0.7969, -0.7266]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0830,  1.2109,  0.3184,  ..., -0.3867,  0.8984, -0.7344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2070,  1.1641,  0.3750,  ..., -0.3594,  0.8906, -0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4688,  1.0469,  0.4961,  ..., -0.3184,  0.9531, -0.6562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6406,  1.1875,  0.6289,  ...,  0.0020,  0.9961, -0.6484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7773,  1.2734,  0.2754,  ..., -0.4980,  0.9336, -1.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7539,  1.0859, -0.2070,  ...,  0.3750,  1.3047, -1.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.7812,  2.1875, -0.4453,  ...,  0.0273, -0.8594,  0.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0010,  0.0035, -0.0016,  ..., -0.0036, -0.0161,  0.0137]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0133,  0.0118, -0.0023,  ..., -0.0205, -0.0183,  0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0001,  0.0325, -0.0022,  ..., -0.0159, -0.0408,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0056,  0.0654, -0.0253,  ..., -0.0221, -0.0376,  0.0247]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0305,  0.0266, -0.0048,  ..., -0.0086, -0.0045, -0.0045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0432,  0.0518,  0.0203,  ...,  0.0197, -0.0464, -0.0118]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0275,  0.0869,  0.0122,  ...,  0.0104, -0.0172, -0.0393]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0503,  0.0947, -0.0012,  ...,  0.0757,  0.0295, -0.0028]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0564,  0.1133, -0.0354,  ...,  0.0337,  0.0593, -0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0085,  0.0327, -0.0232,  ...,  0.0430,  0.0811,  0.0248]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0232,  0.0630,  0.0356,  ...,  0.0012,  0.1191,  0.0242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0430,  0.1250,  0.0571,  ..., -0.0312,  0.1104, -0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0771,  0.1445, -0.0195,  ...,  0.0012,  0.1621, -0.0664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1113,  0.0923, -0.1260,  ...,  0.0610,  0.1328, -0.0684]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0801,  0.0869, -0.0503,  ..., -0.0029,  0.1074, -0.0703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0396,  0.0530,  0.0752,  ...,  0.0310,  0.1099, -0.0186]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0471,  0.0200,  0.2266,  ...,  0.1167,  0.1250,  0.0393]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0583,  0.1367,  0.1118,  ...,  0.0752,  0.0879, -0.0645]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1123,  0.1836,  0.0220,  ...,  0.1206,  0.0464, -0.0168]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1167,  0.1445,  0.0928,  ...,  0.1270,  0.0286, -0.0371]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0081,  0.1709,  0.1484,  ..., -0.0444,  0.1064,  0.0610]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1699,  0.1670,  0.2324,  ...,  0.0151,  0.0356, -0.1934]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2734,  0.3145,  0.2354,  ..., -0.0496,  0.1182, -0.2480]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3184,  0.3223,  0.1602,  ...,  0.0513,  0.2178, -0.3516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3184,  0.2832,  0.1895,  ..., -0.0217,  0.2617, -0.2891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3516,  0.4141,  0.1108,  ...,  0.0344,  0.1855, -0.4414]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4883,  0.4199,  0.1270,  ..., -0.0035,  0.2129, -0.7344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5938,  0.4844,  0.1523,  ...,  0.1738,  0.3242, -0.8203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5547,  0.7188,  0.0254,  ...,  0.1797,  0.3574, -0.9219]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4844,  0.8086,  0.1758,  ...,  0.2363,  0.3809, -1.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4941,  0.7812,  0.1533,  ...,  0.2354,  0.3438, -1.3750]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4688,  0.8945,  0.2441,  ...,  0.3086,  0.3867, -1.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5586,  0.8203,  0.2158,  ...,  0.3223,  0.4629, -1.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5391,  1.1406,  0.0020,  ...,  0.4336,  0.4102, -1.5703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7695,  1.2031, -0.0513,  ...,  0.4922,  0.3633, -1.6953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9609,  1.0391, -0.0212,  ...,  0.3496,  0.2266, -1.6328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1406,  0.8555, -0.0452,  ...,  0.5781,  0.2988, -1.7891]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.6406,  0.6211, -0.4023,  ...,  0.2383,  0.4375, -2.2500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.6953,  0.0703, -0.2988,  ...,  0.5898,  0.2344, -2.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7344, -0.2227, -1.4766,  ...,  0.4277, -2.2344, -1.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0034,  0.0010,  0.0183,  ..., -0.0084,  0.0026,  0.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0101, -0.0062,  0.0391,  ..., -0.0159, -0.0019, -0.0140]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0016,  0.0129,  0.0291,  ...,  0.0400,  0.0092, -0.0304]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0171,  0.0349,  0.0275,  ...,  0.0312,  0.0087, -0.0145]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0200,  0.0002,  0.0188,  ...,  0.0266,  0.0256, -0.0128]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0114,  0.0123,  0.0176,  ...,  0.0923, -0.0228, -0.0109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0066,  0.0376, -0.0386,  ...,  0.0088,  0.0190, -0.0117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0057,  0.0776, -0.0986,  ...,  0.0039,  0.0255,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0771,  0.0471, -0.1216,  ..., -0.0425,  0.0420,  0.0165]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0474,  0.0454, -0.0859,  ...,  0.0190,  0.0513,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0010,  0.0732, -0.0845,  ..., -0.0713,  0.0615,  0.1357]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0077,  0.1777, -0.0674,  ..., -0.0801,  0.0393,  0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0493,  0.1211, -0.0977,  ..., -0.1001,  0.0933,  0.0088]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0060,  0.1445, -0.1064,  ..., -0.0635,  0.0386,  0.0688]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0967,  0.1709, -0.1250,  ..., -0.0762,  0.0388,  0.0254]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0078,  0.1113, -0.0117,  ..., -0.1045,  0.0037,  0.0171]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0688,  0.1572,  0.0391,  ..., -0.0486,  0.0791,  0.1191]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0393,  0.2109, -0.0530,  ..., -0.0393, -0.0254,  0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0166,  0.2695, -0.0762,  ..., -0.0452, -0.0166,  0.1592]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0630,  0.1582,  0.0522,  ..., -0.0835, -0.0630,  0.1221]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2031,  0.1982, -0.0486,  ..., -0.0977, -0.0176,  0.1953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0352,  0.2500, -0.1216,  ..., -0.0244, -0.1543,  0.0156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2090,  0.3047, -0.0928,  ..., -0.0474,  0.0176, -0.0134]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2188,  0.2656, -0.1777,  ..., -0.0957,  0.0420, -0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0654,  0.2852, -0.2988,  ..., -0.2148,  0.0513, -0.0618]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0215,  0.2617, -0.2236,  ..., -0.2500, -0.0654, -0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1475,  0.4023, -0.1465,  ..., -0.0566, -0.0879, -0.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1855,  0.6055, -0.0557,  ...,  0.1689, -0.0168, -0.5703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1602,  0.6367, -0.1641,  ...,  0.1123,  0.0137, -0.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0068,  0.6914, -0.1621,  ...,  0.0737,  0.1699, -0.6680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0435,  0.9688, -0.2773,  ...,  0.1289,  0.0850, -0.8320]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1562,  0.8789, -0.2500,  ...,  0.1816,  0.1562, -0.9922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1201,  0.7812, -0.1992,  ...,  0.0342,  0.2090, -1.0859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1089,  0.9297, -0.2168,  ..., -0.0530,  0.2441, -1.1797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3535,  0.9336, -0.2002,  ..., -0.0283,  0.2676, -1.3281]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5391,  0.5977, -0.1680,  ..., -0.0850,  0.4570, -1.3516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5391,  0.5703, -0.0176,  ...,  0.0791,  0.3301, -1.3672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8828,  0.2344, -0.3848,  ..., -0.2617,  0.4414, -1.6250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.9922,  0.0078, -0.5000,  ...,  0.1904,  0.2188, -1.5625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3438, -0.0498, -0.6211,  ...,  0.6602, -1.0312, -0.8984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0094, -0.0025,  0.0030,  ..., -0.0046,  0.0347,  0.0031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0110,  0.0015, -0.0011,  ..., -0.0070,  0.0234,  0.0132]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0195,  0.0226,  0.0125,  ...,  0.0223,  0.0513,  0.0554]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0004,  0.0215,  0.0177,  ...,  0.0396,  0.0505,  0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0091, -0.0417, -0.0233,  ...,  0.0620,  0.0820,  0.0718]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0610, -0.0311,  0.0010,  ...,  0.1230,  0.0811,  0.1040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0466, -0.0496, -0.0332,  ...,  0.0537,  0.1108,  0.1240]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0002, -0.0957, -0.0459,  ...,  0.0664,  0.1021,  0.1152]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0879, -0.0967, -0.0371,  ...,  0.0542,  0.1089,  0.1074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0549, -0.0254,  0.0381,  ...,  0.0664,  0.0464,  0.2041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0173, -0.0086,  0.0247,  ...,  0.0352,  0.0747,  0.1885]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0248, 0.0425, 0.0408,  ..., 0.0138, 0.0347, 0.2148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0596,  0.0284,  0.0330,  ..., -0.0016,  0.0063,  0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0283,  0.0371,  0.0005,  ...,  0.0315, -0.0188,  0.2275]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0049,  0.0029, -0.0535,  ...,  0.0654,  0.0192,  0.1611]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0518,  0.0005,  0.0679,  ...,  0.0947,  0.0113,  0.0942]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0811,  0.0245,  0.1367,  ...,  0.0840,  0.1094,  0.0996]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.1094, 0.0194, 0.0928,  ..., 0.0137, 0.0503, 0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[0.1855, 0.0276, 0.1055,  ..., 0.0513, 0.0693, 0.1484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.1504, 0.0435, 0.1846,  ..., 0.0258, 0.0361, 0.1611]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 3.2422e-01,  8.3496e-02,  5.3711e-02,  ..., -2.6703e-04,\n",
      "           4.4678e-02,  1.8457e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2070,  0.2129,  0.0874,  ...,  0.0210, -0.2246, -0.0088]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0518,  0.1807,  0.0503,  ...,  0.0659, -0.1191, -0.0315]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0747,  0.2500,  0.0322,  ..., -0.0010, -0.0615, -0.0776]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2520,  0.3145, -0.0146,  ..., -0.2432, -0.1357, -0.0330]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3906,  0.2930,  0.0015,  ..., -0.2324, -0.1982, -0.1855]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3496,  0.5000, -0.0615,  ..., -0.1484, -0.2383, -0.3848]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4531,  0.6953,  0.0879,  ..., -0.1826, -0.1689, -0.1904]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.6094,  0.7617,  0.0195,  ..., -0.2441, -0.1299, -0.2773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.8281,  0.8242,  0.2891,  ..., -0.3340, -0.1758, -0.1475]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.8125,  0.8086,  0.2236,  ..., -0.2383, -0.1621, -0.3027]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.7266,  0.9609,  0.1982,  ..., -0.2197, -0.4336, -0.4180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6250,  0.9414,  0.3340,  ..., -0.2314, -0.3262, -0.3242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.5117,  1.1250,  0.1797,  ..., -0.3262, -0.3906, -0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3867,  1.0000,  0.2090,  ..., -0.4238, -0.5078, -0.5312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2285,  0.8672,  0.1475,  ..., -0.4648, -0.4941, -0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1562,  0.5859,  0.4180,  ..., -0.3379, -0.7812, -0.5352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3125,  0.5547,  0.3086,  ..., -0.6875, -0.6836, -1.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5391,  0.0312,  0.2676,  ..., -0.3555, -0.7930, -0.8984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8594,  0.1426, -0.0566,  ...,  0.1387, -2.5625, -0.5039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0135,  0.0044,  0.0210,  ..., -0.0074, -0.0044, -0.0037]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0128, -0.0046,  0.0449,  ...,  0.0021, -0.0159, -0.0298]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0070,  0.0227,  0.0334,  ...,  0.0540, -0.0044, -0.0120]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0121,  0.0096,  0.0171,  ...,  0.0493, -0.0430, -0.0089]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0089,  0.0488, -0.0215,  ...,  0.0425, -0.0173, -0.0281]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0549,  0.0598, -0.0131,  ...,  0.1436, -0.0055, -0.0292]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0312,  0.0776, -0.0447,  ...,  0.0552,  0.0093, -0.0625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0378,  0.0020, -0.0112,  ...,  0.0654, -0.0056, -0.0227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1006, -0.0127,  0.0255,  ...,  0.0625, -0.0271, -0.0135]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0933,  0.0070,  0.0386,  ...,  0.1289, -0.0991,  0.0410]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0520,  0.0210,  0.0166,  ...,  0.1060, -0.0752,  0.0020]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0522,  0.0972,  0.1016,  ...,  0.0767, -0.0918,  0.0173]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0459,  0.0840,  0.0732,  ...,  0.0459, -0.0938,  0.0076]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0247,  0.0496,  0.0486,  ...,  0.0260, -0.1162,  0.1162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0079,  0.0801,  0.0029,  ...,  0.0664, -0.0125,  0.0447]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0051,  0.0928,  0.1123,  ...,  0.0869, -0.0698,  0.0093]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0128, -0.0029,  0.1895,  ...,  0.0991, -0.0430,  0.0075]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1094,  0.0139,  0.1416,  ...,  0.0649, -0.0557,  0.0535]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1240, -0.0349,  0.1641,  ...,  0.1045, -0.0220,  0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1055, -0.0879,  0.3086,  ...,  0.0449, -0.1201,  0.1113]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3555, -0.0347,  0.1660,  ...,  0.0586, -0.2266,  0.1250]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1875,  0.1157,  0.2617,  ...,  0.0762, -0.5078,  0.0571]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1875,  0.1562,  0.2090,  ..., -0.0830, -0.2617, -0.1152]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1885,  0.1670,  0.1523,  ..., -0.0776, -0.1934, -0.0513]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3711,  0.1211,  0.0742,  ..., -0.2109, -0.2695, -0.0776]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4863,  0.0957,  0.1523,  ..., -0.1035, -0.3125, -0.2090]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3672,  0.1914, -0.0332,  ..., -0.2432, -0.3262, -0.5781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4766,  0.2158,  0.1621,  ..., -0.1201, -0.4961, -0.6914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.6797,  0.4004,  0.0017,  ..., -0.1436, -0.4219, -0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.8320,  0.3535,  0.2002,  ..., -0.2852, -0.5078, -0.6094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.8516,  0.3887, -0.0469,  ..., -0.2363, -0.6250, -0.8047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5781,  0.6406,  0.0212,  ..., -0.4375, -0.7227, -0.8008]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5078,  0.7812,  0.1396,  ..., -0.5508, -0.7578, -0.8672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.5703,  0.9922,  0.0334,  ..., -0.3262, -0.7617, -0.8164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3301,  1.1328,  0.2930,  ..., -0.4297, -0.8477, -0.8203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1543,  1.1016,  0.2422,  ..., -0.3848, -0.9531, -0.8359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2520,  0.9805,  0.2227,  ..., -0.2676, -1.0469, -0.7578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2305,  1.0781, -0.0527,  ..., -0.2617, -1.1094, -1.1953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.3945,  0.4062, -0.0098,  ..., -0.0327, -1.6719, -1.2031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.0938,  1.1484, -0.0486,  ...,  0.1855, -4.0000, -0.8125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0035, -0.0081,  0.0038,  ..., -0.0281, -0.0033, -0.0036]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0107, -0.0194, -0.0012,  ..., -0.0728, -0.0182, -0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0041,  0.0020, -0.0005,  ..., -0.0732, -0.0015,  0.0073]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0055,  0.0101,  0.0027,  ..., -0.0869,  0.0050,  0.0291]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0327, -0.0048, -0.0347,  ..., -0.0549,  0.0598,  0.0173]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0190,  0.0713, -0.0134,  ...,  0.0034, -0.0364,  0.0684]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0457,  0.1270,  0.0112,  ...,  0.0330,  0.0603, -0.0225]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0654,  0.0532,  0.0776,  ..., -0.0176,  0.0835,  0.0040]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0923,  0.0596,  0.1006,  ..., -0.0041,  0.0845, -0.0148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.1143, 0.0339, 0.0718,  ..., 0.0698, 0.0151, 0.0737]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0537,  0.1016,  0.0320,  ...,  0.0225, -0.0133,  0.0447]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0996,  0.0869,  0.0859,  ...,  0.0095, -0.0522,  0.0625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0649,  0.0486,  0.0332,  ...,  0.0264,  0.0112, -0.0083]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1016,  0.0262, -0.0166,  ...,  0.0090, -0.0425,  0.0430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0684,  0.0300, -0.0781,  ...,  0.0425, -0.0299, -0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0203,  0.0344,  0.0063,  ...,  0.0615, -0.0454, -0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0408,  0.0251,  0.0879,  ...,  0.0928,  0.0012, -0.1001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0359,  0.0566,  0.0864,  ...,  0.0591,  0.0510, -0.0444]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[0.0378, 0.0718, 0.1729,  ..., 0.0552, 0.1030, 0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1406,  0.0635,  0.2324,  ..., -0.0081,  0.0620,  0.0270]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1904,  0.1455,  0.0322,  ...,  0.0154, -0.0100, -0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0068,  0.1094,  0.1064,  ...,  0.1235, -0.1680, -0.0557]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0664,  0.1021,  0.1328,  ..., -0.0117,  0.1045, -0.1914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1191,  0.1055,  0.1021,  ...,  0.0190,  0.2139, -0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2461,  0.2178,  0.1318,  ...,  0.0811,  0.0205, -0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3281,  0.1807,  0.1855,  ..., -0.0054,  0.0116, -0.2871]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0742,  0.0703, -0.0391,  ...,  0.0300,  0.2119, -0.4844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 2.4414e-01,  2.4707e-01, -2.4414e-04,  ...,  1.1523e-01,\n",
      "           2.1094e-01, -4.7852e-01]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3301,  0.3750, -0.0278,  ...,  0.2090,  0.3379, -0.4473]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2773,  0.3301, -0.2305,  ...,  0.1533,  0.1914, -0.4805]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3359,  0.3086, -0.4883,  ...,  0.1377,  0.1348, -0.7383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2246,  0.3809, -0.6172,  ..., -0.1240,  0.2539, -0.6953]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0000,  0.4902, -0.4609,  ..., -0.0786,  0.3574, -0.6406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0193,  0.6055, -0.5898,  ..., -0.0537,  0.6016, -0.5625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4961,  0.8047, -0.5430,  ..., -0.1147,  0.7070, -0.8125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5273,  0.6758, -0.6484,  ..., -0.1621,  0.7812, -0.9180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6641,  0.4336, -0.3223,  ..., -0.2031,  0.8945, -1.3906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1328,  0.4668, -0.8906,  ..., -0.2559,  1.3828, -1.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.1250, -0.0781, -0.7812,  ...,  0.2969,  1.1094, -1.2500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7227,  1.3203, -1.0156,  ..., -0.0938,  0.6797, -2.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0096, -0.0065, -0.0071,  ..., -0.0243, -0.0110,  0.0088]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0083,  0.0015, -0.0010,  ..., -0.0554, -0.0167,  0.0125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0087,  0.0183, -0.0061,  ..., -0.0654, -0.0023, -0.0015]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0039,  0.0371,  0.0238,  ..., -0.1177,  0.0041,  0.0154]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0293,  0.0310,  0.0403,  ..., -0.1367,  0.0200,  0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0371,  0.0820,  0.0097,  ..., -0.0986, -0.0474,  0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0120,  0.0781,  0.0017,  ..., -0.0830,  0.0049, -0.0044]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0234,  0.0378,  0.0454,  ..., -0.0972,  0.0139,  0.0371]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0183,  0.0664,  0.0620,  ..., -0.1035,  0.0245,  0.0525]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0288,  0.1191,  0.0991,  ..., -0.0664,  0.0486,  0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0297,  0.0791,  0.0928,  ..., -0.0820,  0.0425,  0.0547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0001,  0.0605,  0.1211,  ..., -0.0977,  0.0742,  0.0613]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0288,  0.0532,  0.0098,  ..., -0.0444,  0.0879,  0.0693]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0571,  0.0564,  0.0011,  ..., -0.0352,  0.0464,  0.1514]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0454,  0.0669,  0.0229,  ..., -0.0175,  0.0635,  0.0603]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0250,  0.1230,  0.0625,  ...,  0.0222,  0.0325, -0.0117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0024,  0.0684,  0.1279,  ...,  0.0598,  0.0378, -0.0103]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0244,  0.0815,  0.1348,  ...,  0.0010,  0.0698, -0.0206]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1270,  0.1060,  0.1709,  ..., -0.0674,  0.0928,  0.0801]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2012,  0.0645,  0.2734,  ..., -0.0977,  0.0811,  0.0356]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1582,  0.1494,  0.1309,  ..., -0.1309, -0.0168,  0.0281]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0176,  0.1553,  0.1426,  ..., -0.0938, -0.2236, -0.1436]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0620,  0.1348,  0.0869,  ..., -0.1689,  0.0693, -0.1553]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0181,  0.1719,  0.0957,  ..., -0.2246,  0.1621, -0.2168]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0481,  0.3008,  0.0698,  ..., -0.1738,  0.0200, -0.3496]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0513,  0.3320,  0.1206,  ..., -0.1914,  0.0977, -0.4551]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0024,  0.4023, -0.0938,  ..., -0.1660,  0.1172, -0.7461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0277,  0.3691,  0.0469,  ...,  0.1035,  0.1670, -0.8594]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0200,  0.5000,  0.0835,  ...,  0.1377,  0.2637, -0.7695]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1289,  0.4766, -0.0654,  ...,  0.0874,  0.3340, -0.7969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1089,  0.6094, -0.2324,  ...,  0.1689,  0.3574, -1.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0442,  0.7109, -0.2422,  ...,  0.0303,  0.4121, -1.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1211,  0.7109, -0.1504,  ..., -0.0061,  0.6641, -0.9922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0693,  0.8945, -0.1025,  ..., -0.1582,  0.7422, -1.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4980,  1.0469,  0.1187,  ..., -0.3457,  0.6680, -1.2500]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3750,  0.9375,  0.1680,  ..., -0.4414,  0.6328, -1.2422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2129,  0.7461,  0.3398,  ..., -0.5508,  0.6406, -1.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5391,  0.4902, -0.0098,  ..., -0.6680,  0.8203, -1.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4961,  0.1562, -0.0244,  ..., -0.3516,  0.5977, -1.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1504,  0.6562,  0.7227,  ..., -0.2715,  0.4336, -1.0938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0187, -0.0145, -0.0164,  ..., -0.0154, -0.0193,  0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0154, -0.0305, -0.0303,  ..., -0.0518, -0.0034,  0.0167]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0107, -0.0288, -0.0312,  ..., -0.0244, -0.0012,  0.0047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0199, -0.0398, -0.0508,  ..., -0.0325, -0.0076, -0.0126]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0193, -0.0664, -0.0742,  ..., -0.0654,  0.0023, -0.0544]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0674, -0.0508, -0.0635,  ..., -0.0173, -0.0309,  0.0073]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0410, -0.0474, -0.1045,  ..., -0.0010, -0.0306, -0.0244]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0283, -0.0608,  0.0078,  ...,  0.0522, -0.0347,  0.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0121, -0.0381,  0.0264,  ...,  0.0820, -0.0649,  0.0070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0265,  0.0186,  0.0532,  ...,  0.0142, -0.0723,  0.0552]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0275,  0.0047,  0.1318,  ...,  0.0762, -0.0830, -0.0840]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0322, -0.0120,  0.1982,  ..., -0.0063,  0.0210,  0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0128,  0.0260,  0.1035,  ..., -0.0113,  0.1016,  0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0381,  0.0056,  0.1191,  ..., -0.0225,  0.0977,  0.0786]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0315, 0.0078, 0.0620,  ..., 0.0275, 0.0840, 0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.1177, 0.0679, 0.0620,  ..., 0.0449, 0.0352, 0.0620]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1787,  0.0181,  0.0598,  ...,  0.0513, -0.0591,  0.0889]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3223,  0.0129,  0.1396,  ..., -0.0461, -0.0918,  0.0503]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3066, -0.0713,  0.1128,  ..., -0.0613, -0.1328,  0.0479]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3145, -0.0066,  0.1855,  ..., -0.1206, -0.1445,  0.0571]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4961, -0.1147,  0.0933,  ..., -0.1865, -0.1973, -0.0042]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2988,  0.0181,  0.1992,  ..., -0.0654, -0.3340, -0.1230]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1914, -0.0554,  0.2695,  ..., -0.1060, -0.0508, -0.3711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1406,  0.1143,  0.2969,  ..., -0.1758, -0.0195, -0.3730]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1738, -0.0205,  0.2832,  ..., -0.2334, -0.1123, -0.3633]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1602, -0.1064,  0.2695,  ..., -0.3535, -0.1377, -0.4805]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1162, -0.0684,  0.0908,  ..., -0.3379, -0.0342, -0.6523]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0684,  0.0547,  0.3359,  ..., -0.0381, -0.1079, -0.6992]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1895,  0.2734,  0.2520,  ..., -0.0525, -0.0388, -0.6797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3633,  0.3574,  0.1709,  ..., -0.1992,  0.0059, -0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1553,  0.4336,  0.1030,  ..., -0.2012, -0.1699, -0.7461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2422,  0.5469,  0.1230,  ..., -0.2598, -0.0688, -0.7773]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0391,  0.4609,  0.2988,  ..., -0.2480,  0.0234, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0996,  0.7422,  0.2598,  ..., -0.1582,  0.1689, -0.7148]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3242,  0.7695,  0.5078,  ..., -0.1436,  0.2217, -0.8516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4004,  0.5898,  0.5430,  ..., -0.1914,  0.5234, -0.9375]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1982,  0.5625,  0.6484,  ..., -0.2148,  0.4902, -0.9609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5156,  0.6250,  0.5117,  ..., -0.2852,  1.0000, -1.2031]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.7031,  0.3359,  0.7773,  ..., -0.4160,  0.5039, -1.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1875,  1.2891,  0.7656,  ..., -0.9453, -0.6836, -2.2344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0021,  0.0017,  0.0048,  ...,  0.0027, -0.0020,  0.0004]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0060,  0.0060, -0.0096,  ...,  0.0069, -0.0104, -0.0019]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0073,  0.0222, -0.0023,  ...,  0.0146, -0.0186, -0.0007]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0031,  0.0015, -0.0238,  ..., -0.0393, -0.0110, -0.0310]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0208,  0.0179, -0.0211,  ..., -0.0176,  0.0101, -0.0427]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0255,  0.0176,  0.0007,  ...,  0.0552,  0.0242, -0.0006]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0302,  0.0042, -0.0305,  ...,  0.0693,  0.0381, -0.0518]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0137,  0.0457, -0.0029,  ...,  0.0986,  0.0820,  0.0193]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0145,  0.0396,  0.0120,  ...,  0.0908,  0.0962, -0.0344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0420, 0.0366, 0.0352,  ..., 0.1118, 0.0552, 0.0718]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0054,  0.0635,  0.0183,  ...,  0.0332, -0.0015,  0.0322]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0133,  0.1445,  0.0151,  ...,  0.0850, -0.0041,  0.1270]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0483,  0.0791, -0.0791,  ...,  0.0359,  0.0272,  0.1167]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0913,  0.1123, -0.0835,  ...,  0.0312, -0.0718,  0.1836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0083,  0.0947, -0.1084,  ...,  0.0226, -0.0752,  0.0845]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0303,  0.0024, -0.0098,  ..., -0.0073,  0.0430,  0.1816]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0432, -0.0723,  0.0972,  ..., -0.0171,  0.0223,  0.2559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0388,  0.0747,  0.1523,  ..., -0.1592,  0.0962,  0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0139,  0.1309,  0.2197,  ..., -0.1387,  0.0854,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0664,  0.0117,  0.3965,  ..., -0.2402, -0.0933,  0.0884]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1050, -0.1201,  0.4082,  ..., -0.3398, -0.0186,  0.1846]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0977,  0.0327,  0.3809,  ..., -0.2002, -0.2158, -0.0620]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1924, -0.0928,  0.4375,  ..., -0.2129, -0.0635, -0.1611]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1011, -0.0400,  0.4727,  ..., -0.2197,  0.0566, -0.3203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1543, -0.0527,  0.4512,  ..., -0.3262,  0.0625, -0.2715]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3125, -0.0693,  0.4746,  ..., -0.3516, -0.0029, -0.3457]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4141, -0.1953,  0.3242,  ..., -0.2930,  0.1562, -0.5586]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3574, -0.0107,  0.3125,  ..., -0.2441,  0.1182, -0.5781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2168,  0.0762,  0.2012,  ..., -0.1152,  0.2539, -0.6719]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1074,  0.0649,  0.1787,  ..., -0.2314,  0.3965, -0.4883]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1040,  0.1680,  0.1235,  ..., -0.1338,  0.2988, -0.6289]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0564,  0.1729,  0.2275,  ..., -0.1611,  0.4727, -0.7539]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2178,  0.0249,  0.1133,  ..., -0.2139,  0.4727, -0.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0352,  0.3457,  0.1240,  ...,  0.1543,  0.5898, -0.7305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2363,  0.3555,  0.1592,  ...,  0.1079,  0.4844, -0.7969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1562,  0.0352,  0.1426,  ...,  0.0771,  0.5508, -0.7344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0566, -0.0596,  0.3281,  ..., -0.1055,  0.4160, -1.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4219, -0.6016, -0.0566,  ..., -0.1221,  0.7930, -1.4375]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6211, -1.1094,  0.0352,  ...,  0.2832,  0.9102, -1.3438]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3711, -0.1719, -1.2266,  ...,  0.4727,  0.8984, -1.8281]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0019,  0.0100, -0.0071,  ..., -0.0249, -0.0121, -0.0028]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0030, -0.0033, -0.0063,  ..., -0.0339, -0.0059, -0.0032]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0195, -0.0008,  0.0237,  ..., -0.0728, -0.0200, -0.0228]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0127, -0.0013, -0.0145,  ..., -0.1230, -0.0376, -0.0027]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0273, -0.0139, -0.0133,  ..., -0.1196,  0.0011, -0.0195]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0076,  0.0101,  0.0317,  ..., -0.0552, -0.0176,  0.0117]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0121,  0.0053, -0.0142,  ..., -0.0444,  0.0383, -0.0300]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0179,  0.0103, -0.0013,  ..., -0.0669,  0.0459,  0.0039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0840,  0.0913, -0.0160,  ..., -0.0288,  0.1318, -0.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0059,  0.0859,  0.0571,  ..., -0.0273,  0.0752,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0259,  0.0432,  0.1064,  ..., -0.0232,  0.0059, -0.0376]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0267,  0.0845,  0.1113,  ..., -0.1006,  0.0415, -0.0265]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0311,  0.1484,  0.0664,  ..., -0.0601,  0.0786, -0.0361]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0830,  0.1738,  0.0068,  ..., -0.0271,  0.0781,  0.0322]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0183,  0.2012,  0.0386,  ..., -0.1050,  0.0669, -0.0425]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0044,  0.1484,  0.0493,  ..., -0.0938, -0.0186,  0.1030]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0439,  0.0947,  0.0698,  ...,  0.0137,  0.0244,  0.1060]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0093,  0.1011,  0.0184,  ..., -0.1206, -0.0210, -0.0537]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0728,  0.1328,  0.0137,  ..., -0.1108, -0.0967,  0.0298]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0190,  0.0464,  0.1826,  ..., -0.1602, -0.1118,  0.0757]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1348, -0.0535,  0.1484,  ..., -0.1914, -0.0928,  0.1562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1128,  0.0732,  0.2539,  ..., -0.0400, -0.2285, -0.1099]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0371,  0.1836,  0.3066,  ..., -0.1235, -0.1719, -0.2988]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0483,  0.1621,  0.4180,  ..., -0.2148, -0.1289, -0.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0146,  0.0869,  0.2314,  ..., -0.2988,  0.0112, -0.4297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0889,  0.0276,  0.4219,  ..., -0.3242, -0.1328, -0.5742]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0312, -0.0811,  0.3555,  ..., -0.2500,  0.1445, -0.7109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0080,  0.1289,  0.2734,  ..., -0.2246,  0.2930, -0.6562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1416,  0.3477,  0.2256,  ..., -0.1943,  0.2930, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1475,  0.4004,  0.4023,  ..., -0.2275,  0.4199, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1582,  0.6055,  0.3145,  ..., -0.2471,  0.4180, -0.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1602,  0.5859,  0.4180,  ..., -0.3262,  0.5234, -0.7109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0425,  0.6562,  0.5117,  ..., -0.3379,  0.6953, -0.6406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0254,  0.8867,  0.4434,  ..., -0.2051,  0.7500, -0.5547]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0518,  0.7891,  0.4688,  ..., -0.3105,  0.7734, -0.3672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2402,  0.6875,  0.7344,  ..., -0.2266,  0.8867, -0.4180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3750,  0.7773,  0.9844,  ...,  0.1738,  1.0625, -0.4648]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5586,  0.7539,  0.6523,  ..., -0.2100,  1.0469, -1.0781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.4004,  0.5547,  0.3750,  ...,  0.5898,  1.4844, -1.2969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.7578,  1.6875,  0.1641,  ...,  0.3398, -0.4688,  0.4453]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0027,  0.0041,  0.0004,  ...,  0.0022, -0.0125,  0.0134]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0119,  0.0143, -0.0049,  ..., -0.0051, -0.0184, -0.0036]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0156,  0.0104, -0.0032,  ..., -0.0300, -0.0266,  0.0013]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0153,  0.0347, -0.0352,  ..., -0.0188, -0.0325, -0.0081]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0089,  0.0225, -0.0273,  ..., -0.0217,  0.0298, -0.0396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0347,  0.0410,  0.0092,  ...,  0.0035, -0.0129, -0.0129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0192,  0.0520,  0.0186,  ..., -0.0062,  0.0192, -0.0569]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0295,  0.0615, -0.0006,  ...,  0.0361,  0.0596, -0.0322]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0938,  0.0542, -0.0056,  ...,  0.0105,  0.0654,  0.0024]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0293, 0.0698, 0.0182,  ..., 0.0374, 0.0645, 0.0222]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0111,  0.1089,  0.0222,  ...,  0.0007,  0.0679, -0.0286]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0027,  0.1943,  0.0376,  ..., -0.0654,  0.0110, -0.0332]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0566,  0.2041, -0.0020,  ..., -0.0466,  0.0537, -0.1104]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0830,  0.2012, -0.0703,  ...,  0.0015,  0.0908, -0.0908]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0192,  0.2109, -0.0432,  ..., -0.0249,  0.0967, -0.1240]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0327,  0.1611,  0.0151,  ...,  0.0033,  0.0083, -0.0540]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0466,  0.1279,  0.1138,  ...,  0.1167,  0.0120,  0.0986]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 2.6855e-03,  1.9824e-01, -1.2207e-04,  ...,  4.1260e-02,\n",
      "           1.3367e-02, -8.3984e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1245,  0.2285, -0.0129,  ...,  0.1216, -0.0073,  0.0107]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0449,  0.2129,  0.0991,  ...,  0.1035, -0.0508,  0.0422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0903,  0.1318,  0.1309,  ..., -0.0175,  0.0405,  0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0195,  0.2266,  0.2402,  ...,  0.0884, -0.1533, -0.1914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1855,  0.3359,  0.3203,  ...,  0.0391, -0.0293, -0.2559]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1885,  0.3730,  0.3809,  ...,  0.0767,  0.0442, -0.2812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1963,  0.3066,  0.3320,  ..., -0.0010,  0.1201, -0.2578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2168,  0.4180,  0.4160,  ...,  0.1270,  0.1079, -0.5430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3340,  0.3691,  0.4316,  ...,  0.1025,  0.1206, -0.8555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3340,  0.4258,  0.4941,  ...,  0.3633,  0.1807, -0.8555]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2676,  0.7266,  0.3555,  ...,  0.3906,  0.2129, -0.9336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2832,  0.8398,  0.4336,  ...,  0.3789,  0.3066, -1.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2500,  0.8906,  0.3848,  ...,  0.4473,  0.1748, -1.3984]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1602,  1.0312,  0.3398,  ...,  0.5625,  0.2402, -1.6172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2256,  0.9180,  0.3770,  ...,  0.5664,  0.4102, -1.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2324,  1.2188,  0.1797,  ...,  0.6445,  0.3477, -1.5625]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4531,  1.2422,  0.2207,  ...,  0.5469,  0.2598, -1.7578]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6211,  1.0625,  0.3164,  ...,  0.3477,  0.1953, -1.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.8047,  0.9062,  0.4180,  ...,  0.6328,  0.2852, -1.8906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1953,  0.6250,  0.1943,  ...,  0.4609,  0.4336, -2.2969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.2500, -0.0156,  0.3281,  ...,  0.6953,  0.2031, -2.3906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4648, -0.1797, -0.9453,  ...,  0.1172, -1.9219, -2.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0037,  0.0012,  0.0177,  ..., -0.0052,  0.0022,  0.0078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0118, -0.0034,  0.0405,  ..., -0.0131, -0.0026, -0.0176]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0038,  0.0072,  0.0304,  ...,  0.0254,  0.0131, -0.0347]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0211,  0.0276,  0.0267,  ...,  0.0226,  0.0024, -0.0226]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0258, -0.0051,  0.0109,  ...,  0.0088,  0.0258, -0.0228]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0229,  0.0146, -0.0129,  ...,  0.0957, -0.0400, -0.0001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0168,  0.0154, -0.0586,  ...,  0.0015, -0.0254, -0.0136]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0183,  0.0488, -0.0918,  ..., -0.0183, -0.0262,  0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0991,  0.0420, -0.0786,  ..., -0.0125, -0.0425,  0.0131]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1055,  0.1328, -0.0796,  ...,  0.0383, -0.0500,  0.1201]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0752,  0.2061, -0.1328,  ..., -0.0549, -0.0515,  0.0806]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0801,  0.2461, -0.0444,  ..., -0.1201, -0.0845,  0.0415]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0068,  0.1816, -0.0835,  ..., -0.1226, -0.0383, -0.0488]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0018,  0.1465, -0.0938,  ..., -0.1035, -0.0371,  0.0068]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1196,  0.1729, -0.1289,  ..., -0.0466, -0.0315, -0.0581]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0483,  0.1113, -0.0483,  ..., -0.0630, -0.1270,  0.0029]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0356,  0.1465, -0.0293,  ..., -0.0245, -0.0195,  0.0820]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0272,  0.2422, -0.1367,  ...,  0.0457, -0.0630,  0.0039]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0159,  0.3125, -0.1572,  ...,  0.0601, -0.0588,  0.1279]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0693,  0.2500,  0.0479,  ...,  0.0396, -0.1055,  0.1504]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1172,  0.1279, -0.1270,  ..., -0.0625, -0.0137,  0.1709]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0605,  0.2539, -0.0908,  ..., -0.0271, -0.1895, -0.0566]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2539,  0.3047, -0.0255,  ..., -0.0620, -0.0732, -0.1060]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2422,  0.2656, -0.0518,  ..., -0.0840,  0.0281, -0.0684]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0869,  0.2578, -0.1201,  ..., -0.2363,  0.0640, -0.1001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0781,  0.2363, -0.0559,  ..., -0.2295,  0.0366, -0.2461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1973,  0.2949, -0.0029,  ..., -0.0293, -0.0486, -0.5703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2461,  0.5859,  0.1494,  ...,  0.1953, -0.0125, -0.5312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2344,  0.5781,  0.0649,  ...,  0.1562,  0.0212, -0.5703]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0039,  0.6562, -0.0073,  ...,  0.0840,  0.2422, -0.6055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1055,  0.9141, -0.1523,  ...,  0.2012,  0.1699, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1602,  0.8125, -0.1060,  ...,  0.2275,  0.2168, -0.8672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0723,  0.7383, -0.0947,  ...,  0.1680,  0.2314, -0.9023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0378,  0.8828, -0.0527,  ...,  0.0947,  0.2734, -0.9180]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2383,  0.8242, -0.0253,  ...,  0.0811,  0.2891, -1.0391]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3750,  0.5000,  0.0159,  ..., -0.0103,  0.5547, -1.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3418,  0.5117,  0.2334,  ...,  0.1855,  0.4707, -1.2109]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.6992,  0.2891,  0.0029,  ..., -0.0186,  0.5781, -1.2656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8984,  0.0156, -0.0713,  ...,  0.5469,  0.4180, -1.1172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1885,  0.0442, -0.1631,  ...,  1.0547, -0.3828, -0.6406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0092, -0.0033,  0.0028,  ..., -0.0056,  0.0337,  0.0022]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0070,  0.0031, -0.0060,  ..., -0.0048,  0.0210,  0.0058]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0135,  0.0031,  0.0003,  ...,  0.0132,  0.0547,  0.0325]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-9.1553e-05,  6.1035e-05,  5.4932e-03,  ...,  2.1851e-02,\n",
      "           4.7607e-02,  3.2471e-02]]], device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0110, -0.0613, -0.0408,  ...,  0.0322,  0.0918,  0.0245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0557, -0.0684, -0.0457,  ...,  0.1016,  0.0928,  0.0791]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0264, -0.0703, -0.0461,  ...,  0.0781,  0.1006,  0.0708]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0211, -0.1030, -0.0786,  ...,  0.0454,  0.0981,  0.0369]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1030, -0.1133, -0.0623,  ...,  0.0444,  0.0869,  0.0413]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.0703, 0.0132, 0.0078,  ..., 0.0386, 0.0190, 0.1699]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0146,  0.0996, -0.0361,  ..., -0.0125, -0.0275,  0.1465]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0488,  0.0908,  0.0400,  ..., -0.0518, -0.0332,  0.1245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0515,  0.0195,  0.0515,  ..., -0.0737, -0.0111,  0.1387]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1094, -0.0312,  0.0449,  ..., -0.0630, -0.0195,  0.1914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0244, -0.0094, -0.0154,  ..., -0.0723, -0.0054,  0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0576,  0.0664, -0.0518,  ..., -0.0093, -0.0315,  0.1680]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1602,  0.0801,  0.0283,  ...,  0.0021,  0.0430,  0.1318]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0513,  0.1279, -0.0684,  ..., -0.0190,  0.0444,  0.1445]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1074,  0.0918, -0.0762,  ...,  0.0038,  0.0347,  0.1855]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1963,  0.1113,  0.0947,  ...,  0.0017, -0.0280,  0.2715]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3477,  0.0425, -0.0444,  ..., -0.1562,  0.0205,  0.2061]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2354,  0.1855,  0.0129,  ..., -0.1050, -0.2070,  0.0361]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0596,  0.1699, -0.0266,  ..., -0.0752, -0.1035, -0.0086]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0991,  0.2256, -0.1069,  ..., -0.1060, -0.0107, -0.0184]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1953,  0.3379, -0.0620,  ..., -0.3008, -0.0371,  0.0205]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3457,  0.3633, -0.0801,  ..., -0.3320, -0.0713, -0.2070]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2812,  0.5234, -0.1123,  ..., -0.3047, -0.1309, -0.4023]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3926,  0.7812,  0.1108,  ..., -0.2949, -0.0820, -0.1367]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.4746,  0.8164,  0.0393,  ..., -0.3418, -0.0310, -0.2256]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.6758,  0.8711,  0.2363,  ..., -0.3750, -0.0095, -0.1602]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.5625,  0.9414,  0.1943,  ..., -0.2969, -0.0898, -0.2871]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4902,  1.0156,  0.1040,  ..., -0.2188, -0.2812, -0.3770]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4121,  1.0469,  0.2363,  ..., -0.1973, -0.1768, -0.2734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.3789,  1.2188,  0.1621,  ..., -0.2402, -0.1846, -0.4277]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2480,  1.0312,  0.2334,  ..., -0.3750, -0.1934, -0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0312,  0.9453,  0.2871,  ..., -0.4141, -0.1689, -0.5938]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0386,  0.7930,  0.6016,  ..., -0.3262, -0.4199, -0.6055]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3750,  0.6719,  0.4375,  ..., -0.5469, -0.3379, -1.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.5742,  0.2266,  0.3555,  ..., -0.1855, -0.4473, -0.8203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7578,  0.4453,  0.1787,  ...,  0.3281, -2.0156, -0.4863]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0135,  0.0046,  0.0210,  ..., -0.0071, -0.0049, -0.0045]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0170, -0.0042,  0.0415,  ...,  0.0039, -0.0189, -0.0337]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0022, -0.0054,  0.0265,  ...,  0.0403,  0.0008, -0.0299]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0126, -0.0130,  0.0120,  ...,  0.0386, -0.0339, -0.0276]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0145,  0.0200, -0.0354,  ...,  0.0327,  0.0020, -0.0674]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0317, -0.0010, -0.0742,  ...,  0.1406, -0.0222, -0.0488]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0137,  0.0179, -0.0811,  ...,  0.1221, -0.0244, -0.0977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0532, -0.0094, -0.0737,  ...,  0.0918, -0.0486, -0.0513]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0947, -0.0320, -0.0400,  ...,  0.1104, -0.0713, -0.0243]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0723,  0.0258,  0.0294,  ...,  0.1934, -0.1006,  0.0410]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0674,  0.0610,  0.0049,  ...,  0.1216, -0.1196, -0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0459,  0.0967,  0.1387,  ...,  0.1060, -0.1001, -0.0635]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0918,  0.1357,  0.1289,  ...,  0.0369, -0.0723, -0.0342]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1543,  0.0630,  0.1133,  ...,  0.0284, -0.0488,  0.0303]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0640,  0.1338,  0.0483,  ...,  0.0117,  0.0164, -0.0601]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0791,  0.1406,  0.0383,  ...,  0.0522, -0.1025,  0.0105]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1641,  0.0488,  0.1719,  ...,  0.0208, -0.1396,  0.0352]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0303,  0.1436,  0.0376,  ...,  0.0023, -0.1367,  0.0483]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0461,  0.0806,  0.0288,  ...,  0.0352, -0.1465,  0.1172]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0898,  0.0186,  0.2061,  ...,  0.0371, -0.1592,  0.2246]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2559, -0.0664,  0.0654,  ..., -0.0659, -0.2109,  0.2051]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1465,  0.0840,  0.1396,  ..., -0.0210, -0.4004,  0.1582]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0854,  0.0742,  0.0273,  ..., -0.1592, -0.1426,  0.0664]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0649,  0.0874, -0.0635,  ..., -0.1914, -0.0508,  0.0869]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2422,  0.0483, -0.0981,  ..., -0.2871, -0.1543,  0.0654]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.3320,  0.0522, -0.0459,  ..., -0.2988, -0.1924, -0.1133]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1318,  0.1143, -0.1611,  ..., -0.4668, -0.2852, -0.4336]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1680,  0.1611, -0.0117,  ..., -0.3262, -0.4297, -0.3809]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2969,  0.3047, -0.1025,  ..., -0.3086, -0.3555, -0.3047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4336,  0.3184, -0.0488,  ..., -0.4336, -0.3574, -0.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4141,  0.4004, -0.1660,  ..., -0.4336, -0.5820, -0.4980]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2471,  0.6406, -0.2100,  ..., -0.4609, -0.5742, -0.5234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2461,  0.6914, -0.1074,  ..., -0.4746, -0.5703, -0.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2695,  0.8281, -0.2236,  ..., -0.2168, -0.5547, -0.4609]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0283,  1.0156,  0.0078,  ..., -0.3906, -0.5742, -0.6016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1836,  0.9766,  0.0649,  ..., -0.3770, -0.6289, -0.6211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1641,  0.9336,  0.2041,  ..., -0.3398, -0.7305, -0.6094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.4883,  0.9648,  0.0234,  ..., -0.2676, -0.7773, -1.0469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.6406,  0.2891,  0.0156,  ..., -0.0293, -1.1562, -0.9844]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.2656,  0.9688, -0.1758,  ...,  0.0684, -3.2500, -0.9141]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0034, -0.0085,  0.0038,  ..., -0.0277, -0.0033, -0.0032]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0084, -0.0188, -0.0023,  ..., -0.0703, -0.0177, -0.0041]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0020, -0.0187, -0.0051,  ..., -0.0864, -0.0007, -0.0021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0013, -0.0195, -0.0103,  ..., -0.1011,  0.0115,  0.0217]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0271, -0.0342, -0.0371,  ..., -0.0776,  0.0659,  0.0001]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0261,  0.0123, -0.0461,  ...,  0.0046, -0.0278,  0.0522]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0850,  0.0540, -0.0098,  ...,  0.0439,  0.0078, -0.0273]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0820,  0.0369,  0.0320,  ..., -0.0161,  0.0072, -0.0332]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0889,  0.0437,  0.0557,  ...,  0.0004, -0.0078, -0.0339]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0977,  0.0742,  0.0811,  ...,  0.0830, -0.0498,  0.0114]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1240,  0.1289,  0.0481,  ..., -0.0073, -0.1021, -0.0098]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1582,  0.1240,  0.1143,  ..., -0.0391, -0.0293, -0.0928]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1035,  0.1152,  0.0547,  ..., -0.0405,  0.0197, -0.0571]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0337,  0.0146,  0.0242,  ..., -0.0277, -0.0068, -0.0302]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0430,  0.1104, -0.0371,  ..., -0.0095, -0.0237, -0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0337,  0.1621, -0.0303,  ..., -0.0137, -0.0027, -0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0908,  0.0986,  0.0996,  ...,  0.0732, -0.0291,  0.0574]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0206,  0.1914, -0.0204,  ...,  0.0928,  0.0205,  0.0376]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0167,  0.1865, -0.0144,  ...,  0.1240,  0.1040,  0.0771]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.2393, 0.2578, 0.1152,  ..., 0.1191, 0.1982, 0.1777]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2217,  0.2988, -0.0156,  ...,  0.0840,  0.0986,  0.1211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0518,  0.1582, -0.1025,  ...,  0.1602,  0.0449,  0.0056]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0688,  0.0972, -0.0815,  ..., -0.0469,  0.2754, -0.1245]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.1035,  0.0752, -0.0972,  ..., -0.0605,  0.4082, -0.1455]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2031,  0.0859, -0.0281,  ..., -0.0079,  0.2373, -0.1221]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2578,  0.1099, -0.0315,  ..., -0.0171,  0.1885, -0.2129]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0527, -0.0112, -0.1885,  ...,  0.0889,  0.4453, -0.4414]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2422,  0.1260, -0.0767,  ...,  0.2578,  0.5898, -0.3730]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.2656,  0.2402, -0.0464,  ...,  0.3301,  0.7148, -0.3535]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2100,  0.1289, -0.2637,  ...,  0.2441,  0.6289, -0.3066]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.2734,  0.0381, -0.4531,  ...,  0.2695,  0.5312, -0.5859]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0869,  0.0554, -0.5469,  ...,  0.0176,  0.6914, -0.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0747,  0.1147, -0.4180,  ...,  0.0713,  0.8164, -0.4102]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0840,  0.2041, -0.4688,  ...,  0.0659,  1.0391, -0.3906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2656,  0.2148, -0.3984,  ..., -0.0513,  1.1641, -0.6562]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3320,  0.0474, -0.5156,  ..., -0.2090,  1.2891, -0.6484]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.5859, -0.2148, -0.0703,  ..., -0.1885,  1.4219, -1.1094]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1562, -0.3398, -0.6914,  ..., -0.1118,  1.7969, -0.9922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-1.2031, -0.8984, -0.5664,  ...,  0.2969,  1.7031, -0.7969]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.9727,  0.4688, -1.1719,  ...,  0.3047,  1.5000, -1.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0094, -0.0070, -0.0080,  ..., -0.0234, -0.0115,  0.0088]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0090,  0.0020, -0.0012,  ..., -0.0542, -0.0175,  0.0127]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0059,  0.0048, -0.0145,  ..., -0.0771,  0.0018, -0.0106]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0030,  0.0167,  0.0085,  ..., -0.1201, -0.0040,  0.0018]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0054,  0.0228,  0.0146,  ..., -0.1396,  0.0195,  0.0065]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0046,  0.0786,  0.0013,  ..., -0.0796, -0.0356,  0.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0114,  0.0566, -0.0201,  ..., -0.0845, -0.0088,  0.0071]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0181,  0.0327,  0.0264,  ..., -0.0669, -0.0090,  0.0430]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0151,  0.0574,  0.0708,  ..., -0.0776, -0.0117,  0.0669]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0859,  0.1328,  0.0938,  ..., -0.0547,  0.0299,  0.1133]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0068,  0.0806,  0.1187,  ..., -0.0654, -0.0066,  0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0032,  0.0811,  0.2402,  ..., -0.1157,  0.1104,  0.0007]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0344,  0.1396,  0.1377,  ..., -0.1069,  0.1387,  0.0378]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0781,  0.1318,  0.1260,  ..., -0.1562,  0.1348,  0.0530]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0630,  0.1631,  0.0659,  ..., -0.1670,  0.1328, -0.0317]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0645,  0.1328,  0.0337,  ..., -0.0986,  0.1050,  0.0110]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1104,  0.0894,  0.1650,  ...,  0.0332,  0.0125,  0.1084]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0093,  0.1953,  0.1245,  ...,  0.0889, -0.0037,  0.0840]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[0.0087, 0.1777, 0.1030,  ..., 0.1035, 0.0459, 0.1738]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[0.2031, 0.2412, 0.2119,  ..., 0.0674, 0.0806, 0.1875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1836,  0.2217,  0.1475,  ..., -0.0928, -0.0264,  0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0356,  0.2295,  0.0532,  ..., -0.0620, -0.1475, -0.1621]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0393,  0.1357,  0.0654,  ..., -0.0869,  0.1084, -0.1914]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0229,  0.1514,  0.1030,  ..., -0.1426,  0.2197, -0.2207]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0327,  0.2148,  0.1182,  ..., -0.0547,  0.1025, -0.2383]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0625,  0.2246,  0.1123,  ..., -0.1396,  0.0898, -0.3047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0278,  0.2285, -0.0820,  ..., -0.1055,  0.2119, -0.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0278,  0.1934,  0.0845,  ...,  0.0859,  0.3066, -0.6836]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 0.0840,  0.2891,  0.1260,  ...,  0.1348,  0.4141, -0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.1445,  0.2256, -0.0254,  ...,  0.0400,  0.5039, -0.5781]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0801,  0.3027, -0.2217,  ...,  0.1045,  0.4180, -0.8516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0542,  0.3047, -0.2158,  ..., -0.0264,  0.5000, -0.8789]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1553,  0.3066, -0.0605,  ...,  0.0146,  0.6641, -0.6797]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1582,  0.4824, -0.0039,  ..., -0.0996,  0.7344, -0.7734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3984,  0.5742,  0.2988,  ..., -0.3633,  0.7812, -1.0156]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3887,  0.5703,  0.2969,  ..., -0.5312,  0.7773, -1.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3086,  0.3086,  0.6367,  ..., -0.7500,  0.7734, -1.3047]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.7383, -0.1191,  0.1660,  ..., -0.8672,  1.0312, -1.2656]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.8477, -0.4141,  0.2295,  ..., -0.6211,  0.8125, -1.1016]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.3242,  0.1289,  0.7305,  ..., -0.4844,  0.5273, -1.3359]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), DynamicCache())\n",
      "\n",
      "<|begin_of_text|><|image|><|begin_of_text|>what you can see in the picture is the office of the company I work for. I am the only one in the office. I am the only one in the office\n"
     ]
    }
   ],
   "source": [
    "image = hololens_image\n",
    "\n",
    "prompt = \"<|image|><|begin_of_text|>what you can see\"\n",
    "inputs = expert_model_processor(image, prompt, return_tensors=\"pt\").to(expert_model.device)\n",
    "\n",
    "output = expert_model.generate(**inputs, max_new_tokens=30)\n",
    "print(expert_model_processor.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# >>> import requests\n",
    "# >>> from transformers import AutoProcessor, MllamaVisionModel\n",
    "\n",
    "# >>> checkpoint = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "# >>> model = MllamaVisionModel.from_pretrained(checkpoint)\n",
    "# >>> processor = AutoProcessor.from_pretrained(checkpoint)\n",
    "\n",
    "# >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "# >>> image = Image.open(requests.get(url, stream=True).raw)\n",
    "# >>> inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "# >>> output = model(**inputs)\n",
    "\n",
    "# >>> print(output.last_hidden_state.shape)\n",
    "# torch.Size([1, 1, 4, 1025, 7680])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_outputs_dict = expert_model.get_text_model_hidden_layers_attentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cross_attention': [tensor([[[-0.1201,  0.0588, -0.0630,  ...,  0.8633,  0.1855,  0.1035],\n",
       "           [-0.0282, -0.0233,  0.0135,  ..., -0.0200,  0.0121,  0.0138],\n",
       "           [-0.0889,  0.0282, -0.0796,  ...,  0.6758,  0.1670,  0.0386],\n",
       "           ...,\n",
       "           [ 0.0083, -0.0771,  0.0674,  ..., -0.0425, -0.0079, -0.0537],\n",
       "           [ 0.0354, -0.0139,  0.0299,  ..., -0.0938, -0.0317,  0.0505],\n",
       "           [-0.0771, -0.0820, -0.0391,  ..., -0.1201, -0.0232,  0.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0884,  0.1069,  0.0081,  ...,  0.8633,  0.2266,  0.0830],\n",
       "           [-0.0962,  0.0014,  0.1230,  ...,  0.0244, -0.0244,  0.0664],\n",
       "           [-0.0096,  0.1504, -0.0586,  ...,  0.6406,  0.1484,  0.0096],\n",
       "           ...,\n",
       "           [-0.0225,  0.0280, -0.0591,  ..., -0.2266,  0.0708, -0.0100],\n",
       "           [ 0.0977, -0.0376, -0.0850,  ..., -0.1504,  0.1709,  0.0481],\n",
       "           [-0.1514, -0.0630, -0.0796,  ..., -0.1309,  0.0449,  0.0366]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0503,  0.2734,  0.0208,  ...,  0.5938,  0.2891,  0.1240],\n",
       "           [-0.1523,  0.0508,  0.0674,  ...,  0.1172, -0.0767,  0.1055],\n",
       "           [-0.1484,  0.1426,  0.0079,  ...,  0.3594,  0.0859, -0.0737],\n",
       "           ...,\n",
       "           [ 0.0430, -0.1348, -0.0532,  ..., -0.0776,  0.1621, -0.0437],\n",
       "           [-0.1514, -0.2598,  0.0156,  ...,  0.0359,  0.0859,  0.0732],\n",
       "           [-0.1934, -0.2227, -0.1816,  ...,  0.0625,  0.0996,  0.1836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0544,  0.2891, -0.0781,  ...,  0.3945,  0.3125,  0.0537],\n",
       "           [-0.1777,  0.1299,  0.1699,  ..., -0.0957, -0.1328, -0.0254],\n",
       "           [-0.2969, -0.0092, -0.0874,  ...,  0.2852,  0.0040, -0.0532],\n",
       "           ...,\n",
       "           [-0.0332, -0.0830, -0.0146,  ..., -0.1309,  0.1182, -0.1445],\n",
       "           [ 0.2871, -0.2334, -0.1084,  ..., -0.0623,  0.1582,  0.0552],\n",
       "           [-0.0275, -0.2441, -0.1118,  ..., -0.0869, -0.1787,  0.2393]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1709,  0.0635,  0.0085,  ...,  0.2676,  0.5078, -0.0007],\n",
       "           [-0.1855,  0.2070,  0.1875,  ..., -0.2969, -0.0625, -0.3047],\n",
       "           [-0.3613, -0.2617,  0.0156,  ...,  0.0840, -0.1094, -0.0396],\n",
       "           ...,\n",
       "           [-0.2734, -0.2363,  0.3008,  ..., -0.4785,  0.1416, -0.1221],\n",
       "           [ 0.0801, -0.2129,  0.2578,  ..., -0.2754,  0.2559, -0.0581],\n",
       "           [-0.3281, -0.1699,  0.0835,  ..., -0.4922, -0.1001,  0.2832]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2051, -0.0101, -0.0811,  ...,  0.2539,  0.4355,  0.1777],\n",
       "           [-0.2891,  0.3516, -0.0032,  ..., -0.3770, -0.0947, -0.5000],\n",
       "           [ 0.1709, -0.1924, -0.1514,  ...,  0.1582, -0.6211, -0.4160],\n",
       "           ...,\n",
       "           [-0.0352,  0.0200, -0.2002,  ..., -0.6367, -0.0244, -0.3867],\n",
       "           [ 0.4727, -0.5312,  0.1367,  ..., -0.4062,  0.4277, -0.5312],\n",
       "           [-0.0693,  0.0430,  0.0039,  ..., -0.4277,  0.2168,  0.1152]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1167, -0.0493, -0.3184,  ...,  0.1045,  0.5312,  0.0952],\n",
       "           [-0.1650,  0.6133, -0.1943,  ..., -0.0859, -0.2070, -0.3086],\n",
       "           [ 0.0781, -0.0991, -0.7812,  ..., -0.2480, -0.6406, -1.0000],\n",
       "           ...,\n",
       "           [-0.0986,  0.2676,  0.0820,  ..., -1.5625, -0.0146, -0.3164],\n",
       "           [ 1.0156, -0.2910,  0.1475,  ..., -0.3203,  0.2148, -0.5234],\n",
       "           [ 0.0039,  0.2148,  0.3438,  ..., -0.4512, -0.1562,  0.2754]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 2.3535e-01,  8.3594e-01,  2.3828e-01,  ...,  3.3984e-01,\n",
       "             1.8359e-01,  7.5391e-01],\n",
       "           [-7.6562e-01,  3.3594e-01, -1.4648e-03,  ..., -4.6875e-01,\n",
       "            -7.4219e-01, -1.7188e+00],\n",
       "           [ 7.6562e-01,  5.8203e-01, -1.8594e+00,  ...,  6.9922e-01,\n",
       "            -1.4922e+00, -1.4062e+00],\n",
       "           ...,\n",
       "           [-1.1016e+00, -4.2969e-02, -3.1445e-01,  ..., -1.9844e+00,\n",
       "             3.1641e-01, -1.0156e+00],\n",
       "           [ 7.2656e-01, -1.4844e+00, -4.6875e-01,  ..., -1.2344e+00,\n",
       "             4.0234e-01, -7.5000e-01],\n",
       "           [-5.9766e-01, -2.0605e-01,  6.8359e-02,  ..., -1.0303e-01,\n",
       "            -4.2578e-01, -7.3438e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0275, -0.0334, -0.0046,  ..., -0.0649,  0.0101, -0.0288]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0286,  0.2109, -0.1309,  ...,  0.0361, -0.0767,  0.1040]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0566,  0.1240, -0.1777,  ...,  0.0918, -0.0410, -0.0693]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1914,  0.0503, -0.0410,  ...,  0.2012, -0.1582,  0.0330]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1030,  0.1738,  0.1484,  ..., -0.0010, -0.0449, -0.0039]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2207,  0.3945,  0.2227,  ..., -0.2773,  0.3301,  0.0576]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0320,  0.8633,  0.5234,  ..., -0.5156, -0.5430,  0.8867]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1836, -0.7734, -0.3672,  ..., -0.4316, -0.6641,  0.7656]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0096,  0.0101, -0.0728,  ..., -0.1084, -0.0056, -0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0542,  0.0332, -0.1562,  ..., -0.0986,  0.0684,  0.0229]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1035,  0.0510, -0.1631,  ..., -0.0029, -0.0791, -0.0400]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1094, -0.0371, -0.0275,  ..., -0.0664, -0.1250,  0.0801]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0576,  0.2969,  0.1328,  ..., -0.0947,  0.2129,  0.1279]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4414,  0.2793, -0.2539,  ..., -0.6992,  0.4062, -0.1572]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0488,  0.7266, -0.6367,  ..., -0.8594,  1.3125,  0.0498]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5234,  0.6406, -0.8320,  ..., -1.0547,  1.5938, -0.3828]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0135,  0.0757,  0.0064,  ..., -0.0129,  0.0150,  0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0016,  0.0503, -0.0420,  ...,  0.1099, -0.0486,  0.1016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0076,  0.0272, -0.1367,  ...,  0.1104, -0.0977,  0.1309]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.1055,  0.0085,  ...,  0.1660,  0.0047, -0.0166]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4531,  0.0059, -0.1445,  ...,  0.2041,  0.2178, -0.2793]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0684,  0.2754, -0.1143,  ...,  0.2656,  0.5078, -0.4453]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2305,  0.1748,  0.0063,  ..., -0.3867,  1.0000, -0.4453]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1367, -0.7422, -1.4922,  ..., -1.0312,  1.0469, -0.1035]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0381,  0.0654, -0.0096,  ...,  0.0244,  0.0280, -0.0204]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0320,  0.0620, -0.0562,  ...,  0.1270,  0.0084,  0.0258]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0391,  0.0640,  0.0859,  ...,  0.0684, -0.0488, -0.0032]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0181,  0.1455,  0.1182,  ...,  0.1221,  0.0393,  0.0234]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3047,  0.3906,  0.2812,  ...,  0.2207,  0.1445,  0.1572]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1855,  0.1562,  0.5078,  ...,  0.1709,  0.4316, -0.2500]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0659,  0.3340,  0.8555,  ...,  0.3730,  0.9609, -0.1182]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.3828, -1.1172,  1.1328,  ...,  0.6367,  0.4512,  0.3457]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0197,  0.0010,  0.0220,  ..., -0.0771, -0.0396,  0.0096]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0287,  0.0101, -0.0547,  ...,  0.0801, -0.0271,  0.1914]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1143, -0.0107,  0.0093,  ..., -0.0042, -0.0007,  0.1157]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0403,  0.1543,  0.1240,  ...,  0.0425, -0.1504,  0.1475]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0486,  0.1152,  0.1348,  ..., -0.2354, -0.0476,  0.0869]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2080,  0.2734,  0.3906,  ..., -0.4551,  0.0791,  0.2412]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5625,  0.5156,  0.2080,  ..., -0.4473, -0.4688,  0.6719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1060, -0.1816,  0.5312,  ..., -0.1943, -0.4844,  0.6875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0156,  0.0031, -0.0014,  ..., -0.0615, -0.0359, -0.0062]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1143, -0.1367, -0.0938,  ...,  0.1133,  0.0181,  0.0649]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1748, -0.1260, -0.0010,  ...,  0.0361, -0.0229,  0.0200]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1270, -0.0215, -0.0713,  ..., -0.2344, -0.0312,  0.0869]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0021,  0.0791,  0.2051,  ..., -0.5312,  0.2090,  0.2969]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0562,  0.4082,  0.5508,  ..., -1.2031,  0.7422,  0.2070]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3242,  1.0000,  0.6641,  ..., -1.1094,  0.6836,  0.2139]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1826,  0.2441,  0.6367,  ..., -1.1172,  0.7734, -0.1089]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0327, -0.0070,  0.0383,  ..., -0.1069, -0.0012,  0.0071]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1777,  0.0874, -0.0815,  ..., -0.0146,  0.0620,  0.0933]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1865,  0.0100,  0.0309,  ...,  0.0469,  0.0564,  0.0635]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0625,  0.2451,  0.0654,  ..., -0.2988, -0.0464,  0.1484]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0859,  0.2490,  0.0986,  ..., -0.3867,  0.0535,  0.3945]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3008,  0.6094,  0.6172,  ..., -0.3984, -0.0352,  0.9570]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5625,  1.0000,  0.5430,  ..., -0.6250, -0.1196,  0.9336]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 1.0391,  0.6133,  0.0410,  ..., -0.7812, -0.7852,  0.8750]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0361, -0.0317,  0.0082,  ..., -0.0469, -0.0276,  0.0111]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2158,  0.0087,  0.0085,  ...,  0.1177, -0.0459,  0.0332]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2129,  0.0586,  0.0757,  ..., -0.0361, -0.1855, -0.0283]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0188,  0.1338,  0.0078,  ..., -0.0986, -0.4453,  0.2617]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0410,  0.2393,  0.0054,  ..., -0.0967,  0.1270,  0.3691]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4492, -0.0859,  0.1963,  ..., -0.2969, -0.1562,  0.5312]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0273,  0.7031,  0.2188,  ..., -0.4609,  0.4492,  0.3711]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1562, -0.0786, -0.0884,  ..., -0.6641, -0.2070,  0.2578]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0167,  0.0135, -0.0020,  ..., -0.0024,  0.0095, -0.0383]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1270,  0.0151, -0.1758,  ...,  0.1367, -0.0957, -0.1426]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0074, -0.2129, -0.0031,  ..., -0.0645, -0.0610, -0.1689]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0552,  0.0474,  0.1484,  ..., -0.1797, -0.1533,  0.1436]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2344,  0.0703,  0.1865,  ..., -0.0786, -0.0098,  0.0947]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1201,  0.3438,  0.5195,  ..., -0.5859,  0.1768, -0.2021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4004,  0.0469,  0.5664,  ...,  0.0098,  0.1328,  0.0146]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2500, -0.4805,  0.1582,  ..., -0.0566,  0.5156,  0.4570]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0083,  0.0181,  0.0206,  ..., -0.0347,  0.0118, -0.0080]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0469, -0.0825, -0.0767,  ..., -0.0015,  0.1338,  0.1357]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1816, -0.1816,  0.0105,  ..., -0.0815,  0.0061, -0.0239]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0498, -0.0835,  0.1992,  ..., -0.2227, -0.2314,  0.2451]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0073, -0.0488, -0.0352,  ...,  0.0576, -0.1943,  0.1040]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1641,  0.4531,  0.5547,  ..., -0.3125, -0.0972, -0.0544]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3066,  0.5234,  0.7812,  ..., -0.2012, -0.2275, -0.1553]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.3281, -0.0273,  0.2500,  ..., -0.9844, -0.5039,  0.4883]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0493,  0.0630,  0.0034,  ..., -0.0017, -0.0203, -0.0242]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1377, -0.0410, -0.0986,  ...,  0.1221,  0.0811,  0.0825]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1055, -0.1426,  0.0559,  ..., -0.0393, -0.0306, -0.0498]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0322, -0.1455,  0.2148,  ..., -0.0801, -0.2090,  0.2139]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0471, -0.0879, -0.0977,  ..., -0.0352,  0.1211,  0.2354]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1133,  0.0449,  0.1543,  ...,  0.2432, -0.0596, -0.0217]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2402,  0.0508,  0.2061,  ..., -0.0742,  0.1426,  0.3984]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.3906, -1.5234, -0.6055,  ...,  0.6328, -0.5156,  0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0498,  0.0115,  0.0322,  ..., -0.0615, -0.0271, -0.0066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2002, -0.0232, -0.0469,  ...,  0.0111, -0.0369,  0.1128]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1602, -0.1562, -0.0146,  ..., -0.0591, -0.0830, -0.0586]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2246,  0.1064, -0.1406,  ..., -0.1562, -0.1367,  0.0537]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4434, -0.1680, -0.0522,  ..., -0.1895, -0.0508,  0.0605]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3789, -0.1074,  0.3086,  ...,  0.0928, -0.0110,  0.0762]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6562,  0.5469,  0.2139,  ...,  0.1162, -0.8086,  0.6484]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.4766, -0.7891, -0.0107,  ...,  0.2871, -0.8359,  1.4609]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0020, -0.0007,  0.0240,  ..., -0.0991, -0.0061,  0.0396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0732, -0.1621, -0.0723,  ..., -0.0200,  0.0576, -0.0332]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2168, -0.3906,  0.0962,  ...,  0.0493, -0.0396, -0.0021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1465, -0.0967,  0.0811,  ..., -0.1074,  0.0278,  0.3086]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0771, -0.2363, -0.0481,  ..., -0.0181,  0.2266,  0.1963]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0068, -0.5312,  0.0420,  ..., -0.4707,  0.4922, -0.0303]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1182, -0.3516,  0.3105,  ..., -0.1543,  0.5312, -0.0232]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0771, -1.8750, -1.1172,  ..., -0.7031,  0.6016,  0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0986, -0.0371, -0.0396,  ..., -0.1133, -0.0415,  0.0459]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0034, -0.1533, -0.1504,  ..., -0.0282,  0.0081,  0.0415]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1367, -0.2285, -0.0747,  ...,  0.0923, -0.0503, -0.0227]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0269, -0.1455,  0.0732,  ..., -0.1914, -0.2578,  0.3320]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2676, -0.4043, -0.0107,  ..., -0.0576, -0.0215,  0.2207]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1904, -0.5703,  0.3711,  ..., -0.0208,  0.1953,  0.0452]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8281,  0.0957,  0.2656,  ..., -0.0312,  0.5234,  0.2891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0391, -0.4258, -0.2363,  ...,  0.4297,  0.0508,  0.2061]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0067, -0.0095, -0.0208,  ...,  0.0186,  0.0101, -0.0107]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1001, -0.0603, -0.1514,  ...,  0.1074, -0.0002, -0.0134]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1270, -0.0488,  0.0112,  ...,  0.2119, -0.0459, -0.0898]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0288, -0.0776,  0.1240,  ..., -0.2773, -0.1240,  0.0928]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2324,  0.0381,  0.2041,  ..., -0.3848, -0.1147,  0.0889]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2617, -0.2490, -0.1914,  ..., -0.6094, -0.0396,  0.4629]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6719,  0.9492,  0.3496,  ..., -0.6250, -0.5586,  1.0938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1562,  0.1406, -0.2617,  ...,  0.1079, -1.5781,  0.8477]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0026, -0.0022, -0.0153,  ..., -0.1104,  0.0015, -0.0131]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0938, -0.0312, -0.1533,  ..., -0.0046,  0.0864,  0.0574]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2695,  0.0156, -0.0938,  ...,  0.0415, -0.1836, -0.0056]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0435, -0.0576,  0.2109,  ..., -0.0630, -0.3125,  0.0244]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0825,  0.0986,  0.2930,  ...,  0.1523,  0.0063,  0.3789]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3008,  0.0583, -0.4277,  ..., -0.2539,  0.2656,  0.2061]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0254,  0.8594, -0.4453,  ..., -0.7188,  1.0625,  0.5742]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0056,  0.6797, -0.7305,  ..., -0.7422,  1.1875, -0.2598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0105,  0.0126, -0.0356,  ..., -0.0459,  0.0188, -0.0160]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1016, -0.0625, -0.0623,  ...,  0.0728,  0.0276, -0.0601]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0452, -0.1143,  0.0713,  ...,  0.0214,  0.0269, -0.0767]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0889,  0.0063,  0.2295,  ..., -0.2148, -0.1719, -0.0618]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0391, -0.2285,  0.2129,  ..., -0.4961,  0.0286, -0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1162, -0.0894,  0.1270,  ..., -0.8828,  0.3594, -0.3750]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1514,  0.0474, -0.1108,  ..., -1.1094,  0.6562, -0.0781]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2051, -0.6641, -0.8438,  ..., -1.8594,  0.7266,  0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0109,  0.0245,  0.0322,  ..., -0.0620, -0.0282, -0.0095]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0149, -0.0322, -0.0771,  ..., -0.0116,  0.1338, -0.0461]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0488, -0.0042, -0.0752,  ..., -0.0277, -0.1001, -0.0297]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3203,  0.0004,  0.0654,  ..., -0.1816, -0.1133, -0.1562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1133,  0.1064,  0.3438,  ..., -0.2598,  0.2617, -0.0903]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0381,  0.3145,  0.2852,  ..., -1.0312,  0.0327,  0.0576]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2168,  0.4258,  0.3203,  ..., -1.7812,  0.2109,  0.2988]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7969, -0.3320, -0.8242,  ..., -2.0625, -0.1973,  0.2637]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0016,  0.0132,  0.0830,  ..., -0.0056, -0.0339,  0.0201]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1162,  0.0042, -0.0286,  ...,  0.1797,  0.0864,  0.1348]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1011, -0.1895,  0.0977,  ...,  0.1270, -0.1206, -0.0039]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2598,  0.1221,  0.0574,  ..., -0.0752, -0.1455, -0.0181]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2490, -0.0903,  0.3105,  ..., -0.1108,  0.0303,  0.0742]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0132, -0.0962,  0.0947,  ..., -0.5352, -0.0557,  0.2178]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1826,  0.0947, -0.0322,  ..., -0.8750, -0.2520,  0.0234]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3047,  0.0254, -1.1641,  ..., -0.9727, -0.4980, -0.2051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0050,  0.0052,  0.0208,  ..., -0.0356, -0.0544,  0.0349]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0549,  0.0605,  0.1099,  ...,  0.0056, -0.0166,  0.0317]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0640, -0.1797,  0.0386,  ..., -0.0449, -0.0024, -0.0583]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2852, -0.1543,  0.0435,  ...,  0.0427, -0.2871, -0.0903]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3223, -0.3047,  0.3320,  ..., -0.3008, -0.2305,  0.4258]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3008, -0.4023,  0.4961,  ..., -0.7305, -0.0908,  0.5430]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3359, -0.1836,  0.3086,  ..., -0.9062, -0.7344,  0.4629]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4121, -0.4180,  0.1250,  ..., -1.1250, -0.9609,  0.8086]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0157, -0.0430,  0.0112,  ..., -0.0513, -0.0077, -0.0278]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0674, -0.0500,  0.0476,  ...,  0.1660, -0.0957, -0.0503]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0913, -0.1670,  0.1816,  ...,  0.0000, -0.2363,  0.0830]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2422,  0.0187, -0.1357,  ...,  0.0396, -0.1465, -0.1270]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0762, -0.1152,  0.0588,  ...,  0.1475, -0.0352,  0.1108]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1328, -0.0664, -0.0615,  ..., -0.1924, -0.0066,  0.2285]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6719,  0.4648, -0.0723,  ...,  0.1914,  0.7578,  0.3340]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2236,  0.2041, -1.0547,  ...,  0.0781,  0.7891,  0.4824]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0439, -0.0133, -0.0271,  ..., -0.0135, -0.0044,  0.0339]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0535, -0.0566,  0.0732,  ...,  0.1177, -0.0513,  0.0148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1719, -0.2031,  0.0425,  ..., -0.0889, -0.0933,  0.0182]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1050,  0.0513, -0.0874,  ..., -0.0437, -0.0500, -0.1543]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0222,  0.0148,  0.2852,  ...,  0.1602,  0.1387, -0.2852]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2402, -0.2373,  0.1641,  ..., -0.0879,  0.2266, -0.3809]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1660, -0.0918,  0.3652,  ..., -0.0947,  0.7031, -0.5508]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1973, -0.8281, -0.1660,  ..., -0.5352,  0.2930, -0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0013,  0.0474,  0.0249,  ..., -0.0674,  0.0134,  0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0723,  0.0405,  0.0850,  ...,  0.0554,  0.0393,  0.2188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1016, -0.0615,  0.2451,  ..., -0.1338, -0.0366,  0.1768]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2070,  0.0566, -0.0913,  ...,  0.0210, -0.0283,  0.1045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3691,  0.2168,  0.2002,  ..., -0.1118,  0.3223, -0.0159]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1582,  0.3945,  0.5078,  ..., -0.1670,  0.3203, -0.3379]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1650,  0.8164,  0.2402,  ..., -0.5469,  0.4590, -0.1318]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8594,  0.2139,  0.4727,  ..., -0.3965,  0.0325, -0.2188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0088,  0.0391,  0.0208,  ...,  0.0043, -0.0221,  0.0276]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0410,  0.1011, -0.1299,  ...,  0.0007,  0.0056,  0.1846]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1738, -0.0190,  0.0166,  ..., -0.0439, -0.1758,  0.1396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1904,  0.0591,  0.0483,  ..., -0.1455,  0.1025,  0.0272]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0420, -0.0381,  0.1758,  ..., -0.1211,  0.2832, -0.0947]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.2393,  0.2168,  ..., -0.1943,  0.2656, -0.2324]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1719,  0.5352,  0.3320,  ...,  0.0332,  0.2754, -0.4531]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4941, -0.0488, -0.1992,  ..., -0.3594, -0.4590, -0.4297]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0552, -0.0107,  0.0742,  ..., -0.0703, -0.0117,  0.0364]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0510,  0.0649,  0.0654,  ...,  0.0498, -0.0820,  0.0045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1201, -0.0684, -0.0520,  ..., -0.2031, -0.1504, -0.1104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0693,  0.0771,  0.1270,  ..., -0.2891, -0.0596, -0.2031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0190,  0.2217,  0.2246,  ..., -0.2930, -0.0669, -0.0017]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1050,  0.5273,  0.2236,  ..., -0.1758,  0.0859, -0.3711]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2734,  1.6719,  0.2109,  ..., -0.8594,  0.6836,  0.3398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 1.0234,  0.3594, -0.6406,  ..., -1.3281, -0.4062,  0.2949]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0134, -0.0300,  0.0430,  ..., -0.0552, -0.0112,  0.0043]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0143,  0.1172,  0.0747,  ..., -0.0002, -0.0947,  0.0403]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0723, -0.1025,  0.1011,  ..., -0.1240, -0.0664,  0.0311]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0240,  0.0845, -0.0085,  ..., -0.2930, -0.0071, -0.2051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3711,  0.1030,  0.1504,  ..., -0.2793,  0.1934,  0.0165]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2275,  0.1094,  0.1226,  ..., -0.1099,  0.3164, -0.3223]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2314,  1.1719,  0.7656,  ..., -0.8125, -0.1152, -0.0854]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1973,  1.0000,  0.5469,  ..., -0.3125, -0.1426,  0.4609]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0168,  0.0123,  0.0154,  ..., -0.0869, -0.0143, -0.0004]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0300, -0.0010, -0.0347,  ..., -0.0947,  0.0234,  0.0645]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2070, -0.1807, -0.0874,  ..., -0.0437,  0.0176,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0171,  0.0083,  0.0000,  ..., -0.3867, -0.0200,  0.0623]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2715, -0.0605,  0.3574,  ..., -0.2041,  0.1807, -0.0688]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0664, -0.0938,  0.3730,  ..., -0.3379,  0.7305, -0.3164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3516,  0.5547,  0.7617,  ..., -1.1406,  0.2812, -0.4727]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3359,  0.0742, -0.2734,  ..., -1.8984, -0.0840,  0.1826]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0840, -0.0273, -0.0312,  ..., -0.1426, -0.0732,  0.0510]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0898, -0.0737, -0.1904,  ..., -0.1494, -0.0035,  0.0223]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0182, -0.1514, -0.1514,  ..., -0.0532, -0.0840,  0.0278]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1904, -0.0432,  0.0381,  ..., -0.3242, -0.3555,  0.1025]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2090, -0.3906, -0.0444,  ..., -0.2520, -0.3984,  0.0938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2227, -0.2178, -0.1021,  ..., -0.0786, -0.5078,  0.2246]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5078,  0.3828, -0.0630,  ..., -0.5156, -0.7891, -0.1016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4648, -0.0156, -0.5234,  ..., -0.8047, -1.1953,  0.6797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0013, -0.0130,  0.0048,  ..., -0.0236,  0.0176, -0.0021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1055, -0.0242,  0.0015,  ...,  0.0967, -0.0295, -0.0486]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1147, -0.2949, -0.0820,  ..., -0.1328, -0.1172, -0.1104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0928, -0.1484,  0.0508,  ..., -0.2363, -0.3203, -0.0256]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0510, -0.0024,  0.2207,  ..., -0.2236, -0.4922,  0.0054]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0264, -0.4531,  0.3770,  ..., -0.3555, -0.7461,  0.4609]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2344,  0.0020,  0.2383,  ..., -0.6250, -0.2695,  0.4766]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5938, -0.7500,  0.1367,  ..., -0.6719, -0.5781,  0.5352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1201,  0.0588, -0.0630,  ...,  0.8633,  0.1855,  0.1035],\n",
       "           [-0.0282, -0.0233,  0.0135,  ..., -0.0200,  0.0121,  0.0138],\n",
       "           [-0.0889,  0.0282, -0.0796,  ...,  0.6758,  0.1670,  0.0386],\n",
       "           ...,\n",
       "           [ 0.0361,  0.0210,  0.0312,  ..., -0.1025, -0.0469, -0.0256],\n",
       "           [ 0.0625, -0.0162,  0.0055,  ..., -0.0026, -0.0684, -0.0083],\n",
       "           [ 0.0210, -0.0649, -0.0566,  ..., -0.0015, -0.0767, -0.0162]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0884,  0.1069,  0.0081,  ...,  0.8633,  0.2266,  0.0830],\n",
       "           [-0.0962,  0.0014,  0.1230,  ...,  0.0244, -0.0244,  0.0664],\n",
       "           [-0.0096,  0.1504, -0.0586,  ...,  0.6406,  0.1484,  0.0096],\n",
       "           ...,\n",
       "           [ 0.0386,  0.2441,  0.1001,  ...,  0.0106,  0.0376,  0.1138],\n",
       "           [ 0.1777,  0.1270,  0.1123,  ..., -0.1660, -0.0791,  0.0396],\n",
       "           [-0.0840,  0.0737, -0.1123,  ..., -0.1484,  0.0654,  0.0605]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0503,  0.2734,  0.0208,  ...,  0.5938,  0.2891,  0.1240],\n",
       "           [-0.1523,  0.0508,  0.0674,  ...,  0.1172, -0.0767,  0.1055],\n",
       "           [-0.1484,  0.1426,  0.0079,  ...,  0.3594,  0.0859, -0.0737],\n",
       "           ...,\n",
       "           [ 0.0679,  0.1348,  0.0239,  ...,  0.0840, -0.0525,  0.2949],\n",
       "           [ 0.1011,  0.0493,  0.0732,  ..., -0.0879,  0.0781,  0.1226],\n",
       "           [ 0.0012, -0.1582, -0.0903,  ..., -0.2070,  0.0732,  0.1299]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0544,  0.2891, -0.0781,  ...,  0.3945,  0.3125,  0.0537],\n",
       "           [-0.1777,  0.1299,  0.1699,  ..., -0.0957, -0.1328, -0.0254],\n",
       "           [-0.2969, -0.0092, -0.0874,  ...,  0.2852,  0.0040, -0.0532],\n",
       "           ...,\n",
       "           [ 0.0352,  0.2891, -0.1699,  ...,  0.1992, -0.1680,  0.2139],\n",
       "           [ 0.1108,  0.3047, -0.0566,  ...,  0.0269, -0.1079, -0.1138],\n",
       "           [-0.0491,  0.0327, -0.1973,  ...,  0.0107, -0.0205, -0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1709,  0.0635,  0.0085,  ...,  0.2676,  0.5078, -0.0007],\n",
       "           [-0.1855,  0.2070,  0.1875,  ..., -0.2969, -0.0625, -0.3047],\n",
       "           [-0.3613, -0.2617,  0.0156,  ...,  0.0840, -0.1094, -0.0396],\n",
       "           ...,\n",
       "           [-0.0381, -0.0874, -0.1128,  ...,  0.2080,  0.2480,  0.3066],\n",
       "           [ 0.1328,  0.1055,  0.0310,  ..., -0.3203,  0.2891, -0.0791],\n",
       "           [-0.3008,  0.1260, -0.0737,  ..., -0.4473,  0.1475, -0.0151]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2051, -0.0101, -0.0811,  ...,  0.2539,  0.4355,  0.1777],\n",
       "           [-0.2891,  0.3516, -0.0032,  ..., -0.3770, -0.0947, -0.5000],\n",
       "           [ 0.1709, -0.1924, -0.1514,  ...,  0.1582, -0.6211, -0.4160],\n",
       "           ...,\n",
       "           [ 0.2383, -0.1006, -0.3984,  ..., -0.3730,  0.1309,  0.3633],\n",
       "           [ 0.4863,  0.4297, -0.2891,  ..., -1.1016,  0.3730, -0.2363],\n",
       "           [-0.2891,  0.2734, -0.4824,  ..., -0.5430,  0.1543, -0.5977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1167, -0.0493, -0.3184,  ...,  0.1045,  0.5312,  0.0952],\n",
       "           [-0.1650,  0.6133, -0.1943,  ..., -0.0859, -0.2070, -0.3086],\n",
       "           [ 0.0781, -0.0991, -0.7812,  ..., -0.2480, -0.6406, -1.0000],\n",
       "           ...,\n",
       "           [ 0.6250,  0.3555, -0.7500,  ..., -0.0154,  0.1465,  0.3965],\n",
       "           [ 0.7305,  1.1250, -0.3008,  ..., -1.1250,  0.4688,  0.0197],\n",
       "           [-0.2598,  0.5859, -0.7188,  ..., -0.4805,  0.2676, -0.8711]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 2.3535e-01,  8.3594e-01,  2.3828e-01,  ...,  3.3984e-01,\n",
       "             1.8359e-01,  7.5391e-01],\n",
       "           [-7.6562e-01,  3.3594e-01, -1.4648e-03,  ..., -4.6875e-01,\n",
       "            -7.4219e-01, -1.7188e+00],\n",
       "           [ 7.6562e-01,  5.8203e-01, -1.8594e+00,  ...,  6.9922e-01,\n",
       "            -1.4922e+00, -1.4062e+00],\n",
       "           ...,\n",
       "           [ 5.0781e-01, -7.9297e-01, -1.2969e+00,  ...,  6.2109e-01,\n",
       "             3.8281e-01,  1.3672e-01],\n",
       "           [ 7.7344e-01,  7.6562e-01, -7.2656e-01,  ..., -1.2188e+00,\n",
       "             1.4531e+00,  5.7617e-02],\n",
       "           [-5.5859e-01, -1.5625e-02, -4.0625e-01,  ..., -7.6562e-01,\n",
       "             5.0781e-01, -1.0312e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0197, -0.0786,  0.0776,  ..., -0.0325, -0.0062, -0.0432]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0205,  0.0811,  0.0898,  ..., -0.2148,  0.0337, -0.0620]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0200, -0.1934, -0.0283,  ..., -0.0233,  0.1016,  0.1318]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0352, -0.0742, -0.0591,  ...,  0.0742, -0.0400, -0.0121]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.0171,  0.1221,  ..., -0.3672,  0.1226, -0.1123]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5508,  0.2539, -0.3477,  ..., -0.4355,  0.1602, -0.6602]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8438,  0.3672, -0.5742,  ..., -0.7539,  0.5820, -0.6211]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-2.0938, -0.5156, -1.0312,  ..., -0.8906,  1.0938, -0.5391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0127,  0.0159,  0.0035,  ..., -0.0737, -0.0260, -0.0251]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0903,  0.0520, -0.0330,  ..., -0.1719, -0.0430,  0.1147]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934, -0.0957, -0.1943,  ..., -0.1143,  0.0776,  0.2676]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1602, -0.1211, -0.0698,  ..., -0.0239, -0.0378,  0.1387]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2139, -0.0457, -0.0449,  ..., -0.4434, -0.0074, -0.1406]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5938,  0.2773, -0.4121,  ..., -0.6992, -0.0581, -0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7969,  0.5586, -0.6250,  ..., -0.9727,  0.6562, -0.8398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.5312, -0.0820, -1.0859,  ..., -1.8750,  0.7422, -0.5859]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0459, -0.0311,  0.0165,  ..., -0.1836, -0.0197, -0.0077]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0737,  0.0703, -0.0664,  ..., -0.1895,  0.1182,  0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0239, -0.1357, -0.0957,  ..., -0.0376,  0.1348,  0.1709]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1299,  0.0520, -0.1660,  ...,  0.0288, -0.1250,  0.1855]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0178,  0.0151, -0.1514,  ..., -0.1270, -0.1797,  0.1367]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1309, -0.0349, -0.3555,  ..., -0.1914, -0.2969, -0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4336,  0.3418, -0.1650,  ...,  0.0679,  0.1396, -0.0747]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8086, -0.2422,  0.0508,  ..., -0.0854,  0.2148, -0.8320]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0120, -0.0161, -0.0075,  ..., -0.0132,  0.0026,  0.0004]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1187,  0.0938, -0.0337,  ..., -0.0420, -0.0124,  0.0532]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1099, -0.2031,  0.1064,  ..., -0.0376,  0.2539,  0.0835]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0503, -0.0176,  0.0298,  ...,  0.1777,  0.2246, -0.0059]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0347,  0.0498, -0.0063,  ...,  0.0576,  0.1533, -0.1201]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1289,  0.1914,  0.0242,  ..., -0.0469,  0.3164, -0.3984]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0703,  0.3945, -0.0820,  ..., -0.0820,  0.5703, -0.7812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6758, -0.7656, -0.7422,  ..., -0.8438,  1.1250, -1.2891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0121,  0.0242,  0.0094,  ..., -0.0388, -0.0131, -0.0112]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0479,  0.1021,  0.0234,  ..., -0.1807,  0.1299,  0.0439]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1196,  0.0278, -0.0491,  ..., -0.1226,  0.1128,  0.1221]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0835,  0.0339,  0.1211,  ..., -0.1074,  0.1089, -0.0623]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0486, -0.0525,  0.0304,  ..., -0.3359,  0.0698, -0.2617]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1572,  0.2578,  0.0220,  ..., -0.4805,  0.0605, -0.7070]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3242,  0.6250, -0.1650,  ..., -0.8984,  0.5664, -1.0781]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7617, -0.5430, -0.7070,  ..., -1.8594,  0.5312, -0.8594]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0269,  0.0121, -0.0830,  ...,  0.0115, -0.0312, -0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0344,  0.1553,  0.0264,  ...,  0.0522,  0.0239, -0.0381]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2324, -0.0396,  0.0635,  ..., -0.0229,  0.0352,  0.0540]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0164, -0.0054, -0.0122,  ..., -0.1465, -0.1582,  0.0820]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3027, -0.1201, -0.0464,  ..., -0.4336, -0.0781, -0.2148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1592,  0.3281,  0.0562,  ..., -0.3262, -0.3086, -0.7773]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1689,  0.8516, -0.1553,  ..., -0.7266, -0.3242, -1.4219]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7930,  0.0781, -0.0566,  ..., -1.9844, -0.6328, -1.4297]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0483,  0.0195, -0.0021,  ...,  0.0302, -0.0237, -0.0104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0205, 0.0957, 0.0674,  ..., 0.1826, 0.0042, 0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0742, -0.0732,  0.0466,  ...,  0.1426,  0.0923,  0.0393]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0164,  0.0933,  0.0015,  ...,  0.0032, -0.0713, -0.1035]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1582,  0.1035,  0.0820,  ..., -0.2852,  0.0200, -0.0586]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8320,  0.0305,  0.0305,  ..., -0.0732,  0.4434, -0.5156]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1484, -0.1504, -0.2178,  ...,  0.0435,  0.6602, -0.4824]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.9062, -1.5625, -0.5703,  ...,  0.5000,  1.2109, -0.2432]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0068, -0.0036,  0.0036,  ..., -0.0488, -0.0060, -0.0226]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0825,  0.0052, -0.0762,  ..., -0.0153, -0.0060, -0.0503]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1406, -0.1123,  0.0327,  ...,  0.1953,  0.0579,  0.0889]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0889, -0.0312, -0.0713,  ...,  0.0410,  0.0908,  0.0554]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1533, -0.0454, -0.0952,  ..., -0.1992,  0.0396, -0.0815]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5391,  0.0850,  0.2119,  ..., -0.2158,  0.1318, -0.5938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7578,  0.1133,  0.2334,  ..., -0.2734,  0.3438, -0.5742]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2969, -1.1484, -0.5430,  ..., -0.0420,  0.3633, -0.3418]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0236, -0.0157, -0.0576,  ..., -0.0178, -0.0894, -0.0181]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0236,  0.0371, -0.0259,  ...,  0.0493, -0.1162,  0.0776]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0762, -0.0591,  0.1699,  ...,  0.1113, -0.0547,  0.0889]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0400, -0.1064,  0.1514,  ...,  0.1152, -0.3516,  0.3184]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3086, -0.1318, -0.1172,  ..., -0.0786, -0.4883,  0.1094]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3945, -0.1631, -0.1523,  ..., -0.0801, -0.1484, -0.2266]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5938,  0.1660, -0.3359,  ..., -0.0977, -0.2617, -0.2539]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7695, -1.0703, -0.3535,  ...,  0.0186, -0.2422, -0.7031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0142,  0.0417, -0.0254,  ..., -0.0991, -0.0474, -0.0400]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0220,  0.2617, -0.0118,  ..., -0.0342,  0.1021, -0.0184]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0107,  0.0952,  0.0137,  ...,  0.0977,  0.0137,  0.1543]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0322,  0.0771,  0.0542,  ...,  0.0359, -0.0020,  0.0645]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1826,  0.0005, -0.0479,  ...,  0.0200, -0.1157, -0.2002]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1914,  0.4883, -0.1592,  ..., -0.2119,  0.0240, -0.1064]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1426,  0.7109,  0.2402,  ..., -0.2500,  0.0654, -0.1777]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0547,  0.9219, -0.6641,  ...,  0.5156, -0.2773, -0.1855]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0583,  0.0386,  0.0071,  ..., -0.0098, -0.0092, -0.0070]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1328,  0.1377,  0.0042,  ...,  0.1455, -0.0112,  0.0679]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0159,  0.0654, -0.0684,  ...,  0.2910, -0.0020,  0.1660]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0271, -0.0972, -0.0801,  ...,  0.2734, -0.0093,  0.0220]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2637, -0.0020,  0.0471,  ...,  0.3203, -0.0278, -0.1641]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1299,  0.3672, -0.1758,  ...,  0.0195, -0.1406, -0.4219]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6641,  0.6172, -0.4004,  ...,  0.3555, -0.0625, -0.5938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.5781,  0.0391, -0.7773,  ...,  0.2734, -0.5703, -0.6289]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0684,  0.0276,  0.0201,  ...,  0.0986,  0.0209, -0.0116]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0364,  0.0518, -0.0327,  ...,  0.0693, -0.0615, -0.0635]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1309,  0.0391,  0.1367,  ..., -0.0889, -0.0042, -0.0171]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0170, -0.0126,  0.0889,  ...,  0.0337, -0.0464, -0.1104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3262,  0.0879, -0.0164,  ..., -0.1855, -0.2695, -0.3633]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7305, -0.0281, -0.1445,  ..., -0.4102,  0.2041, -0.5938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8438,  0.2051, -0.1885,  ..., -0.5352,  0.3301, -0.3379]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9336, -0.4961, -0.5234,  ..., -0.5078,  0.6094, -0.9844]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0317, -0.0094, -0.0214,  ...,  0.0073,  0.0075, -0.0249]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0698,  0.1865,  0.1338,  ..., -0.0327,  0.0413,  0.0786]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2217, -0.0142, -0.0205,  ..., -0.1758,  0.0396,  0.3086]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0879,  0.0723,  0.2051,  ..., -0.1494,  0.1602,  0.1807]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0762,  0.2266,  0.2598,  ..., -0.2773, -0.0347, -0.4336]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1250,  0.3164, -0.1484,  ..., -0.5156,  0.3027, -0.5664]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0359,  0.5391, -0.2500,  ..., -0.5547,  0.6172, -0.7227]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4941, -1.0781, -0.5000,  ..., -0.7266,  1.0312, -1.1797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0166,  0.0150, -0.0269,  ..., -0.1128, -0.0223, -0.0231]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0713,  0.1299,  0.0059,  ..., -0.0391,  0.1758,  0.0031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0474,  0.0732, -0.1221,  ..., -0.0864,  0.1162,  0.1562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1338,  0.1582,  0.0435,  ..., -0.0339, -0.0068,  0.0154]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0024,  0.2598,  0.2344,  ..., -0.1738,  0.0092, -0.3594]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1074,  0.6445,  0.0225,  ..., -0.3809,  0.4102, -0.7109]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0830,  1.2109,  0.3184,  ..., -0.3867,  0.8984, -0.7344]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7539,  1.0859, -0.2070,  ...,  0.3750,  1.3047, -1.5469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0056,  0.0654, -0.0253,  ..., -0.0221, -0.0376,  0.0247]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0564,  0.1133, -0.0354,  ...,  0.0337,  0.0593, -0.0004]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1113,  0.0923, -0.1260,  ...,  0.0610,  0.1328, -0.0684]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1123,  0.1836,  0.0220,  ...,  0.1206,  0.0464, -0.0168]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3184,  0.3223,  0.1602,  ...,  0.0513,  0.2178, -0.3516]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5547,  0.7188,  0.0254,  ...,  0.1797,  0.3574, -0.9219]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5391,  1.1406,  0.0020,  ...,  0.4336,  0.4102, -1.5703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.6953,  0.0703, -0.2988,  ...,  0.5898,  0.2344, -2.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0171,  0.0349,  0.0275,  ...,  0.0312,  0.0087, -0.0145]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0771,  0.0471, -0.1216,  ..., -0.0425,  0.0420,  0.0165]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0060,  0.1445, -0.1064,  ..., -0.0635,  0.0386,  0.0688]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0166,  0.2695, -0.0762,  ..., -0.0452, -0.0166,  0.1592]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2188,  0.2656, -0.1777,  ..., -0.0957,  0.0420, -0.0356]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1602,  0.6367, -0.1641,  ...,  0.1123,  0.0137, -0.5938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1089,  0.9297, -0.2168,  ..., -0.0530,  0.2441, -1.1797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9922,  0.0078, -0.5000,  ...,  0.1904,  0.2188, -1.5625]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0004,  0.0215,  0.0177,  ...,  0.0396,  0.0505,  0.0635]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0879, -0.0967, -0.0371,  ...,  0.0542,  0.1089,  0.1074]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0283,  0.0371,  0.0005,  ...,  0.0315, -0.0188,  0.2275]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.1855, 0.0276, 0.1055,  ..., 0.0513, 0.0693, 0.1484]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0747,  0.2500,  0.0322,  ..., -0.0010, -0.0615, -0.0776]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6094,  0.7617,  0.0195,  ..., -0.2441, -0.1299, -0.2773]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5117,  1.1250,  0.1797,  ..., -0.3262, -0.3906, -0.5078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5391,  0.0312,  0.2676,  ..., -0.3555, -0.7930, -0.8984]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0121,  0.0096,  0.0171,  ...,  0.0493, -0.0430, -0.0089]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1006, -0.0127,  0.0255,  ...,  0.0625, -0.0271, -0.0135]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0247,  0.0496,  0.0486,  ...,  0.0260, -0.1162,  0.1162]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1240, -0.0349,  0.1641,  ...,  0.1045, -0.0220,  0.1318]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1885,  0.1670,  0.1523,  ..., -0.0776, -0.1934, -0.0513]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6797,  0.4004,  0.0017,  ..., -0.1436, -0.4219, -0.6172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5703,  0.9922,  0.0334,  ..., -0.3262, -0.7617, -0.8164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3945,  0.4062, -0.0098,  ..., -0.0327, -1.6719, -1.2031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0055,  0.0101,  0.0027,  ..., -0.0869,  0.0050,  0.0291]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0923,  0.0596,  0.1006,  ..., -0.0041,  0.0845, -0.0148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1016,  0.0262, -0.0166,  ...,  0.0090, -0.0425,  0.0430]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0378, 0.0718, 0.1729,  ..., 0.0552, 0.1030, 0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1191,  0.1055,  0.1021,  ...,  0.0190,  0.2139, -0.2051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3301,  0.3750, -0.0278,  ...,  0.2090,  0.3379, -0.4473]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0193,  0.6055, -0.5898,  ..., -0.0537,  0.6016, -0.5625]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1250, -0.0781, -0.7812,  ...,  0.2969,  1.1094, -1.2500]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0039,  0.0371,  0.0238,  ..., -0.1177,  0.0041,  0.0154]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0183,  0.0664,  0.0620,  ..., -0.1035,  0.0245,  0.0525]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0571,  0.0564,  0.0011,  ..., -0.0352,  0.0464,  0.1514]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1270,  0.1060,  0.1709,  ..., -0.0674,  0.0928,  0.0801]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0181,  0.1719,  0.0957,  ..., -0.2246,  0.1621, -0.2168]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0200,  0.5000,  0.0835,  ...,  0.1377,  0.2637, -0.7695]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0693,  0.8945, -0.1025,  ..., -0.1582,  0.7422, -1.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4961,  0.1562, -0.0244,  ..., -0.3516,  0.5977, -1.3359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0199, -0.0398, -0.0508,  ..., -0.0325, -0.0076, -0.0126]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0121, -0.0381,  0.0264,  ...,  0.0820, -0.0649,  0.0070]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0381,  0.0056,  0.1191,  ..., -0.0225,  0.0977,  0.0786]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3066, -0.0713,  0.1128,  ..., -0.0613, -0.1328,  0.0479]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1406,  0.1143,  0.2969,  ..., -0.1758, -0.0195, -0.3730]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1895,  0.2734,  0.2520,  ..., -0.0525, -0.0388, -0.6797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0996,  0.7422,  0.2598,  ..., -0.1582,  0.1689, -0.7148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7031,  0.3359,  0.7773,  ..., -0.4160,  0.5039, -1.4609]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0031,  0.0015, -0.0238,  ..., -0.0393, -0.0110, -0.0310]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0145,  0.0396,  0.0120,  ...,  0.0908,  0.0962, -0.0344]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0913,  0.1123, -0.0835,  ...,  0.0312, -0.0718,  0.1836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0139,  0.1309,  0.2197,  ..., -0.1387,  0.0854,  0.1445]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1011, -0.0400,  0.4727,  ..., -0.2197,  0.0566, -0.3203]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2168,  0.0762,  0.2012,  ..., -0.1152,  0.2539, -0.6719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0352,  0.3457,  0.1240,  ...,  0.1543,  0.5898, -0.7305]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6211, -1.1094,  0.0352,  ...,  0.2832,  0.9102, -1.3438]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0127, -0.0013, -0.0145,  ..., -0.1230, -0.0376, -0.0027]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0840,  0.0913, -0.0160,  ..., -0.0288,  0.1318, -0.0078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0830,  0.1738,  0.0068,  ..., -0.0271,  0.0781,  0.0322]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0728,  0.1328,  0.0137,  ..., -0.1108, -0.0967,  0.0298]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0483,  0.1621,  0.4180,  ..., -0.2148, -0.1289, -0.3359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1416,  0.3477,  0.2256,  ..., -0.1943,  0.2930, -0.6836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0254,  0.8867,  0.4434,  ..., -0.2051,  0.7500, -0.5547]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4004,  0.5547,  0.3750,  ...,  0.5898,  1.4844, -1.2969]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0153,  0.0347, -0.0352,  ..., -0.0188, -0.0325, -0.0081]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0938,  0.0542, -0.0056,  ...,  0.0105,  0.0654,  0.0024]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0830,  0.2012, -0.0703,  ...,  0.0015,  0.0908, -0.0908]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1245,  0.2285, -0.0129,  ...,  0.1216, -0.0073,  0.0107]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1885,  0.3730,  0.3809,  ...,  0.0767,  0.0442, -0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2676,  0.7266,  0.3555,  ...,  0.3906,  0.2129, -0.9336]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2324,  1.2188,  0.1797,  ...,  0.6445,  0.3477, -1.5625]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2500, -0.0156,  0.3281,  ...,  0.6953,  0.2031, -2.3906]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0211,  0.0276,  0.0267,  ...,  0.0226,  0.0024, -0.0226]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0991,  0.0420, -0.0786,  ..., -0.0125, -0.0425,  0.0131]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0018,  0.1465, -0.0938,  ..., -0.1035, -0.0371,  0.0068]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0159,  0.3125, -0.1572,  ...,  0.0601, -0.0588,  0.1279]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2422,  0.2656, -0.0518,  ..., -0.0840,  0.0281, -0.0684]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2344,  0.5781,  0.0649,  ...,  0.1562,  0.0212, -0.5703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0378,  0.8828, -0.0527,  ...,  0.0947,  0.2734, -0.9180]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8984,  0.0156, -0.0713,  ...,  0.5469,  0.4180, -1.1172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-9.1553e-05,  6.1035e-05,  5.4932e-03,  ...,  2.1851e-02,\n",
       "             4.7607e-02,  3.2471e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1030, -0.1133, -0.0623,  ...,  0.0444,  0.0869,  0.0413]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1094, -0.0312,  0.0449,  ..., -0.0630, -0.0195,  0.1914]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1074,  0.0918, -0.0762,  ...,  0.0038,  0.0347,  0.1855]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0991,  0.2256, -0.1069,  ..., -0.1060, -0.0107, -0.0184]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4746,  0.8164,  0.0393,  ..., -0.3418, -0.0310, -0.2256]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3789,  1.2188,  0.1621,  ..., -0.2402, -0.1846, -0.4277]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5742,  0.2266,  0.3555,  ..., -0.1855, -0.4473, -0.8203]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0126, -0.0130,  0.0120,  ...,  0.0386, -0.0339, -0.0276]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0947, -0.0320, -0.0400,  ...,  0.1104, -0.0713, -0.0243]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1543,  0.0630,  0.1133,  ...,  0.0284, -0.0488,  0.0303]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0461,  0.0806,  0.0288,  ...,  0.0352, -0.1465,  0.1172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0649,  0.0874, -0.0635,  ..., -0.1914, -0.0508,  0.0869]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2969,  0.3047, -0.1025,  ..., -0.3086, -0.3555, -0.3047]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2695,  0.8281, -0.2236,  ..., -0.2168, -0.5547, -0.4609]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6406,  0.2891,  0.0156,  ..., -0.0293, -1.1562, -0.9844]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0013, -0.0195, -0.0103,  ..., -0.1011,  0.0115,  0.0217]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0889,  0.0437,  0.0557,  ...,  0.0004, -0.0078, -0.0339]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0337,  0.0146,  0.0242,  ..., -0.0277, -0.0068, -0.0302]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0167,  0.1865, -0.0144,  ...,  0.1240,  0.1040,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1035,  0.0752, -0.0972,  ..., -0.0605,  0.4082, -0.1455]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2656,  0.2402, -0.0464,  ...,  0.3301,  0.7148, -0.3535]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0840,  0.2041, -0.4688,  ...,  0.0659,  1.0391, -0.3906]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2031, -0.8984, -0.5664,  ...,  0.2969,  1.7031, -0.7969]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0030,  0.0167,  0.0085,  ..., -0.1201, -0.0040,  0.0018]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0151,  0.0574,  0.0708,  ..., -0.0776, -0.0117,  0.0669]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0781,  0.1318,  0.1260,  ..., -0.1562,  0.1348,  0.0530]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0087, 0.1777, 0.1030,  ..., 0.1035, 0.0459, 0.1738]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0229,  0.1514,  0.1030,  ..., -0.1426,  0.2197, -0.2207]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0840,  0.2891,  0.1260,  ...,  0.1348,  0.4141, -0.5977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1582,  0.4824, -0.0039,  ..., -0.0996,  0.7344, -0.7734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8477, -0.4141,  0.2295,  ..., -0.6211,  0.8125, -1.1016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16)],\n",
       " 'self_attention': [tensor([[[ 1.8921e-03,  4.0894e-03, -3.3188e-04,  ...,  1.9043e-02,\n",
       "            -4.4250e-03, -2.3651e-03],\n",
       "           [ 8.7280e-03,  1.7853e-03,  1.4526e-02,  ...,  1.0864e-02,\n",
       "            -1.7090e-02,  3.1433e-03],\n",
       "           [-2.6245e-03,  6.1035e-05,  2.2278e-03,  ...,  2.2705e-02,\n",
       "            -6.8359e-03, -3.2043e-03],\n",
       "           ...,\n",
       "           [ 4.1199e-04, -5.6152e-03, -2.0142e-03,  ..., -3.6011e-03,\n",
       "             6.7139e-04, -1.5869e-03],\n",
       "           [-2.8229e-04,  1.3794e-02, -3.3569e-04,  ..., -3.1128e-02,\n",
       "             7.4463e-03,  1.1108e-02],\n",
       "           [-8.9111e-03,  4.8828e-04,  1.1658e-02,  ..., -2.0264e-02,\n",
       "            -2.5757e-02,  1.7578e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1128,  0.0574, -0.0723,  ...,  0.8477,  0.1738,  0.1138],\n",
       "           [ 0.0011,  0.0032,  0.0098,  ..., -0.0184, -0.0115,  0.0048],\n",
       "           [-0.1035,  0.0444, -0.0625,  ...,  0.7656,  0.1553,  0.1025],\n",
       "           ...,\n",
       "           [ 0.0084, -0.0287, -0.0027,  ..., -0.0059, -0.0094, -0.0052],\n",
       "           [ 0.0217,  0.0063,  0.0031,  ..., -0.0311,  0.0075,  0.0127],\n",
       "           [-0.0054, -0.0123,  0.0059,  ..., -0.0422, -0.0231,  0.0075]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1963e-01,  6.2256e-02, -6.6895e-02,  ...,  8.6328e-01,\n",
       "             1.8848e-01,  1.0889e-01],\n",
       "           [ 3.7537e-03,  1.3672e-02,  1.9043e-02,  ..., -5.5542e-03,\n",
       "            -5.4932e-04, -7.6294e-03],\n",
       "           [-1.0791e-01, -5.9509e-04, -7.5684e-02,  ...,  7.1875e-01,\n",
       "             1.8945e-01,  4.5654e-02],\n",
       "           ...,\n",
       "           [-6.1035e-04, -5.0781e-02,  3.6865e-02,  ..., -2.1973e-02,\n",
       "            -4.8218e-03, -5.3955e-02],\n",
       "           [ 9.7656e-04, -6.1035e-03,  4.7913e-03,  ..., -4.1504e-02,\n",
       "             1.5259e-03,  1.4771e-02],\n",
       "           [-3.4668e-02, -4.1016e-02,  4.6387e-02,  ..., -7.4219e-02,\n",
       "            -2.0874e-02,  2.2705e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0991,  0.0723, -0.0432,  ...,  0.9141,  0.1855,  0.1040],\n",
       "           [-0.0232, -0.0173,  0.0137,  ...,  0.0037,  0.0192,  0.0039],\n",
       "           [-0.0137,  0.0864, -0.0635,  ...,  0.7773,  0.1562,  0.0226],\n",
       "           ...,\n",
       "           [-0.0162, -0.0537,  0.0713,  ..., -0.0938, -0.0129, -0.0503],\n",
       "           [ 0.0605, -0.0337,  0.0591,  ..., -0.0718, -0.0537,  0.0781],\n",
       "           [-0.0674, -0.0811, -0.0659,  ..., -0.1206, -0.0344,  0.0261]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1226,  0.0718, -0.0417,  ...,  0.8906,  0.2197,  0.1138],\n",
       "           [ 0.0035, -0.0161,  0.0312,  ...,  0.0496,  0.0157,  0.0107],\n",
       "           [-0.0220,  0.0962, -0.1050,  ...,  0.7461,  0.1885,  0.0225],\n",
       "           ...,\n",
       "           [ 0.0082, -0.0332,  0.0337,  ..., -0.0947, -0.0771, -0.0190],\n",
       "           [ 0.0806, -0.0101,  0.0325,  ..., -0.0815, -0.1064,  0.0952],\n",
       "           [-0.0884, -0.0374, -0.1235,  ..., -0.0635, -0.0322, -0.0099]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1157,  0.0781, -0.0281,  ...,  0.8828,  0.2363,  0.1206],\n",
       "           [ 0.0035, -0.0079,  0.0024,  ...,  0.0129,  0.0229,  0.0146],\n",
       "           [-0.0240,  0.1182, -0.1230,  ...,  0.7109,  0.1729,  0.0229],\n",
       "           ...,\n",
       "           [ 0.0029,  0.0571,  0.0032,  ..., -0.2158, -0.0630, -0.0493],\n",
       "           [ 0.0674,  0.0413,  0.0610,  ..., -0.1504, -0.0249,  0.0122],\n",
       "           [-0.1128,  0.0200, -0.0542,  ..., -0.1055,  0.0090, -0.0410]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1221,  0.0981, -0.0195,  ...,  0.8398,  0.2451,  0.1167],\n",
       "           [ 0.0046, -0.0028,  0.0148,  ...,  0.0530,  0.0181,  0.0383],\n",
       "           [-0.0427,  0.1553, -0.0996,  ...,  0.6094,  0.1709,  0.0383],\n",
       "           ...,\n",
       "           [ 0.0024,  0.0505,  0.0513,  ..., -0.1221, -0.0369, -0.0879],\n",
       "           [ 0.1025, -0.0135,  0.0208,  ..., -0.1118, -0.0378,  0.0684],\n",
       "           [-0.0796, -0.0124, -0.0186,  ..., -0.0869, -0.0205, -0.0168]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0728,  0.1650, -0.0155,  ...,  0.8008,  0.2207,  0.1104],\n",
       "           [-0.0957,  0.0142,  0.0898,  ..., -0.0403, -0.0527,  0.0464],\n",
       "           [-0.0215,  0.1226, -0.0199,  ...,  0.5625,  0.1240, -0.0010],\n",
       "           ...,\n",
       "           [-0.0410, -0.0635, -0.0276,  ..., -0.1523,  0.0986, -0.0391],\n",
       "           [ 0.0491,  0.0356, -0.0304,  ..., -0.1348,  0.1504,  0.0275],\n",
       "           [-0.1611, -0.0063, -0.0062,  ..., -0.1416, -0.0166,  0.0698]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0806,  0.1982, -0.0013,  ...,  0.7344,  0.2305,  0.1157],\n",
       "           [-0.0820,  0.0859,  0.0728,  ...,  0.0281, -0.0247, -0.0236],\n",
       "           [-0.0459,  0.1152, -0.0189,  ...,  0.5273,  0.1191, -0.0391],\n",
       "           ...,\n",
       "           [ 0.0150, -0.1211,  0.0046,  ..., -0.1084,  0.0291, -0.0366],\n",
       "           [ 0.0510,  0.0131, -0.0037,  ..., -0.1670,  0.1089, -0.0266],\n",
       "           [-0.1582, -0.0618, -0.0386,  ..., -0.1602,  0.0199,  0.0247]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0684,  0.2217, -0.0308,  ...,  0.6875,  0.2500,  0.1465],\n",
       "           [-0.1001,  0.1514,  0.0598,  ..., -0.0332, -0.0164, -0.0034],\n",
       "           [-0.0261,  0.1953, -0.0400,  ...,  0.5195,  0.0386, -0.0713],\n",
       "           ...,\n",
       "           [ 0.0347, -0.0654, -0.0432,  ..., -0.1299,  0.2031, -0.0581],\n",
       "           [ 0.0449, -0.0188, -0.0283,  ..., -0.0957,  0.1914, -0.0559],\n",
       "           [-0.1523, -0.0403, -0.0320,  ..., -0.1719,  0.1035,  0.0269]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0027,  0.3047, -0.0178,  ...,  0.5703,  0.2812,  0.1631],\n",
       "           [-0.0762,  0.1494,  0.0273,  ..., -0.0327, -0.0383, -0.0134],\n",
       "           [-0.0522,  0.2031, -0.0430,  ...,  0.3574,  0.0437, -0.0354],\n",
       "           ...,\n",
       "           [ 0.0723, -0.0742, -0.0547,  ..., -0.1387,  0.1982, -0.1133],\n",
       "           [ 0.0737, -0.0747, -0.0483,  ..., -0.0801,  0.1582, -0.0044],\n",
       "           [-0.1216,  0.0046, -0.0762,  ..., -0.1660,  0.0820,  0.0732]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-6.1279e-02,  3.0078e-01, -2.1729e-02,  ...,  5.7031e-01,\n",
       "             2.9883e-01,  1.2256e-01],\n",
       "           [-1.3184e-01,  7.2266e-02,  8.3984e-02,  ...,  6.6406e-02,\n",
       "            -6.9824e-02,  4.0283e-02],\n",
       "           [-2.1680e-01,  9.4727e-02, -1.6968e-02,  ...,  3.5352e-01,\n",
       "             9.5703e-02, -9.1309e-02],\n",
       "           ...,\n",
       "           [ 3.6133e-02,  2.6855e-02, -7.8125e-02,  ..., -1.5234e-01,\n",
       "             1.6406e-01, -4.9072e-02],\n",
       "           [-4.8828e-04, -2.5781e-01, -2.1484e-02,  ..., -3.2471e-02,\n",
       "             1.7578e-01,  5.1270e-02],\n",
       "           [-5.9570e-02, -2.3828e-01, -2.3438e-01,  ..., -1.9043e-02,\n",
       "             7.2754e-02,  1.5234e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0498,  0.3203, -0.0071,  ...,  0.5000,  0.2832,  0.1128],\n",
       "           [-0.0732,  0.0884,  0.1001,  ...,  0.0466, -0.1128,  0.0027],\n",
       "           [-0.2178,  0.0684, -0.0601,  ...,  0.3125,  0.0645, -0.0674],\n",
       "           ...,\n",
       "           [-0.0198,  0.0181,  0.0327,  ..., -0.0361,  0.1108, -0.1270],\n",
       "           [ 0.0625, -0.2021, -0.0206,  ..., -0.0267,  0.1006, -0.0200],\n",
       "           [-0.0659, -0.2012, -0.1562,  ..., -0.0498,  0.0304,  0.0674]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0513,  0.3027, -0.0513,  ...,  0.4297,  0.2734,  0.1201],\n",
       "           [-0.0576,  0.0894,  0.0840,  ...,  0.0200, -0.0806,  0.0757],\n",
       "           [-0.2676,  0.0698, -0.0796,  ...,  0.2988,  0.0508, -0.1074],\n",
       "           ...,\n",
       "           [-0.0427,  0.0840,  0.1377,  ..., -0.1562,  0.1270, -0.1143],\n",
       "           [ 0.0574, -0.1494, -0.0133,  ..., -0.1226,  0.1143,  0.0334],\n",
       "           [ 0.0228, -0.1523, -0.1006,  ..., -0.1367,  0.0080,  0.1514]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0786,  0.2637, -0.0410,  ...,  0.4316,  0.3125,  0.0366],\n",
       "           [-0.0613,  0.0757,  0.0781,  ..., -0.0286, -0.1152,  0.0216],\n",
       "           [-0.3027, -0.0150, -0.0571,  ...,  0.2793,  0.0066, -0.1328],\n",
       "           ...,\n",
       "           [-0.1177,  0.0403,  0.0078,  ..., -0.1582,  0.1660, -0.0757],\n",
       "           [ 0.1108, -0.1025, -0.0518,  ..., -0.0566,  0.0732,  0.0645],\n",
       "           [-0.0087, -0.0547, -0.1895,  ..., -0.1514, -0.0703,  0.1689]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1084,  0.1533, -0.0107,  ...,  0.3164,  0.3516,  0.0146],\n",
       "           [-0.0933,  0.0957,  0.2090,  ..., -0.0786, -0.1592, -0.0439],\n",
       "           [-0.3340, -0.2090, -0.0728,  ...,  0.1729,  0.0425, -0.0559],\n",
       "           ...,\n",
       "           [ 0.0352,  0.0752,  0.0078,  ..., -0.2197,  0.1250, -0.0391],\n",
       "           [ 0.3398, -0.1436,  0.0869,  ..., -0.1504,  0.2227,  0.1758],\n",
       "           [ 0.0089, -0.1816,  0.0505,  ..., -0.1426, -0.1006,  0.2520]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1216,  0.1523, -0.0012,  ...,  0.3047,  0.3730,  0.0009],\n",
       "           [-0.0737,  0.0449,  0.2246,  ..., -0.1562, -0.1094, -0.0554],\n",
       "           [-0.3457, -0.2578, -0.0259,  ...,  0.1553,  0.0010, -0.0288],\n",
       "           ...,\n",
       "           [-0.0386,  0.0073,  0.0571,  ..., -0.1875,  0.2266, -0.1758],\n",
       "           [ 0.2871, -0.0806,  0.0420,  ..., -0.1602,  0.2100,  0.0171],\n",
       "           [ 0.0383, -0.1689,  0.0281,  ..., -0.2061, -0.0352,  0.1245]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1514,  0.1123,  0.0046,  ...,  0.2930,  0.4668, -0.0225],\n",
       "           [-0.1289,  0.0767,  0.1914,  ..., -0.2148, -0.1562, -0.1465],\n",
       "           [-0.3359, -0.2891,  0.0591,  ...,  0.1455,  0.0060, -0.1143],\n",
       "           ...,\n",
       "           [-0.1787, -0.0400,  0.0640,  ..., -0.1836,  0.2432, -0.1094],\n",
       "           [ 0.1279, -0.1328, -0.0117,  ..., -0.2012,  0.2754, -0.0352],\n",
       "           [-0.1865, -0.1270, -0.0486,  ..., -0.2500, -0.0283,  0.1182]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1680,  0.1079,  0.0198,  ...,  0.3047,  0.4023, -0.0493],\n",
       "           [-0.1875,  0.1279,  0.1865,  ..., -0.2598, -0.1309, -0.1738],\n",
       "           [-0.3320, -0.2002,  0.0317,  ...,  0.1196, -0.1484, -0.0938],\n",
       "           ...,\n",
       "           [-0.2363,  0.0181,  0.0908,  ..., -0.2578,  0.1631, -0.0938],\n",
       "           [ 0.0449, -0.1377,  0.0527,  ..., -0.1035,  0.2539, -0.0198],\n",
       "           [-0.1484, -0.1426,  0.0144,  ..., -0.2598, -0.0469,  0.1514]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1797,  0.0669,  0.0342,  ...,  0.2734,  0.5078,  0.0496],\n",
       "           [-0.1104,  0.1816,  0.2266,  ..., -0.3066, -0.0850, -0.1602],\n",
       "           [-0.2578, -0.2490,  0.1328,  ...,  0.1602, -0.2021, -0.1484],\n",
       "           ...,\n",
       "           [-0.2656, -0.2676,  0.2871,  ..., -0.4199,  0.1973, -0.1709],\n",
       "           [ 0.1162, -0.1855,  0.1396,  ..., -0.3320,  0.2168, -0.0229],\n",
       "           [-0.2656, -0.1641,  0.0554,  ..., -0.4199,  0.0171,  0.3516]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1699,  0.0461,  0.0283,  ...,  0.2578,  0.4551,  0.0605],\n",
       "           [-0.0479,  0.1445,  0.2148,  ..., -0.2715, -0.1367, -0.2432],\n",
       "           [-0.0625, -0.2656,  0.1177,  ...,  0.1406, -0.3242, -0.2285],\n",
       "           ...,\n",
       "           [-0.2148, -0.1953,  0.1328,  ..., -0.4648,  0.1172, -0.2793],\n",
       "           [ 0.0613, -0.1855,  0.0530,  ..., -0.3516,  0.3223, -0.1279],\n",
       "           [-0.2422, -0.1436, -0.0386,  ..., -0.2773,  0.1240,  0.3496]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.5430e-01,  3.3447e-02,  5.2246e-02,  ...,  2.8516e-01,\n",
       "             4.5312e-01,  6.8848e-02],\n",
       "           [-1.7578e-01,  1.9531e-03,  1.5332e-01,  ..., -2.9883e-01,\n",
       "            -5.1270e-02, -5.2344e-01],\n",
       "           [ 1.0352e-01, -2.5586e-01,  5.7129e-02,  ...,  1.5137e-01,\n",
       "            -4.5117e-01, -3.7695e-01],\n",
       "           ...,\n",
       "           [-2.9492e-01, -1.3672e-01,  2.3047e-01,  ..., -4.8438e-01,\n",
       "            -3.2227e-02, -2.7148e-01],\n",
       "           [-2.5757e-02, -1.9336e-01,  4.8828e-04,  ..., -3.8086e-01,\n",
       "             2.0508e-01, -1.0254e-01],\n",
       "           [-2.5000e-01, -8.4961e-02, -6.5918e-02,  ..., -3.6328e-01,\n",
       "             4.0039e-02,  3.1445e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1235,  0.0214,  0.0481,  ...,  0.3008,  0.4609,  0.0654],\n",
       "           [-0.2520,  0.1709,  0.2090,  ..., -0.2969, -0.0391, -0.5000],\n",
       "           [ 0.2891, -0.1387, -0.0337,  ...,  0.2070, -0.5859, -0.5273],\n",
       "           ...,\n",
       "           [-0.3242, -0.1660,  0.0928,  ..., -0.5039, -0.0034, -0.2598],\n",
       "           [-0.0557, -0.2891,  0.0109,  ..., -0.3945,  0.2520, -0.1699],\n",
       "           [-0.2344, -0.0049, -0.0869,  ..., -0.3848, -0.0066,  0.3535]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2100, -0.0232, -0.1250,  ...,  0.2363,  0.3887,  0.1562],\n",
       "           [-0.2676,  0.3496,  0.0630,  ..., -0.4727, -0.0776, -0.4473],\n",
       "           [ 0.2412, -0.1650, -0.2080,  ..., -0.0157, -0.6367, -0.5625],\n",
       "           ...,\n",
       "           [-0.0291, -0.0214, -0.2041,  ..., -0.6055,  0.0557, -0.3398],\n",
       "           [ 0.5898, -0.5352,  0.0588,  ..., -0.3984,  0.4102, -0.5391],\n",
       "           [-0.1406,  0.0048,  0.0243,  ..., -0.3926,  0.2070,  0.2012]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934, -0.0330, -0.1245,  ...,  0.2285,  0.3730,  0.1514],\n",
       "           [-0.2471,  0.4766,  0.0066,  ..., -0.3086, -0.1582, -0.4570],\n",
       "           [ 0.1836, -0.1807, -0.2559,  ..., -0.0601, -0.6953, -0.7734],\n",
       "           ...,\n",
       "           [-0.0781,  0.0527, -0.2520,  ..., -0.6328, -0.0544, -0.3262],\n",
       "           [ 0.5703, -0.4043,  0.0146,  ..., -0.3398,  0.4355, -0.4297],\n",
       "           [-0.3613, -0.0481,  0.0425,  ..., -0.3965,  0.1367,  0.2266]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2012, -0.0522, -0.1631,  ...,  0.2422,  0.3496,  0.1562],\n",
       "           [-0.2520,  0.4648,  0.0164,  ..., -0.4688, -0.1128, -0.4688],\n",
       "           [ 0.1562, -0.1484, -0.4609,  ..., -0.0820, -0.7656, -0.8828],\n",
       "           ...,\n",
       "           [-0.1426,  0.1201, -0.3203,  ..., -0.7734, -0.0264, -0.2598],\n",
       "           [ 0.6289, -0.4102, -0.0432,  ..., -0.2969,  0.4707, -0.4160],\n",
       "           [-0.4180,  0.1279,  0.1709,  ..., -0.2852,  0.2080,  0.2773]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1797, -0.0381, -0.1934,  ...,  0.2246,  0.3789,  0.1436],\n",
       "           [-0.2891,  0.5039,  0.0835,  ..., -0.4902, -0.1396, -0.4180],\n",
       "           [-0.0222, -0.1099, -0.6719,  ..., -0.2012, -0.7148, -0.8672],\n",
       "           ...,\n",
       "           [-0.2188,  0.2539, -0.4102,  ..., -0.6172, -0.0903, -0.3203],\n",
       "           [ 0.5352, -0.4277, -0.1230,  ..., -0.2832,  0.4629, -0.5117],\n",
       "           [-0.5781,  0.0830,  0.4023,  ..., -0.2871,  0.0508,  0.2559]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0938e-01, -2.9297e-02, -2.7148e-01,  ...,  1.4062e-01,\n",
       "             5.7812e-01,  1.1572e-01],\n",
       "           [-3.4180e-01,  7.8125e-01,  8.7891e-03,  ..., -1.4355e-01,\n",
       "            -3.9062e-01, -3.7500e-01],\n",
       "           [-3.8086e-02,  8.7891e-02, -9.8047e-01,  ..., -1.9141e-01,\n",
       "            -8.0859e-01, -1.1094e+00],\n",
       "           ...,\n",
       "           [-3.7109e-01,  3.7891e-01, -9.7656e-04,  ..., -1.6953e+00,\n",
       "            -1.6504e-01, -3.5352e-01],\n",
       "           [ 1.1094e+00, -4.5703e-01,  7.6660e-02,  ..., -4.1211e-01,\n",
       "             2.6367e-01, -3.7891e-01],\n",
       "           [-2.4512e-01, -7.8125e-02,  1.7871e-01,  ..., -3.6133e-01,\n",
       "            -2.1875e-01,  2.1094e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0708,  0.0259, -0.2949,  ...,  0.1562,  0.5742,  0.1426],\n",
       "           [-0.4238,  0.5234,  0.1172,  ..., -0.3418, -0.4023, -0.3340],\n",
       "           [ 0.0122,  0.2227, -1.0547,  ...,  0.0791, -0.8672, -1.2344],\n",
       "           ...,\n",
       "           [-0.5469,  0.0098, -0.0425,  ..., -1.6797, -0.1621, -0.3066],\n",
       "           [ 0.9531, -0.7266,  0.1089,  ..., -0.6016,  0.4688, -0.3652],\n",
       "           [-0.1973, -0.1455, -0.0205,  ..., -0.1768, -0.0547, -0.0322]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0396,  0.0908, -0.1641,  ...,  0.1050,  0.4062,  0.1338],\n",
       "           [-0.1230,  0.6250, -0.0068,  ..., -0.6289, -0.5781, -0.4883],\n",
       "           [-0.0547,  0.2012, -1.1719,  ...,  0.1738, -1.0469, -1.4453],\n",
       "           ...,\n",
       "           [-0.7266, -0.1494,  0.0938,  ..., -1.6562, -0.2129, -0.5742],\n",
       "           [ 0.9414, -0.9141, -0.1953,  ..., -0.6484,  0.3789, -0.2334],\n",
       "           [-0.1406, -0.3691, -0.1738,  ..., -0.1777, -0.4512, -0.2285]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 2.4414e-04,  2.5586e-01,  1.0742e-01,  ...,  1.5039e-01,\n",
       "            -2.0898e-01,  3.2812e-01],\n",
       "           [-5.6250e-01,  5.0781e-01, -3.2031e-01,  ..., -8.9844e-01,\n",
       "            -6.5234e-01, -1.0547e+00],\n",
       "           [ 3.3984e-01,  6.2500e-01, -1.7422e+00,  ...,  3.2812e-01,\n",
       "            -1.5469e+00, -1.8359e+00],\n",
       "           ...,\n",
       "           [-1.0469e+00,  7.9102e-02,  8.9844e-02,  ..., -1.9766e+00,\n",
       "             1.5234e-01, -9.7266e-01],\n",
       "           [ 6.4062e-01, -1.2109e+00, -1.6895e-01,  ..., -9.8047e-01,\n",
       "             4.3750e-01, -7.8125e-01],\n",
       "           [-2.3145e-01, -1.2793e-01, -9.9609e-02,  ..., -3.5156e-01,\n",
       "            -6.0156e-01, -6.2891e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4961,  2.6094,  0.9453,  ..., -0.9727,  1.9453,  2.7656],\n",
       "           [-1.1094,  1.9609,  0.2559,  ..., -0.5859,  1.2656, -1.0312],\n",
       "           [ 0.5469,  1.3750,  0.0156,  ..., -1.5625,  1.3359, -2.8125],\n",
       "           ...,\n",
       "           [-0.6953,  0.4453, -0.9141,  ..., -1.0078,  0.1914, -1.1328],\n",
       "           [ 0.5078, -1.0156, -0.3516,  ..., -0.1562,  1.0000, -0.3867],\n",
       "           [-0.7695,  0.0229, -0.1494,  ...,  0.7344, -0.7930, -0.7539]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0083, -0.0043,  0.0107,  ...,  0.0042,  0.0022, -0.0014]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0089, -0.0103, -0.0007,  ...,  0.0145, -0.0007, -0.0089]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0082, -0.0239, -0.0123,  ...,  0.0007,  0.0033, -0.0259]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-5.3711e-02, -1.8555e-02,  6.1035e-05,  ..., -6.3965e-02,\n",
       "             2.7222e-02, -4.5898e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0771,  0.0161,  0.0054,  ..., -0.0236, -0.0133, -0.0199]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0417,  0.1089,  0.0120,  ...,  0.0002, -0.0028, -0.0947]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0154,  0.1230,  0.0342,  ...,  0.0398, -0.0654, -0.0103]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0240,  0.1875, -0.1719,  ..., -0.0110, -0.0742,  0.0708]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0031,  0.0933, -0.0850,  ..., -0.0449, -0.1299, -0.0105]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0200,  0.0889, -0.0571,  ...,  0.0281,  0.0254, -0.0386]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0244,  0.0303, -0.1680,  ...,  0.0322,  0.0483,  0.0374]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0386,  0.1543, -0.1846,  ...,  0.0527,  0.0356, -0.0723]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0284,  0.1592, -0.1982,  ...,  0.0674, -0.0435, -0.1416]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0094,  0.0425, -0.0527,  ..., -0.0342, -0.0359,  0.1108]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0742,  0.0203, -0.0103,  ...,  0.0435,  0.0209,  0.0588]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1592,  0.0309,  0.0635,  ...,  0.1797, -0.1475,  0.2119]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2422,  0.0630,  0.0972,  ...,  0.1514, -0.2559,  0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1543,  0.0879,  0.0854,  ...,  0.1475, -0.2197,  0.1309]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1030,  0.1826,  0.2148,  ...,  0.1631, -0.1445,  0.2119]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0571,  0.2344,  0.1904,  ..., -0.0356,  0.0483,  0.0630]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0645,  0.2139,  0.1553,  ..., -0.0198,  0.0703,  0.0537]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0243,  0.2295,  0.2617,  ..., -0.1016,  0.1309,  0.0579]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0198,  0.1934,  0.2910,  ..., -0.1299,  0.1436,  0.0291]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3672,  0.4473,  0.2773,  ..., -0.2891,  0.2246,  0.0811]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2500,  0.4316,  0.3711,  ..., -0.1826,  0.1318, -0.0066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2256,  0.4824,  0.5000,  ..., -0.1924,  0.0693,  0.1582]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0889,  0.4258,  0.5625,  ..., -0.2354,  0.0776,  0.2158]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0142,  0.5781,  0.2012,  ..., -0.6484, -0.7227,  0.7266]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1309,  0.5039,  0.1025,  ..., -0.8125, -0.5117,  0.6719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0312, -0.1445,  0.0122,  ..., -0.8398, -0.6562,  0.6836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2754, -0.4062, -0.4004,  ..., -0.7539, -0.9062,  0.7109]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7852, -0.4336,  0.2109,  ...,  0.7734, -0.6016,  1.5156]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0198,  0.0063, -0.0067,  ..., -0.0396, -0.0173,  0.0003]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0091,  0.0027, -0.0099,  ..., -0.0393, -0.0020, -0.0071]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0002,  0.0188, -0.0135,  ..., -0.0527, -0.0244, -0.0086]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0016, -0.0064, -0.0547,  ..., -0.0938,  0.0162, -0.0679]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0194, -0.0018, -0.0206,  ..., -0.0596,  0.0122, -0.0300]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0413,  0.0112, -0.0430,  ..., -0.0684, -0.0195, -0.0022]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0654,  0.0256, -0.0432,  ..., -0.0840, -0.0087, -0.0135]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0645,  0.1123, -0.1416,  ..., -0.1113, -0.0122,  0.0664]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0212,  0.0879, -0.0459,  ..., -0.1387, -0.0376, -0.0129]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0337,  0.0571, -0.0449,  ..., -0.1514,  0.0698, -0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0625,  0.0962, -0.1250,  ..., -0.1514,  0.0933,  0.0078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1504,  0.0474, -0.0693,  ..., -0.0474, -0.0515, -0.0245]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0532,  0.1289, -0.0693,  ..., -0.0598, -0.1001, -0.0569]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0234,  0.1143,  0.0065,  ..., -0.0928,  0.0088,  0.1162]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0586,  0.1133,  0.0112,  ..., -0.0574,  0.0007,  0.1260]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1025, -0.1201, -0.0081,  ...,  0.0205, -0.0786,  0.0762]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1582,  0.0107, -0.1118,  ..., -0.0549, -0.1719,  0.0986]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1914,  0.0090, -0.1523,  ..., -0.0630, -0.1113,  0.0693]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1992,  0.0684,  0.0273,  ...,  0.0566, -0.0952,  0.2305]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1143,  0.2949,  0.3047,  ..., -0.1318,  0.2988,  0.0161]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0693,  0.3359,  0.1191,  ..., -0.0698,  0.2559,  0.1396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0610,  0.3418,  0.1533,  ..., -0.1562,  0.3906,  0.0845]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1387,  0.4023,  0.0762,  ..., -0.1738,  0.1992,  0.0815]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6250,  0.1338, -0.2275,  ..., -0.7109,  0.5508, -0.2441]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5195,  0.1641, -0.2246,  ..., -0.6133,  0.7031, -0.1387]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3867,  0.2695, -0.1846,  ..., -0.5508,  0.7148, -0.3535]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4688,  0.4531, -0.1211,  ..., -0.4941,  1.0391, -0.1738]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0076,  0.6445, -0.5195,  ..., -0.7695,  1.0859,  0.0986]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5742,  1.2500, -0.6016,  ..., -0.8086,  1.0938, -0.3008]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2539,  0.8203, -0.8828,  ..., -0.7812,  0.9219, -0.3926]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5938,  0.7773, -0.8672,  ..., -1.2500,  1.2656, -0.5391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.4609,  2.4062, -1.6562,  ..., -1.7344,  0.7148,  1.4531]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0261, -0.0005,  0.0093,  ...,  0.0079,  0.0142,  0.0099]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0332,  0.0143,  0.0123,  ..., -0.0044,  0.0247,  0.0063]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0140,  0.0479,  0.0151,  ...,  0.0005,  0.0120,  0.0249]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0025,  0.0613,  0.0298,  ...,  0.0400,  0.0361,  0.0083]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0280,  0.0825,  0.0272,  ...,  0.0703, -0.0112,  0.0272]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0299,  0.0835,  0.0332,  ...,  0.0613,  0.0198,  0.0388]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0320,  0.0908,  0.0039,  ...,  0.0845, -0.0150,  0.0447]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0087,  0.0115, -0.0011,  ..., -0.0049, -0.0525,  0.1406]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0298,  0.0723,  0.0703,  ..., -0.0265, -0.0476,  0.1079]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0371,  0.0513,  0.0435,  ..., -0.0209,  0.0204,  0.0703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0210,  0.0305, -0.0649,  ..., -0.0129,  0.0688,  0.0869]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0303,  0.0293, -0.1836,  ...,  0.1025, -0.0298,  0.1582]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1221,  0.1396, -0.0708,  ...,  0.1279, -0.0996,  0.0010]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1465,  0.1494,  0.0159,  ...,  0.1377, -0.0381,  0.1221]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1562,  0.2109,  0.0518,  ...,  0.1553,  0.0133,  0.1416]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1299,  0.1133,  0.0123,  ...,  0.1523,  0.0344,  0.0522]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1514,  0.1973,  0.0016,  ...,  0.0000, -0.0007,  0.1045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2051,  0.1108, -0.0527,  ...,  0.0708,  0.1206,  0.0420]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2314,  0.1196,  0.0718,  ...,  0.1934,  0.1826,  0.1826]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5625, -0.0708,  0.0488,  ...,  0.1943,  0.3418, -0.2793]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5273,  0.0010,  0.0020,  ...,  0.2520,  0.1885, -0.1875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6211,  0.0581,  0.1377,  ...,  0.1367,  0.3398, -0.1465]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4570,  0.1982,  0.1787,  ...,  0.1309,  0.1387, -0.2559]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2354,  0.2129, -0.0039,  ...,  0.2227,  0.3301, -0.6172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1719,  0.0938,  0.0608,  ...,  0.2012,  0.4023, -0.6836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2256,  0.1670,  0.0217,  ...,  0.0771,  0.4180, -0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3223,  0.1748, -0.0815,  ...,  0.2656,  0.5469, -0.7578]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1484,  0.1660, -0.2617,  ..., -0.5703,  0.7891, -0.2637]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0518,  0.1016, -0.5469,  ..., -0.5352,  1.1172, -0.5547]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1299, -0.2656, -0.9688,  ..., -0.1865,  0.8320, -0.0801]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3242, -0.6445, -1.5312,  ..., -0.9375,  0.9688, -0.3340]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6055, -0.7227, -2.6250,  ..., -1.3359,  1.1719,  0.6797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0082,  0.0134,  0.0066,  ...,  0.0208,  0.0245, -0.0077]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0057,  0.0236,  0.0037,  ..., -0.0021,  0.0096, -0.0269]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0175,  0.0369,  0.0109,  ...,  0.0386,  0.0142, -0.0126]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0469,  0.0415,  0.0050,  ...,  0.0233,  0.0266, -0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0415,  0.0603,  0.0147,  ...,  0.0320, -0.0017, -0.0356]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0742, 0.0811, 0.0186,  ..., 0.0298, 0.0159, 0.0264]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0474,  0.0576,  0.0173,  ...,  0.0654, -0.0310,  0.0195]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0615, 0.0249, 0.0366,  ..., 0.0151, 0.0133, 0.0277]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0044,  0.1006,  0.0591,  ..., -0.0183, -0.0513,  0.0415]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0002, 0.0635, 0.0991,  ..., 0.0444, 0.0085, 0.0493]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0100,  0.0630,  0.0488,  ..., -0.0193,  0.0486,  0.0317]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0038,  0.0771,  0.0674,  ...,  0.0225,  0.0996, -0.0016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0466,  0.1455,  0.0674,  ...,  0.0898, -0.0049, -0.0408]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0623,  0.1729,  0.1367,  ...,  0.1299, -0.0271,  0.0559]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0063, 0.2969, 0.1289,  ..., 0.0957, 0.0723, 0.1025]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0381,  0.2070,  0.1226,  ...,  0.1260,  0.0413,  0.0889]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0869,  0.5195,  0.2734,  ...,  0.0273, -0.0243,  0.1758]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0154,  0.5547,  0.1934,  ..., -0.0018,  0.1016,  0.2480]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0791,  0.5625,  0.2891,  ...,  0.1064,  0.0640,  0.3906]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3652,  0.3887,  0.4023,  ...,  0.2715,  0.1250, -0.0645]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3809,  0.4570,  0.3652,  ...,  0.3906, -0.0200, -0.1562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5781,  0.4863,  0.2754,  ...,  0.1270,  0.0923, -0.3320]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5234,  0.3770,  0.3184,  ...,  0.1738,  0.1436, -0.4062]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1436,  0.1475,  0.4023,  ...,  0.2373,  0.2754, -0.2773]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1162,  0.1348,  0.4609,  ...,  0.2168,  0.6250, -0.2656]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0435,  0.2500,  0.4355,  ...,  0.2383,  0.7773, -0.2754]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.2188,  0.7617,  ...,  0.2617,  0.8242, -0.3145]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2832,  0.1035,  0.8594,  ...,  0.3555,  0.7344, -0.0349]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8750, -0.2461,  1.4922,  ...,  0.1484,  0.5859, -0.0398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0391, -0.6797,  1.3984,  ...,  0.3359,  0.1475, -0.0076]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.6250, -1.0234,  1.0859,  ...,  0.4961,  0.4160, -0.0437]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.3438, -1.2656,  0.0898,  ...,  1.8906,  0.3828,  0.3828]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0058,  0.0080,  0.0010,  ..., -0.0005, -0.0037,  0.0081]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0084, -0.0011,  0.0010,  ..., -0.0249, -0.0192,  0.0026]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0064,  0.0031,  0.0016,  ...,  0.0027, -0.0266,  0.0057]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0325, -0.0030,  0.0261,  ..., -0.0415, -0.0113, -0.0029]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0217, -0.0023,  0.0356,  ..., -0.0277,  0.0156,  0.0220]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0206,  0.0459, -0.0095,  ..., -0.0220, -0.0388,  0.0146]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0339,  0.0369,  0.0143,  ...,  0.0208, -0.0439,  0.0884]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0732,  0.0708, -0.0342,  ...,  0.0564, -0.0723,  0.2021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0176, -0.0315, -0.0043,  ..., -0.0103, -0.0767,  0.0913]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0281,  0.0239,  0.0142,  ..., -0.0310, -0.0122,  0.0820]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0144,  0.0237,  0.0254,  ..., -0.0742, -0.0208,  0.0967]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0879,  0.0811,  0.0654,  ..., -0.0479, -0.0239,  0.0396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0967,  0.0962,  0.0391,  ..., -0.0415, -0.0425,  0.0422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0110,  0.1895,  0.1318,  ...,  0.0116, -0.0347,  0.2402]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-2.6855e-03,  5.3711e-02,  9.6680e-02,  ...,  4.5654e-02,\n",
       "            -1.2207e-04,  2.3242e-01]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0703,  0.1357,  0.2363,  ...,  0.0635, -0.0215,  0.2070]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0229,  0.2129,  0.0854,  ..., -0.0459, -0.1895,  0.1011]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0684,  0.1357,  0.0781,  ..., -0.0654, -0.0703,  0.1270]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0449,  0.2285,  0.1660,  ..., -0.1260, -0.0520,  0.1934]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0098, -0.0679,  0.1201,  ..., -0.2539, -0.1807, -0.0098]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0618,  0.0159,  0.1553,  ..., -0.2598, -0.2656,  0.0339]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0601,  0.1797,  0.0825,  ..., -0.4023, -0.1875,  0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0498,  0.3301,  0.2139,  ..., -0.4043, -0.2441,  0.2148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2168,  0.1426,  0.3770,  ..., -0.3848,  0.0132,  0.3984]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1631,  0.3086,  0.3574,  ..., -0.5508, -0.0264,  0.4570]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1562,  0.3320,  0.5273,  ..., -0.6094, -0.0742,  0.3926]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2217,  0.4004,  0.4395,  ..., -0.5938, -0.3555,  0.5664]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4492,  0.6055,  0.2656,  ..., -0.4102, -0.5195,  0.6641]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0078,  0.5664,  0.5391,  ..., -0.2910, -0.5156,  0.7734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1201,  0.2793,  0.7461,  ..., -0.0498, -0.6445,  0.7656]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2520, -0.0410,  0.6797,  ..., -0.2539, -0.3828,  0.4375]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3672, -0.3555,  0.8477,  ...,  1.0156, -0.0820,  0.8555]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0144,  0.0066, -0.0139,  ...,  0.0103, -0.0043, -0.0057]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0054,  0.0056, -0.0102,  ..., -0.0131, -0.0065,  0.0071]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0133,  0.0096, -0.0133,  ..., -0.0199,  0.0036,  0.0137]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0327, -0.0254,  0.0308,  ..., -0.0540, -0.0088, -0.0002]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0540, -0.0569,  0.0576,  ..., -0.0693,  0.0383,  0.0183]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0605, -0.0527, -0.0085,  ..., -0.0762,  0.0201,  0.0532]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0811, -0.0659,  0.0063,  ...,  0.0181, -0.0154,  0.0608]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0938, -0.0771, -0.0620,  ...,  0.0508, -0.0188,  0.0564]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0776, -0.0601, -0.0610,  ...,  0.0178, -0.0220,  0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1211, -0.0435,  0.0134,  ...,  0.1128,  0.0119,  0.0684]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1494, -0.0022,  0.0190,  ...,  0.0005,  0.0175,  0.0693]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0933, -0.0874, -0.0532,  ...,  0.0137,  0.1172,  0.0025]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0337,  0.0132, -0.0513,  ..., -0.0104,  0.0596, -0.0493]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0076,  0.1270,  0.0393,  ..., -0.0034,  0.0223,  0.0408]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0200,  0.0791,  0.0422,  ..., -0.1104,  0.0364,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0557,  0.0040,  0.0703,  ..., -0.2207, -0.0104,  0.1777]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0437,  0.0549,  0.1309,  ..., -0.2930, -0.0552,  0.1436]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0918,  0.0889,  0.1729,  ..., -0.4316,  0.0352,  0.2168]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0100,  0.2090,  0.1699,  ..., -0.4004,  0.2227,  0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0383,  0.1455,  0.1875,  ..., -0.6445,  0.2754,  0.2480]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2119,  0.2617,  0.2119,  ..., -0.6133,  0.4727,  0.2598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1348,  0.2852,  0.3711,  ..., -0.7812,  0.6172,  0.2695]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0303,  0.3750,  0.4570,  ..., -0.9375,  0.6094,  0.0820]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1875,  0.3652,  0.5000,  ..., -1.2109,  0.4590,  0.3125]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0869,  0.4414,  0.5742,  ..., -1.2891,  0.5781,  0.2930]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0557,  0.5938,  0.6367,  ..., -1.2109,  0.6406,  0.1797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0371,  0.7617,  0.6914,  ..., -1.0938,  0.6367,  0.0483]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2383,  0.9609,  0.7109,  ..., -1.1719,  0.7930,  0.1445]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3848,  0.9531,  0.8945,  ..., -1.1875,  0.7734,  0.2852]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2266,  0.4961,  0.7891,  ..., -0.8945,  0.5508,  0.1475]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1855,  0.2422,  0.7344,  ..., -1.3047,  0.7500, -0.3770]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7969,  0.3711,  1.0156,  ..., -0.0156,  0.4727,  0.5977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0265, -0.0028,  0.0107,  ..., -0.0095, -0.0225, -0.0004]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0454,  0.0195,  0.0216,  ..., -0.0383, -0.0264,  0.0023]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0742,  0.0227,  0.0437,  ..., -0.0947, -0.0082, -0.0023]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0337, -0.0308,  0.0361,  ..., -0.1621, -0.0308,  0.0226]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1133, -0.0674,  0.0569,  ..., -0.1133, -0.0703,  0.0581]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0796, -0.0151,  0.0664,  ..., -0.1221, -0.0154,  0.0083]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1484,  0.0146,  0.0107,  ..., -0.1572, -0.0801,  0.0304]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1465,  0.0674, -0.0806,  ..., -0.0110,  0.0708,  0.1104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1592,  0.1436,  0.0212,  ..., -0.0620, -0.0938,  0.0928]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1914,  0.0981,  0.0610,  ..., -0.0190,  0.0498,  0.0708]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1406,  0.0356,  0.0435,  ..., -0.0317,  0.0059,  0.0564]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1533,  0.1250, -0.0078,  ..., -0.0022,  0.1445,  0.1016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1367,  0.0352,  0.0309,  ..., -0.0505,  0.0859,  0.1118]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0181,  0.1064, -0.0115,  ..., -0.0781, -0.0830,  0.1553]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0227,  0.1670,  0.0569,  ..., -0.1611,  0.0239,  0.1387]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0947,  0.2344,  0.1777,  ..., -0.3145,  0.0074,  0.2148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0574,  0.2227,  0.0537,  ..., -0.2793, -0.1621,  0.2402]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2148,  0.2871,  0.2148,  ..., -0.3301, -0.1030,  0.3359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1348,  0.2871,  0.1836,  ..., -0.3555, -0.0413,  0.4414]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1650,  0.2373,  0.1406,  ..., -0.4062,  0.1475,  0.5000]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2100,  0.2891,  0.2617,  ..., -0.3633,  0.1475,  0.4844]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2461,  0.3008,  0.3535,  ..., -0.3867,  0.1572,  0.6367]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3477,  0.5664,  0.5273,  ..., -0.3652,  0.0610,  0.9883]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5156,  0.5117,  0.7695,  ..., -0.3555, -0.2129,  0.9297]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5352,  0.5273,  0.7773,  ..., -0.3145, -0.2266,  0.8086]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6992,  0.8281,  0.6328,  ..., -0.3867, -0.3438,  0.7852]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.7031,  0.9297,  0.5469,  ..., -0.3750, -0.1943,  0.9492]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.7773,  0.9141,  0.4922,  ..., -0.6367,  0.0469,  0.7617]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.7148,  0.9219,  0.5977,  ..., -0.5664, -0.1396,  0.8906]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.9609,  0.6055,  0.6523,  ..., -0.4297, -0.6289,  0.9648]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 1.0703,  0.6953,  0.1875,  ..., -0.5469, -0.7031,  0.8359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.9453,  0.5078, -0.5898,  ..., -0.0781, -0.1777,  0.5977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0142, -0.0052, -0.0106,  ..., -0.0243, -0.0189,  0.0031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0212, -0.0212, -0.0156,  ..., -0.0256, -0.0227, -0.0083]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0635, -0.0282,  0.0085,  ..., -0.0260, -0.0420,  0.0138]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0562, -0.0605, -0.0217,  ..., -0.0361, -0.0403, -0.0243]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0830, -0.0674, -0.0260,  ...,  0.0216, -0.0483,  0.0200]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0447, -0.0586,  0.0159,  ...,  0.0405, -0.0796, -0.0124]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1133, -0.0298, -0.0034,  ...,  0.0410, -0.0923,  0.0115]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1758,  0.0967,  0.0708,  ...,  0.1162, -0.0031,  0.0483]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1611,  0.0791,  0.1157,  ...,  0.0645, -0.1084,  0.0674]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2402,  0.0732,  0.1245,  ...,  0.1357,  0.0166,  0.0623]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1436,  0.0225,  0.0913,  ...,  0.0752, -0.0022,  0.0215]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1494,  0.0879,  0.0187,  ..., -0.1055, -0.0439, -0.0206]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0586,  0.0161,  0.0076,  ..., -0.0603, -0.0967,  0.0547]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0129,  0.0386,  0.0302,  ..., -0.0659, -0.2051,  0.1562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0107,  0.0801,  0.0021,  ..., -0.0171, -0.1250,  0.2051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0864,  0.2334,  0.0767,  ...,  0.0024, -0.3301,  0.2451]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0205,  0.1475,  0.0466,  ..., -0.0120, -0.2773,  0.2012]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0771,  0.1689,  0.1250,  ...,  0.0117, -0.1465,  0.3398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0410,  0.2520,  0.0527,  ..., -0.0349, -0.0537,  0.3770]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0593,  0.2139,  0.0564,  ..., -0.1299,  0.0811,  0.3359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0459,  0.1445,  0.0449,  ..., -0.0640,  0.1240,  0.4160]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1201,  0.0879,  0.0928,  ..., -0.1035,  0.0010,  0.4922]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2539,  0.1904,  0.1895,  ..., -0.1553, -0.0267,  0.3262]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.5078, -0.0732,  0.0996,  ..., -0.3945, -0.2246,  0.4648]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4082,  0.0325,  0.0942,  ..., -0.4238, -0.0771,  0.5039]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4883,  0.0820,  0.2422,  ..., -0.2871, -0.0522,  0.4473]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6094,  0.2734,  0.2852,  ..., -0.2500,  0.0598,  0.3242]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0635,  0.5664,  0.0762,  ..., -0.5742,  0.3867,  0.5195]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2930,  0.6445,  0.2354,  ..., -0.4512,  0.2891,  0.7422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5000,  0.4512,  0.2695,  ..., -0.6484,  0.2129,  0.7891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6992, -0.0625, -0.0176,  ..., -0.8203,  0.2578,  0.5078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9258,  0.2930,  0.3770,  ...,  0.6484, -0.0293,  0.0703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0148,  0.0046,  0.0035,  ..., -0.0004,  0.0015,  0.0123]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0012, -0.0074, -0.0159,  ...,  0.0155, -0.0078,  0.0028]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0444,  0.0178, -0.0079,  ...,  0.0025, -0.0048, -0.0041]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0752, -0.0444, -0.0479,  ...,  0.0303, -0.0284, -0.0601]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0771, -0.0649, -0.0481,  ...,  0.0942, -0.0366, -0.0308]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0486, -0.0454, -0.0039,  ...,  0.0811, -0.0549, -0.0479]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1030, -0.0337, -0.0361,  ...,  0.0635, -0.0703, -0.0554]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1670,  0.0042, -0.1514,  ...,  0.1660, -0.0518, -0.0762]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1426, -0.0205, -0.0688,  ...,  0.1611, -0.1025, -0.0566]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1030, -0.0403,  0.0322,  ...,  0.1602,  0.0024, -0.0928]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0306, -0.0693,  0.0237,  ...,  0.0586,  0.0332, -0.0688]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0566, -0.0918, -0.0369,  ..., -0.1045,  0.0593, -0.1426]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0596, -0.0708, -0.0254,  ..., -0.0938, -0.0664, -0.1318]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0537, -0.0347,  0.0601,  ..., -0.0588, -0.0933, -0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0073, -0.0513,  0.1133,  ..., -0.1152,  0.0352,  0.1196]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0527,  0.1875,  0.2031,  ..., -0.0312, -0.3320,  0.2256]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0645,  0.1758,  0.0591,  ...,  0.1074, -0.3574,  0.2109]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1660,  0.1069,  0.1348,  ...,  0.0820, -0.2461,  0.2412]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1143,  0.1592,  0.0596,  ...,  0.0503, -0.1875,  0.2236]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3594,  0.0918,  0.2285,  ..., -0.1099, -0.0391, -0.0508]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3105,  0.2578,  0.2520,  ..., -0.2119, -0.0442, -0.0898]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1660,  0.2363,  0.3535,  ..., -0.2188, -0.0298, -0.0640]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1855,  0.2891,  0.5039,  ..., -0.2676, -0.0103, -0.1118]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0439,  0.3145,  0.3711,  ..., -0.5430,  0.0371,  0.0049]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0024,  0.2891,  0.3457,  ..., -0.4492, -0.0142, -0.0427]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1060,  0.1895,  0.4141,  ..., -0.3066,  0.0176, -0.0430]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1309,  0.2227,  0.5859,  ..., -0.2676,  0.1758,  0.1309]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4004,  0.0693,  0.3828,  ..., -0.0040,  0.1387, -0.1699]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5625,  0.1133,  0.3789,  ..., -0.0425,  0.2949,  0.0459]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5391, -0.1260,  0.4727,  ...,  0.0732,  0.0801,  0.2285]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1562, -0.3066,  0.2539,  ..., -0.0029,  0.2480,  0.2275]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9102, -0.2178, -0.2695,  ...,  0.3066,  0.6406,  0.2617]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0058,  0.0004,  0.0079,  ..., -0.0127, -0.0081,  0.0065]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0086, -0.0081,  0.0031,  ..., -0.0094, -0.0071, -0.0021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0161,  0.0126,  0.0193,  ...,  0.0060, -0.0035, -0.0151]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0249, -0.0228,  0.0220,  ..., -0.0469,  0.0078, -0.0420]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0117, -0.0217, -0.0258,  ..., -0.0500, -0.0060, -0.0066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0133, -0.0101,  0.0317,  ..., -0.0693, -0.0425,  0.0225]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0273, -0.0151,  0.0017,  ..., -0.0776, -0.0227,  0.0204]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0811,  0.0010, -0.0454,  ..., -0.0210,  0.1289,  0.2129]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0593, -0.0356, -0.0129,  ..., -0.0527,  0.0215,  0.1748]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0623, -0.0435, -0.0356,  ..., -0.0247,  0.0518,  0.0400]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0170, -0.0133, -0.0586,  ..., -0.0713,  0.0293,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1196, -0.0762, -0.0282,  ..., -0.1455,  0.0408, -0.0342]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0903, -0.0164, -0.0688,  ..., -0.1045, -0.0063, -0.0337]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0488,  0.1167,  0.0684,  ..., -0.1191, -0.0566,  0.0598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1152,  0.0098,  0.0947,  ..., -0.1582, -0.0138,  0.1963]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0210, -0.0388,  0.2070,  ..., -0.1670, -0.4219,  0.2520]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0154,  0.1021,  0.0596,  ..., -0.0728, -0.3906,  0.2500]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0820,  0.0176,  0.0869,  ..., -0.0029, -0.3047,  0.3125]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0640,  0.0723,  0.0898,  ..., -0.0247, -0.2617,  0.1973]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0104,  0.0486,  0.0242,  ...,  0.0645, -0.1680, -0.0244]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0991,  0.0767,  0.0073,  ..., -0.0229, -0.1484, -0.0510]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1045,  0.1226,  0.0179,  ..., -0.0293, -0.1904, -0.0129]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0146,  0.0811,  0.1670,  ..., -0.1914, -0.1113, -0.1138]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0557,  0.5078,  0.4863,  ..., -0.2402, -0.0835,  0.0276]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0282,  0.5820,  0.5469,  ..., -0.2539, -0.0635, -0.1035]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0625,  0.5977,  0.5664,  ..., -0.2344, -0.1113, -0.0903]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0603,  0.6445,  0.5312,  ..., -0.2168, -0.0432, -0.0124]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2129,  0.5156,  0.8555,  ..., -0.4453, -0.0815, -0.2324]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2061,  0.6445,  1.0000,  ..., -0.3418,  0.0190, -0.1084]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4043,  0.5938,  1.0469,  ..., -0.5703, -0.1641, -0.1836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7070,  0.2812,  0.6406,  ..., -0.6953, -0.2031,  0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7891,  0.2119,  0.6992,  ..., -0.2227, -0.4922,  0.5039]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0088, -0.0396,  0.0031,  ...,  0.0199, -0.0068, -0.0036]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0137, -0.0376,  0.0098,  ..., -0.0046, -0.0132, -0.0186]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0342,  0.0015,  0.0596,  ...,  0.0400, -0.0327, -0.0356]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0776,  0.0332,  0.0315,  ..., -0.0028, -0.0210, -0.0513]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0542,  0.0366,  0.0182,  ...,  0.0449, -0.0087, -0.0271]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0679,  0.0679,  0.0415,  ...,  0.0388, -0.0342, -0.0234]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1030,  0.0044, -0.0427,  ...,  0.0126, -0.0518,  0.0000]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1309, -0.0306, -0.0219,  ...,  0.0566,  0.0859,  0.0251]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0088, -0.0439,  0.0850,  ...,  0.0176, -0.0234,  0.0032]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0231, -0.0432,  0.0525,  ...,  0.0269,  0.0444, -0.0330]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0471, -0.0684,  0.0537,  ..., -0.0139,  0.0063,  0.0479]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1279, -0.1816, -0.0015,  ..., -0.0830, -0.0304, -0.0859]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1426, -0.1245, -0.0603,  ..., -0.0212,  0.0178, -0.0400]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1230, -0.1006, -0.0081,  ..., -0.0525, -0.0417,  0.0728]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1006, -0.0522,  0.0271,  ..., -0.1128,  0.0081,  0.1611]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0378, -0.0986,  0.1445,  ..., -0.0598, -0.2773,  0.2988]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0596, -0.0649,  0.0840,  ...,  0.0190, -0.3242,  0.2832]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0176, -0.0933,  0.0718,  ...,  0.0039, -0.1348,  0.2354]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0449,  0.0752,  0.0242,  ..., -0.1729, -0.0430,  0.2793]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0623,  0.0669, -0.0757,  ...,  0.1050,  0.1348,  0.0527]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1211,  0.0762, -0.1235,  ...,  0.0820,  0.2109,  0.1621]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0564,  0.0569, -0.0977,  ...,  0.1816,  0.1885,  0.0537]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1104,  0.1553, -0.1084,  ...,  0.3047,  0.0273, -0.0605]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1260,  0.0840,  0.0635,  ...,  0.3320, -0.0488,  0.1680]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0967,  0.0488, -0.0762,  ...,  0.3750,  0.0400,  0.2617]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0391,  0.0608, -0.1709,  ...,  0.4453,  0.0723,  0.4492]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0625,  0.0153, -0.1611,  ...,  0.5664,  0.1157,  0.6016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3789, -0.0137,  0.1299,  ..., -0.0845,  0.0654,  0.1289]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6328, -0.3555,  0.1045,  ...,  0.0396, -0.2148,  0.1562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5469, -0.9297, -0.2148,  ...,  0.2559, -0.5977,  0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1953, -1.2344, -0.8594,  ...,  0.3047, -0.8359,  0.4180]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9961, -0.6016, -0.5703,  ...,  1.2188, -0.3438,  0.3301]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0025, -0.0082, -0.0005,  ..., -0.0293, -0.0104,  0.0022]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0029, -0.0048, -0.0063,  ..., -0.0415, -0.0222, -0.0018]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0410,  0.0001,  0.0122,  ..., -0.0391, -0.0215, -0.0186]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0874, -0.0278,  0.0201,  ..., -0.0515, -0.0245,  0.0098]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0674,  0.0027,  0.0173,  ..., -0.0303,  0.0131,  0.0342]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0781, -0.0155,  0.0056,  ..., -0.0986,  0.0258,  0.0240]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1191, -0.0144, -0.0713,  ..., -0.0430, -0.0176,  0.0503]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1445,  0.0188, -0.0327,  ..., -0.0145, -0.0273,  0.1279]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0156,  0.0269,  0.0188,  ..., -0.0801, -0.0986,  0.0679]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0415, -0.0498, -0.0001,  ..., -0.0332, -0.0283,  0.0605]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0620, -0.0811,  0.0265,  ..., -0.0464,  0.0166,  0.1084]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1309, -0.0674, -0.0298,  ..., -0.1221, -0.1133, -0.1182]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1504, -0.0586, -0.1016,  ..., -0.0439, -0.1475, -0.1367]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1631,  0.0273, -0.0718,  ..., -0.1147, -0.1602, -0.0342]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1836,  0.0344, -0.1230,  ..., -0.2295, -0.0146,  0.1040]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2109,  0.0444, -0.0752,  ..., -0.2539, -0.1523,  0.0153]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2715,  0.0840, -0.1250,  ..., -0.2021, -0.0679,  0.0547]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2656, -0.0347, -0.1216,  ..., -0.2002,  0.0981,  0.1377]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3340,  0.0459,  0.0195,  ..., -0.2656,  0.0251,  0.2109]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3242, -0.0854,  0.0566,  ..., -0.2197,  0.0605,  0.0791]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4414,  0.0913,  0.0420,  ..., -0.1953,  0.0806,  0.1172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5195, -0.0103,  0.1235,  ..., -0.3066,  0.0203,  0.1260]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5273, -0.1006,  0.1328,  ..., -0.1426, -0.0674,  0.1436]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3711,  0.1719,  0.2539,  ...,  0.1064, -0.0820,  0.2188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3203,  0.1279,  0.1523,  ...,  0.1680, -0.3066,  0.2715]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3125,  0.1387, -0.0093,  ...,  0.1001, -0.3887,  0.2402]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5039,  0.2051,  0.2305,  ...,  0.2324, -0.5156,  0.3672]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0625,  0.4746,  0.0498,  ..., -0.0742, -0.7734,  0.7227]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9297,  0.5156, -0.1182,  ..., -0.2471, -0.3926,  0.6367]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2188,  0.2031, -0.0610,  ...,  0.0137, -0.5586,  1.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9141, -0.5078, -0.3320,  ..., -0.0225, -0.4805,  1.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0625, -0.0273,  0.5547,  ...,  0.4062, -0.1250,  1.2422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0017,  0.0106, -0.0003,  ..., -0.0349,  0.0068,  0.0123]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0291,  0.0078,  0.0089,  ..., -0.0557,  0.0035,  0.0114]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0122,  0.0009,  0.0113,  ..., -0.0654,  0.0060,  0.0005]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0031, -0.0178, -0.0120,  ..., -0.0884,  0.0048,  0.0493]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0065, -0.0110, -0.0096,  ..., -0.0693, -0.0132,  0.0649]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0251, -0.0337, -0.0215,  ..., -0.1709, -0.0267,  0.0142]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0723, -0.0513, -0.0077,  ..., -0.1216, -0.0085, -0.0107]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0537, -0.0693,  0.0137,  ...,  0.0010, -0.0073, -0.0092]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0144, -0.1338,  0.0820,  ..., -0.1289, -0.0206, -0.0515]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0029, -0.2207,  0.1338,  ..., -0.0850, -0.0151, -0.0327]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0376, -0.2217,  0.1328,  ..., -0.0684, -0.0085,  0.0032]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0623, -0.2910,  0.1289,  ...,  0.0078,  0.0388, -0.0153]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0420, -0.2676,  0.0151,  ..., -0.0090,  0.0159, -0.0718]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0193, -0.1167,  0.0698,  ..., -0.1138, -0.0162, -0.0352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0869, -0.0688,  0.0059,  ..., -0.0625,  0.0103,  0.0349]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1680, -0.0757,  0.0938,  ..., -0.1748, -0.0337,  0.2578]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2676,  0.0374,  0.0664,  ..., -0.0947, -0.0625,  0.2148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2236, -0.0303, -0.0156,  ..., -0.0918,  0.1094,  0.1523]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0547, -0.1836, -0.0339,  ..., -0.0500,  0.1377,  0.2490]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0176, -0.2100, -0.0337,  ..., -0.0535,  0.2100,  0.2012]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1738, -0.1875, -0.0309,  ..., -0.1768,  0.3594,  0.1514]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1836, -0.3848, -0.0500,  ..., -0.3105,  0.1807,  0.1367]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1895, -0.5547, -0.1465,  ..., -0.2617,  0.1689,  0.1152]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0540, -0.4062, -0.0898,  ..., -0.4590,  0.3438,  0.0131]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0554, -0.3457, -0.0957,  ..., -0.4238,  0.3164, -0.1069]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1406, -0.3242, -0.1416,  ..., -0.3320,  0.5156, -0.0815]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1069, -0.2363, -0.0747,  ..., -0.3594,  0.6367, -0.0598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0449, -0.6680, -0.0605,  ..., -0.2637,  0.7031, -0.0098]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0488, -0.7891, -0.1128,  ..., -0.1777,  0.7812,  0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1289, -1.2344, -0.0918,  ..., -0.4375,  0.6055,  0.3711]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0063, -1.6406, -0.8750,  ..., -0.5117,  0.7266,  0.6719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2256, -1.5625, -0.6875,  ..., -0.3984,  1.1250,  0.3691]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0080, -0.0010,  0.0127,  ..., -0.0242, -0.0254,  0.0198]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0092, -0.0182, -0.0007,  ..., -0.0488, -0.0216,  0.0101]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0508, -0.0239,  0.0240,  ..., -0.0630, -0.0215,  0.0171]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0850, -0.0601, -0.0469,  ..., -0.1299, -0.0425,  0.0315]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0967, -0.0466, -0.1118,  ..., -0.0437, -0.0320, -0.0176]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1025, -0.0100, -0.0684,  ..., -0.0469, -0.0522, -0.0045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0894, -0.0525, -0.0483,  ..., -0.0007, -0.0415, -0.0182]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0284, -0.0469, -0.0664,  ..., -0.0186,  0.0157,  0.0752]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0322, -0.1562, -0.0518,  ..., -0.0447, -0.0212, -0.0076]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0082, -0.1768, -0.0083,  ..., -0.0168,  0.0251, -0.0101]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0251, -0.1436, -0.0354,  ..., -0.0457,  0.0466,  0.0010]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0093, -0.1475, -0.0535,  ...,  0.1138, -0.0171, -0.0217]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0466, -0.1211, -0.0654,  ..., -0.0120, -0.0100, -0.0713]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0110, -0.0645,  0.0293,  ..., -0.0767, -0.0811,  0.0649]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0171, -0.0121, -0.0508,  ..., -0.1328, -0.1074,  0.1348]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0035, -0.0630,  0.1621,  ..., -0.1348, -0.2070,  0.3145]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0845, -0.1089,  0.1611,  ..., -0.0674, -0.1924,  0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0381, -0.1064,  0.1377,  ..., -0.0579, -0.0459,  0.3496]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0271, -0.2070,  0.1040,  ..., -0.0120, -0.0674,  0.4102]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1875, -0.4434, -0.0098,  ..., -0.0564,  0.0249,  0.1934]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2051, -0.3945, -0.0013,  ..., -0.0723,  0.0337,  0.1416]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1982, -0.3320,  0.0806,  ..., -0.1719,  0.0361,  0.1245]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1963, -0.2598,  0.1777,  ..., -0.0737,  0.0204,  0.0830]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2168, -0.3145,  0.2793,  ..., -0.0033,  0.0649,  0.1768]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2969, -0.1426,  0.2363,  ..., -0.0991,  0.0305,  0.1172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3418, -0.0527,  0.1157,  ..., -0.1787,  0.1729,  0.0864]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4434,  0.1001,  0.1748,  ..., -0.3418,  0.1816,  0.2676]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8008, -0.0698,  0.2070,  ...,  0.1504,  0.4980,  0.4121]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7227, -0.0889,  0.0146,  ...,  0.1064,  0.4648,  0.5352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5820, -0.3125, -0.4199,  ...,  0.0278,  0.0000,  0.4062]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8242, -0.2334, -0.7109,  ...,  0.0688, -0.3242,  0.3262]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8672,  0.3594,  0.0723,  ...,  1.0703, -0.8477,  0.4629]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0007, -0.0018,  0.0096,  ...,  0.0023,  0.0005,  0.0037]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0059, -0.0033, -0.0027,  ...,  0.0140, -0.0079,  0.0006]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0034,  0.0015,  0.0050,  ...,  0.0317, -0.0205, -0.0016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0302, -0.0166, -0.0149,  ...,  0.0281, -0.0081, -0.0029]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0457, -0.0334, -0.0356,  ...,  0.1221, -0.0093, -0.0432]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0625, -0.0371,  0.0195,  ...,  0.0645,  0.0108, -0.0183]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0933, -0.0059,  0.0152,  ...,  0.0427,  0.0044, -0.0298]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0889,  0.0249, -0.0137,  ...,  0.0457,  0.0284,  0.0120]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1162,  0.0415, -0.0049,  ..., -0.0046, -0.0552,  0.0201]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0476,  0.0194,  0.0075,  ...,  0.0962,  0.0317, -0.0269]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0437, -0.0366, -0.0156,  ...,  0.0081,  0.0234,  0.0210]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0962, -0.0256,  0.0141,  ...,  0.1777, -0.0109, -0.0859]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1230, -0.0454, -0.0244,  ...,  0.0840,  0.0164, -0.1982]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1230,  0.0093,  0.1270,  ..., -0.0508, -0.0566,  0.0127]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1133, -0.0234,  0.0601,  ..., -0.1318, -0.0095,  0.0635]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0781,  0.0300,  0.1914,  ..., -0.2500, -0.1123,  0.0806]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0732,  0.0728,  0.2207,  ..., -0.2168, -0.2246,  0.0481]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0088,  0.0056,  0.1465,  ..., -0.1592, -0.2236, -0.0308]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0776,  0.0522,  0.2139,  ..., -0.1357, -0.1152,  0.0464]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2324,  0.0403,  0.2715,  ..., -0.3750, -0.1147,  0.0059]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3652, -0.0654,  0.2695,  ..., -0.4141, -0.0471,  0.0918]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3105, -0.0315,  0.2676,  ..., -0.5234, -0.0293,  0.1445]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1338, -0.0386,  0.1689,  ..., -0.4609, -0.1367,  0.1001]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0889, -0.0410, -0.2715,  ..., -0.6172, -0.2305,  0.4688]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1357,  0.2949, -0.1934,  ..., -0.5977, -0.1602,  0.5195]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0654,  0.4531, -0.1729,  ..., -0.5078, -0.1904,  0.6484]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1562,  0.5391, -0.0557,  ..., -0.4805, -0.1191,  0.7461]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8828,  0.7539,  0.3203,  ..., -0.5430, -0.7344,  1.1094]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9727,  0.4805,  0.2031,  ..., -0.5938, -0.6641,  1.1797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8750,  0.4082, -0.1553,  ..., -0.4824, -1.1719,  1.3203]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.2109,  0.3789, -0.4824,  ..., -0.1748, -1.7969,  0.7188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.4219,  0.7734,  0.0361,  ...,  0.9688, -2.0625,  0.8164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0081,  0.0068, -0.0055,  ..., -0.0315, -0.0120, -0.0002]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0012, -0.0043, -0.0052,  ..., -0.0291,  0.0046, -0.0055]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0090,  0.0123,  0.0243,  ..., -0.0601, -0.0190, -0.0171]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0190, -0.0206, -0.0027,  ..., -0.1064,  0.0071, -0.0398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0332, -0.0254,  0.0045,  ..., -0.0496,  0.0186,  0.0024]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0127, -0.0190, -0.0237,  ..., -0.0305,  0.0277,  0.0051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0598,  0.0269, -0.0493,  ..., -0.0228, -0.0262,  0.0076]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0942,  0.0435, -0.0801,  ...,  0.0020,  0.0430,  0.1289]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0447,  0.0449,  0.0193,  ..., -0.0183, -0.0461,  0.0728]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0732,  0.0193,  0.0117,  ..., -0.0005,  0.0542, -0.0127]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1133,  0.0403, -0.1572,  ..., -0.0259,  0.0825,  0.0295]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2070, -0.0388, -0.0703,  ..., -0.0352, -0.0303,  0.0101]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1777,  0.0023, -0.0747,  ..., -0.0991, -0.0457, -0.0249]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1348,  0.0728,  0.0664,  ..., -0.1328, -0.0898,  0.1094]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0247,  0.1206,  0.1328,  ..., -0.0811, -0.1270,  0.1177]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0308, -0.0415,  0.1216,  ..., -0.0198, -0.2461, -0.0479]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0635,  0.0142,  0.0530,  ..., -0.1201, -0.3066,  0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0869, -0.0249,  0.0466,  ..., -0.0664, -0.2207,  0.0525]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0220, -0.0255,  0.0918,  ...,  0.0459, -0.1260,  0.2139]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1758, -0.0166,  0.2637,  ...,  0.0530,  0.0996,  0.3066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0693,  0.0222,  0.1582,  ...,  0.1406,  0.1641,  0.3867]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0498,  0.0378,  0.1128,  ...,  0.0225,  0.1455,  0.3379]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0737,  0.0278, -0.0635,  ...,  0.0464,  0.0396,  0.2871]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4453,  0.0366, -0.4824,  ..., -0.2539,  0.3848,  0.2090]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3418,  0.2695, -0.3926,  ..., -0.0928,  0.5703,  0.5117]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3086,  0.2988, -0.4102,  ..., -0.1357,  0.6523,  0.3828]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3496,  0.4141, -0.5352,  ..., -0.2432,  0.8984,  0.5547]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0361,  0.7695, -0.2070,  ..., -0.5938,  0.9062,  0.3750]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3359,  0.9336, -0.2402,  ..., -0.6172,  1.0000,  0.0273]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0273,  0.5391, -0.6172,  ..., -0.5469,  0.8008, -0.2090]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1670,  0.5938, -0.7031,  ..., -0.8125,  1.0078, -0.3457]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4316,  1.9375, -0.5977,  ..., -1.3672,  0.9219,  0.6523]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0135,  0.0138,  0.0012,  ..., -0.0150,  0.0058,  0.0030]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0076,  0.0046, -0.0107,  ..., -0.0239,  0.0084, -0.0028]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0033,  0.0172, -0.0156,  ..., -0.0483,  0.0103, -0.0178]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0139,  0.0311,  0.0088,  ..., -0.0132,  0.0464, -0.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0134,  0.0299,  0.0148,  ...,  0.0214,  0.0461, -0.0188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0908,  0.0249, -0.0057,  ...,  0.0330,  0.0173, -0.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0820,  0.0420, -0.0361,  ...,  0.0386, -0.0371, -0.0405]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0342, -0.0854, -0.0576,  ...,  0.0654,  0.0327,  0.0090]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0137, -0.0549,  0.0444,  ...,  0.0393, -0.0457, -0.0190]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0273, -0.0664,  0.1055,  ...,  0.0209,  0.0168, -0.0203]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0894, -0.0664,  0.0225,  ..., -0.0227,  0.0547, -0.0420]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0376, -0.1348, -0.0112,  ...,  0.0183,  0.1133, -0.0247]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0791, -0.0684, -0.0055,  ..., -0.0486,  0.0698, -0.0439]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0742, -0.0288,  0.0996,  ..., -0.1187,  0.0679,  0.0425]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1089,  0.0664,  0.2305,  ..., -0.1396,  0.0403,  0.0027]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1289, -0.0059,  0.2246,  ..., -0.1494, -0.0859,  0.0586]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1553,  0.0264,  0.1128,  ..., -0.2734, -0.0059,  0.0693]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2617, -0.0139,  0.1152,  ..., -0.1992,  0.0579,  0.1348]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1289, -0.0576,  0.0713,  ..., -0.3125, -0.0249,  0.1143]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0053, -0.1270,  0.2031,  ..., -0.7227,  0.0664, -0.2275]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0569, -0.1348,  0.1270,  ..., -0.7969,  0.1787, -0.2188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0044, -0.1377,  0.1133,  ..., -0.8477,  0.3945, -0.2480]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0327, -0.0073,  0.0703,  ..., -0.8320,  0.3848, -0.3477]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3164, -0.0674,  0.0532,  ..., -0.9414,  0.3418, -0.1904]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2734,  0.1973,  0.1230,  ..., -1.0078,  0.3711, -0.2598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2988,  0.1660, -0.0762,  ..., -1.0391,  0.5078, -0.3398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1816, -0.0430, -0.0859,  ..., -1.0312,  0.5547, -0.2451]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2773, -0.1406, -0.2148,  ..., -1.0312,  0.3633,  0.1123]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1025, -0.1211,  0.0195,  ..., -1.2031,  0.3340,  0.2891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0415, -0.3516, -0.3281,  ..., -1.3359,  0.0088,  0.1699]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2734, -0.7305, -0.7344,  ..., -1.7422,  0.4453,  0.1475]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3359, -0.8633, -0.8438,  ..., -1.3984,  0.8633,  1.0469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0106, -0.0044,  0.0356,  ..., -0.0322, -0.0084, -0.0026]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0273, -0.0080,  0.0361,  ..., -0.0410, -0.0137, -0.0155]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0048,  0.0119,  0.0271,  ..., -0.0503, -0.0332, -0.0084]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0308,  0.0022,  0.0078,  ..., -0.0884, -0.0081, -0.0044]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0410,  0.0203,  0.0037,  ..., -0.0674, -0.0430, -0.0554]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1045,  0.0381,  0.0027,  ..., -0.0300,  0.0117, -0.1001]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0483, -0.0094, -0.0352,  ...,  0.0014, -0.0742, -0.0229]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0272, -0.0635, -0.0540,  ..., -0.0210,  0.0947,  0.0342]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0096,  0.0400, -0.0144,  ...,  0.0009, -0.0449,  0.0304]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0525, -0.0166,  0.0091,  ..., -0.0003, -0.0007,  0.0139]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0508, -0.0089, -0.0359,  ..., -0.0420,  0.0197, -0.0123]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0352, -0.0020, -0.1196,  ..., -0.0070,  0.0518,  0.0544]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1211, -0.0923, -0.0620,  ..., -0.0498,  0.0034, -0.0205]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1699, -0.0347, -0.0269,  ..., -0.1494,  0.0082,  0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1738, -0.0190,  0.1064,  ..., -0.1289, -0.0024, -0.0183]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1934,  0.0228,  0.1953,  ..., -0.1279, -0.0049, -0.0098]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2402,  0.0723,  0.0781,  ..., -0.1787, -0.0186, -0.0752]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2227,  0.1299,  0.1484,  ..., -0.1484,  0.0181, -0.0107]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0718,  0.0981,  0.2500,  ..., -0.2207,  0.0120, -0.0127]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0645,  0.3125,  0.3984,  ..., -0.4531,  0.2500, -0.0913]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0410,  0.2412,  0.2178,  ..., -0.6016,  0.1992, -0.0483]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0028,  0.1660,  0.4297,  ..., -0.7188,  0.1680,  0.0732]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0591,  0.2031,  0.3555,  ..., -0.6953,  0.1270,  0.0337]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0737,  0.0928,  0.3457,  ..., -1.1797,  0.0559,  0.0854]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0034, -0.0195,  0.2969,  ..., -1.1797,  0.2070,  0.2090]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1602,  0.2158,  0.3359,  ..., -1.1875,  0.4336,  0.1934]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1523,  0.2520,  0.2461,  ..., -1.4219,  0.5820,  0.2578]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0586,  0.2617,  0.4453,  ..., -1.7578,  0.0420,  0.3555]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3105,  0.1855,  0.6523,  ..., -1.8672, -0.2461,  0.5078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4375, -0.1221,  0.2793,  ..., -1.9531, -0.4297,  0.5977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.8125, -0.2578, -0.5273,  ..., -2.1250, -0.4395,  0.3613]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2031,  0.4492, -0.3555,  ..., -2.0938,  0.0322, -0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0040,  0.0100,  0.0225,  ...,  0.0212, -0.0177,  0.0066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0109,  0.0165,  0.0262,  ...,  0.0121, -0.0134,  0.0322]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0038,  0.0444,  0.0500,  ...,  0.0216, -0.0393,  0.0188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0330,  0.0020,  0.0742,  ..., -0.0295, -0.0405, -0.0056]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0425,  0.0179,  0.0869,  ...,  0.0459, -0.0305, -0.0160]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0498, -0.0029,  0.1270,  ...,  0.1240, -0.0173, -0.0057]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0084,  0.0090,  0.0278,  ...,  0.1504, -0.0654,  0.0352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1357,  0.0034, -0.0571,  ...,  0.1807,  0.0747,  0.1602]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0889,  0.0312,  0.0181,  ...,  0.1406, -0.0281,  0.1191]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0610, -0.0081,  0.1172,  ...,  0.1729, -0.0013,  0.0781]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0928, -0.0815,  0.0742,  ...,  0.1299, -0.0289,  0.0562]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0027, -0.0947, -0.0085,  ...,  0.1836, -0.0579, -0.0374]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1279, -0.0854,  0.0159,  ...,  0.0659, -0.0884,  0.0024]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1797,  0.0015,  0.0864,  ..., -0.0203, -0.0923,  0.0124]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1875,  0.0850,  0.0806,  ...,  0.0459, -0.1069, -0.1162]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2119,  0.1797,  0.1992,  ..., -0.0947, -0.1523,  0.1191]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1426,  0.1309,  0.1572,  ..., -0.0208, -0.0498,  0.1543]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2969,  0.0447,  0.1826,  ...,  0.0244, -0.1826,  0.2217]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2793,  0.0039,  0.2246,  ..., -0.0177, -0.2148,  0.2734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2207,  0.1660,  0.3086,  ..., -0.2012,  0.0825,  0.0376]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1777,  0.0166,  0.0547,  ..., -0.2715,  0.1387,  0.1660]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1621, -0.0273,  0.0288,  ..., -0.3945,  0.1973,  0.1738]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1416, -0.1094,  0.0854,  ..., -0.3926,  0.0415,  0.1699]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2617, -0.2930,  0.2031,  ..., -0.7070, -0.2119,  0.1768]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0010, -0.3555,  0.1885,  ..., -0.7109, -0.4062,  0.2754]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0189, -0.1875,  0.0962,  ..., -0.7422, -0.3242,  0.0732]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0151, -0.1816,  0.0540,  ..., -0.7930, -0.2773,  0.1875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3711,  0.0122, -0.2188,  ..., -1.0469, -0.3281,  0.0251]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1973,  0.2061, -0.0747,  ..., -0.9023, -0.4805,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1221,  0.0869, -0.1787,  ..., -0.8672, -0.6016,  0.1045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0454,  0.1357, -0.8906,  ..., -1.0156, -0.4199, -0.1396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1699,  0.2637, -1.1328,  ..., -0.9453,  0.2559, -0.5352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0036, -0.0047, -0.0013,  ...,  0.0023,  0.0011,  0.0120]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0106, -0.0276, -0.0002,  ...,  0.0037, -0.0131,  0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0220, -0.0256,  0.0100,  ..., -0.0342, -0.0483,  0.0229]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0469, -0.0430,  0.0317,  ..., -0.0466, -0.0625,  0.0229]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0417, -0.0332,  0.0840,  ...,  0.0143, -0.0503,  0.0022]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0991, -0.0103,  0.1406,  ..., -0.0366, -0.0198, -0.0118]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0400,  0.0106,  0.1016,  ..., -0.0183, -0.0684,  0.0072]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0554,  0.0742,  0.0549,  ..., -0.0781,  0.0063,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0605,  0.0957,  0.0967,  ..., -0.1245, -0.0654,  0.0330]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0018,  0.0137,  0.0244,  ..., -0.0981,  0.0947, -0.0059]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0193,  0.0325,  0.0103,  ..., -0.1279,  0.0811, -0.0212]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0918, -0.1768, -0.0850,  ..., -0.0479,  0.0986,  0.0156]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0645, -0.1602, -0.0354,  ..., -0.0312,  0.0439,  0.0474]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1836, -0.0708,  0.0713,  ..., -0.0913, -0.0270,  0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2656, -0.0515,  0.1074,  ..., -0.0098, -0.1484, -0.0420]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3477, -0.2041,  0.2285,  ...,  0.0791, -0.2207, -0.0354]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3633, -0.1680,  0.1738,  ..., -0.0093, -0.2637,  0.1167]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4766, -0.2930,  0.0977,  ..., -0.0024, -0.2656,  0.2197]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4219, -0.2314,  0.1855,  ..., -0.0791, -0.3711,  0.3594]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3945, -0.2266,  0.2773,  ..., -0.3848, -0.0742,  0.3652]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2471, -0.1328,  0.1992,  ..., -0.3379, -0.0112,  0.4043]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2314, -0.1816,  0.2139,  ..., -0.6094,  0.0374,  0.3984]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3027, -0.3477,  0.4414,  ..., -0.7148,  0.1250,  0.4473]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3125, -0.5156,  0.6328,  ..., -0.9141, -0.2598,  0.7578]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1445, -0.6875,  0.6953,  ..., -1.0234, -0.2871,  0.8203]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0283, -0.4336,  0.5234,  ..., -0.9922, -0.2676,  0.6445]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3086, -0.3008,  0.4082,  ..., -1.0234, -0.3398,  0.7383]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2930,  0.0215,  0.3242,  ..., -1.0781, -0.7266,  0.3223]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3125, -0.0315,  0.5195,  ..., -0.9062, -0.8359,  0.5625]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2324,  0.0967,  0.6406,  ..., -0.9102, -0.8789,  0.7383]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1865, -0.2500,  0.2695,  ..., -1.1797, -1.0391,  0.6172]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2598,  0.6250,  0.1128,  ..., -0.2695, -0.8633,  0.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0116, -0.0186, -0.0016,  ...,  0.0040, -0.0102, -0.0053]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0082, -0.0342,  0.0126,  ..., -0.0234, -0.0038,  0.0018]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0067, -0.0713,  0.0366,  ..., -0.0457, -0.0289, -0.0155]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0051, -0.0376,  0.0012,  ..., -0.0366, -0.0101, -0.0811]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0082, -0.0203,  0.0121,  ..., -0.0123, -0.0559, -0.0449]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0317, -0.0850,  0.0996,  ...,  0.0065, -0.0664, -0.0430]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0325, -0.0996,  0.0554,  ...,  0.0928, -0.0845, -0.0923]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0286, -0.0635, -0.0044,  ...,  0.0781, -0.0381,  0.0503]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0076, -0.0240,  0.0396,  ...,  0.0845, -0.1582,  0.0034]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0752, -0.0703,  0.0417,  ...,  0.1138, -0.0962,  0.0043]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0537, -0.1338,  0.1016,  ...,  0.0625, -0.1235,  0.0481]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0410, -0.1494, -0.0449,  ...,  0.0459, -0.0923,  0.0791]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0356, -0.1533,  0.0242,  ...,  0.0073, -0.0815,  0.1260]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0625, -0.0449, -0.0127,  ..., -0.0315, -0.1138,  0.1074]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1221, -0.0165, -0.0371,  ...,  0.0479, -0.1514,  0.0146]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2520,  0.0361,  0.0825,  ...,  0.1768, -0.2139, -0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2520, -0.0835, -0.0273,  ...,  0.1797, -0.0820,  0.0713]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2539, -0.0229, -0.1396,  ...,  0.1289, -0.0381, -0.0352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0771, -0.0918, -0.0498,  ...,  0.1260, -0.1562,  0.0991]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1455, -0.1069, -0.0386,  ...,  0.0830,  0.1152,  0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1182, -0.1377, -0.1650,  ...,  0.0928,  0.1328,  0.3555]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0864, -0.0649, -0.2168,  ..., -0.0015,  0.0171,  0.4922]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1836, -0.0540, -0.1855,  ...,  0.0137, -0.0093,  0.3125]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1602, -0.1113, -0.1289,  ..., -0.0845,  0.0815,  0.3359]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934, -0.2539, -0.1875,  ..., -0.1118,  0.0884,  0.2734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4277, -0.1147, -0.2285,  ..., -0.1885,  0.1768,  0.1895]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4961, -0.0713, -0.2871,  ..., -0.0449,  0.3750,  0.2598]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7383,  0.4531, -0.0957,  ..., -0.0410,  0.7383,  0.3887]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.9844,  0.6133, -0.0532,  ...,  0.0200,  0.4941,  0.5820]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5820,  0.3066, -0.1816,  ...,  0.1445,  0.4492,  0.4434]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3184,  0.0693, -0.5625,  ...,  0.2090,  0.2734,  0.4043]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2314,  0.6602, -0.9570,  ...,  0.4531,  0.4883,  0.4453]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0092, -0.0195, -0.0050,  ...,  0.0096, -0.0081,  0.0089]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0101, -0.0479, -0.0084,  ..., -0.0020,  0.0129,  0.0168]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0138, -0.0371,  0.0055,  ..., -0.0077, -0.0225,  0.0398]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0056, -0.0107, -0.0019,  ...,  0.0205, -0.0256,  0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0312,  0.0073,  0.0132,  ...,  0.0510, -0.0317,  0.0149]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0291, -0.0168,  0.1270,  ...,  0.0120, -0.0645, -0.0425]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0026, -0.0410,  0.1069,  ...,  0.0640, -0.0894, -0.0211]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0581, -0.0376,  0.0178,  ...,  0.1079, -0.0542,  0.0703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0476, -0.0400,  0.0933,  ...,  0.0801, -0.0811,  0.0415]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0742, -0.0962,  0.0459,  ...,  0.1152,  0.0359, -0.0535]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0615, -0.1016,  0.0311,  ...,  0.0161, -0.0063,  0.0078]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1104, -0.1885, -0.0610,  ..., -0.0874, -0.0011,  0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0718, -0.1396, -0.0310,  ..., -0.0005, -0.0483,  0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0054, -0.0181, -0.0276,  ..., -0.0874, -0.0083,  0.0381]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0022,  0.0493, -0.0337,  ..., -0.0146, -0.0327,  0.0232]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0244,  0.0210,  0.0947,  ...,  0.1406, -0.1240, -0.1021]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0342, -0.0138,  0.2012,  ...,  0.2520, -0.0806, -0.0767]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0708, -0.1650,  0.1123,  ...,  0.2480, -0.0703, -0.1162]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0352, -0.0293,  0.1592,  ...,  0.2422, -0.1289, -0.1816]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0164,  0.0020,  0.1875,  ...,  0.1689,  0.0566, -0.2373]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0334, -0.0908,  0.1602,  ...,  0.1260,  0.0156, -0.2480]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1387, -0.1182,  0.0625,  ...,  0.0674,  0.1426, -0.2754]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1660, -0.1670,  0.0051,  ...,  0.0398,  0.0742, -0.3652]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0566, -0.1914,  0.1504,  ..., -0.2070,  0.1553, -0.2295]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2490, -0.3164,  0.0845,  ..., -0.0356,  0.2451, -0.4141]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2402, -0.4258,  0.1079,  ..., -0.0908,  0.3652, -0.6094]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2158, -0.4668,  0.0068,  ..., -0.1045,  0.3711, -0.6602]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0605, -0.3750,  0.3145,  ..., -0.4102,  0.8008, -0.7109]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1035, -0.3359,  0.4590,  ..., -0.6992,  0.8438, -0.5234]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2432, -0.7148,  0.3398,  ..., -0.6094,  0.5000, -0.4824]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6094, -0.6094, -0.0996,  ..., -0.4746,  0.2832, -0.6289]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934, -1.3125, -0.2852,  ..., -0.4121,  0.2930, -0.2363]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0036, -0.0010,  0.0156,  ...,  0.0035,  0.0148,  0.0178]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0036,  0.0046,  0.0157,  ..., -0.0083,  0.0209,  0.0240]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0032,  0.0194,  0.0203,  ..., -0.0300,  0.0082,  0.0601]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0029,  0.0020,  0.0168,  ..., -0.0459, -0.0134,  0.0801]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0352, -0.0094,  0.0182,  ..., -0.0179,  0.0251,  0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0266,  0.0111,  0.0991,  ..., -0.0586,  0.0459,  0.0444]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0476, -0.0095,  0.0825,  ..., -0.0012,  0.0225,  0.1055]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0679,  0.0771,  0.0972,  ..., -0.0264,  0.0408,  0.2520]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0222,  0.0898,  0.0415,  ...,  0.0022,  0.0059,  0.2480]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0322,  0.0205,  0.0554,  ...,  0.0488,  0.0811,  0.2090]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0183,  0.0371,  0.0737,  ..., -0.0552,  0.0532,  0.2148]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0552, -0.0728,  0.0332,  ..., -0.0903,  0.0942,  0.2832]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0547, -0.1089, -0.0009,  ..., -0.0605,  0.1143,  0.2988]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0129,  0.0405, -0.0479,  ..., -0.0991,  0.1992,  0.2305]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.1299, 0.1128, 0.0508,  ..., 0.0557, 0.1797, 0.2852]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2578,  0.0400,  0.2178,  ...,  0.0197, -0.0262,  0.1768]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.3730, 0.1855, 0.1445,  ..., 0.0938, 0.0108, 0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3418,  0.0527, -0.0264,  ...,  0.0247,  0.0118,  0.0864]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2949,  0.1797,  0.0381,  ..., -0.0005,  0.1719,  0.0522]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3457,  0.1924,  0.0420,  ..., -0.0498,  0.2354, -0.0471]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3398,  0.2715,  0.0400,  ..., -0.2334,  0.2490, -0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2578,  0.4922,  0.1816,  ..., -0.3789,  0.2217, -0.1040]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1816,  0.3613,  0.2754,  ..., -0.2891,  0.0938, -0.2041]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3789,  0.5508,  0.3965,  ..., -0.1650,  0.3047, -0.3672]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3320,  0.6641,  0.3691,  ..., -0.1934,  0.2207, -0.1602]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2910,  0.4648,  0.5312,  ..., -0.1367,  0.1729, -0.1875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1992,  0.5547,  0.2402,  ..., -0.1865,  0.3848,  0.0010]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4180,  0.4551,  0.1387,  ..., -0.5664,  0.0977,  0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6562,  0.3359,  0.4551,  ..., -0.7266,  0.1079, -0.0120]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7148, -0.0996,  0.2178,  ..., -0.6719, -0.3633, -0.2402]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0312, -0.0427, -0.1318,  ..., -0.4863, -0.1152, -0.4082]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.7812,  0.3613,  1.1250,  ..., -0.2207,  0.2891, -0.3066]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0008,  0.0139, -0.0002,  ...,  0.0291,  0.0090, -0.0023]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0216,  0.0159, -0.0101,  ...,  0.0119,  0.0013,  0.0275]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0032,  0.0276,  0.0102,  ..., -0.0251, -0.0025,  0.0297]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0020,  0.0315, -0.0039,  ..., -0.0132, -0.0081,  0.0254]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0018,  0.0684,  0.0084,  ..., -0.0212, -0.0083,  0.0347]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0103,  0.1426, -0.0015,  ..., -0.0874, -0.0102,  0.0781]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0437,  0.1025, -0.0486,  ..., -0.0474, -0.0408,  0.0903]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0801,  0.1030, -0.0801,  ..., -0.0154,  0.0287,  0.1885]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0884,  0.0879, -0.1582,  ..., -0.0396, -0.0078,  0.1318]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1328,  0.0796, -0.0767,  ..., -0.0361,  0.0376,  0.1309]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1338,  0.0381, -0.0161,  ..., -0.0825,  0.0168,  0.1406]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0713, -0.0532, -0.0811,  ..., -0.0947,  0.0381,  0.1138]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1045, -0.0476, -0.0693,  ..., -0.0806, -0.0081,  0.0884]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0703,  0.0635, -0.0243,  ..., -0.0908,  0.1445,  0.0757]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0479,  0.0598, -0.0020,  ..., -0.0298,  0.1582,  0.0544]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1738,  0.0205,  0.2539,  ..., -0.0757, -0.0361,  0.1572]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[0.0928, 0.1191, 0.2148,  ..., 0.0029, 0.0649, 0.1328]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1094,  0.1035,  0.0342,  ..., -0.1494,  0.1299,  0.1025]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0427,  0.1221,  0.0674,  ..., -0.1016,  0.2285,  0.0034]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0762,  0.0474, -0.1934,  ...,  0.0107,  0.1152, -0.1719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1089,  0.1514, -0.1807,  ..., -0.2539,  0.0723, -0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.2012, -0.0356,  ..., -0.3789,  0.1494, -0.2275]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934,  0.3164, -0.0986,  ..., -0.2207,  0.2734, -0.3594]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0669,  0.3398,  0.2471,  ...,  0.0293,  0.3887, -0.1680]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0957,  0.3711,  0.3438,  ..., -0.0469,  0.3887, -0.2852]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0459,  0.1504,  0.4883,  ..., -0.0625,  0.4160, -0.4141]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1309,  0.3438,  0.3770,  ...,  0.2041,  0.4746, -0.4336]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1748,  0.4453,  0.4180,  ..., -0.0757,  0.2617, -0.5156]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1914,  0.2949,  0.3086,  ..., -0.1309,  0.0498, -0.7305]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3340,  0.1016,  0.0918,  ...,  0.0088, -0.3574, -0.5508]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5625,  0.1641, -0.1738,  ..., -0.3906, -0.5430, -0.7656]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0938,  0.1426,  0.3340,  ..., -0.0215, -0.5625, -0.3242]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.0559e-02,  1.6212e-05,  4.8340e-02,  ..., -1.4771e-02,\n",
       "             5.9204e-03, -1.2695e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0098, -0.0298,  0.0430,  ..., -0.0294, -0.0026, -0.0183]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0354, -0.0266,  0.0830,  ..., -0.0269, -0.0284,  0.0247]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0557, -0.0282,  0.1055,  ..., -0.0503,  0.0064,  0.0383]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0193,  0.0214,  0.0947,  ..., -0.0664,  0.0197,  0.0155]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0055,  0.1118,  0.1338,  ..., -0.0830,  0.0151, -0.0302]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0015,  0.0869,  0.0923,  ..., -0.0115, -0.0518,  0.0073]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1299,  0.0562,  0.0635,  ..., -0.0664, -0.0674,  0.0645]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0483, -0.0142, -0.0039,  ..., -0.0659, -0.0408,  0.0115]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0762, -0.0273,  0.0320,  ..., -0.0723,  0.0093,  0.0104]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0564, -0.0513, -0.0004,  ..., -0.1035, -0.0305, -0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0439, -0.1123, -0.1235,  ..., -0.1895,  0.0928, -0.0811]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0854, -0.0732, -0.1309,  ..., -0.1543,  0.0327, -0.1484]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0005,  0.0142,  0.0498,  ..., -0.1699,  0.0291, -0.1006]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1079,  0.0732,  0.0518,  ..., -0.2207,  0.0366, -0.0703]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1328,  0.1025,  0.2432,  ..., -0.2217, -0.1367, -0.0234]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0469,  0.1650,  0.2344,  ..., -0.1206, -0.2949,  0.0352]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0466,  0.0527,  0.0215,  ..., -0.2988, -0.2188,  0.1235]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1250,  0.1318, -0.0232,  ..., -0.2051, -0.1650,  0.0571]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1240,  0.2832,  0.1650,  ..., -0.1182, -0.0579, -0.1128]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0825,  0.5078,  0.0356,  ..., -0.1309, -0.0664, -0.1729]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0383,  0.6172,  0.2344,  ..., -0.2070, -0.0752, -0.2891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0776,  0.6562,  0.2334,  ..., -0.1816, -0.0459, -0.3789]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2363,  0.5508,  0.0693,  ..., -0.0864,  0.1797, -0.2891]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3086,  0.6562,  0.2207,  ..., -0.1118,  0.2090, -0.1797]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3359,  0.8633,  0.2695,  ..., -0.1777,  0.3477, -0.1406]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3984,  0.8633,  0.1875,  ..., -0.2285,  0.3867, -0.0493]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4199,  1.6406,  0.1680,  ..., -1.0078,  0.4746,  0.4785]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0410,  0.9258, -0.0537,  ..., -0.8906, -0.1035,  0.3652]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4219,  0.8320, -0.3809,  ..., -0.9219, -0.1650,  0.3965]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6094,  0.6016, -0.7188,  ..., -0.9102, -0.3438,  0.0918]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 1.1953,  1.1875, -0.0898,  ..., -0.7227, -0.1211, -0.3496]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0031, -0.0101,  0.0100,  ..., -0.0114, -0.0003,  0.0036]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0011, -0.0234,  0.0024,  ..., -0.0347, -0.0147, -0.0068]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0269, -0.0148,  0.0498,  ..., -0.0311, -0.0178,  0.0087]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0092, -0.0442,  0.0476,  ..., -0.0552, -0.0091,  0.0356]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0159, -0.0216,  0.0610,  ..., -0.0500, -0.0181,  0.0771]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0062,  0.0457,  0.0845,  ..., -0.1230,  0.0146,  0.0212]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0007,  0.0889,  0.0796,  ..., -0.0254, -0.0356,  0.0320]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0155,  0.0576,  0.0723,  ..., -0.0928, -0.0137,  0.0449]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0305,  0.0161, -0.0273,  ..., -0.1113, -0.0422,  0.0289]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0737, -0.0245,  0.0151,  ..., -0.0981,  0.0684,  0.0239]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0415, -0.0557,  0.0496,  ..., -0.0869,  0.0947,  0.0493]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0208, -0.0369, -0.0918,  ..., -0.2031,  0.1152, -0.0194]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1406, -0.0564, -0.0332,  ..., -0.1826,  0.0269, -0.0732]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0845,  0.0996,  0.0330,  ..., -0.1875,  0.0791, -0.0586]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0156,  0.1680, -0.0586,  ..., -0.1055, -0.0503, -0.0645]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0444,  0.0527,  0.1621,  ..., -0.2617, -0.0952, -0.0391]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1289,  0.1133,  0.1387,  ..., -0.1250, -0.0830, -0.1099]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1162,  0.0581,  0.0771,  ..., -0.2422,  0.0654, -0.0127]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2334,  0.0986,  0.0801,  ..., -0.2295,  0.0693, -0.0031]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3262,  0.1157,  0.1758,  ..., -0.1553,  0.1206, -0.0075]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4395,  0.2305,  0.1748,  ..., -0.1973,  0.1162, -0.1875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4512,  0.3027,  0.1777,  ..., -0.3438,  0.0962, -0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4238,  0.2480,  0.1191,  ..., -0.2314,  0.1992, -0.2363]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1836,  0.2373,  0.0176,  ..., -0.1768,  0.2891, -0.1250]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1377,  0.2910,  0.1484,  ..., -0.2852,  0.1113, -0.1719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0581,  0.3848,  0.1455,  ..., -0.3359,  0.0625, -0.1504]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1816,  0.6406,  0.1768,  ..., -0.4062,  0.1807, -0.1387]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.3594,  1.0391,  0.6914,  ..., -0.7539,  0.0029,  0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1104,  1.2578,  0.5195,  ..., -0.5938,  0.0781,  0.2695]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1758,  0.9453,  0.4824,  ..., -0.6094, -0.0327,  0.4043]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2891,  0.9141,  0.5039,  ..., -0.4434, -0.1836,  0.5430]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1865,  0.9219,  0.4883,  ..., -0.1934, -0.1914,  0.0293]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0144, -0.0039, -0.0024,  ..., -0.0292, -0.0117,  0.0107]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0040, -0.0011, -0.0025,  ..., -0.0432, -0.0190,  0.0093]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0261,  0.0105,  0.0036,  ..., -0.0474, -0.0216,  0.0041]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0317,  0.0101,  0.0571,  ..., -0.1270, -0.0084, -0.0096]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0111, -0.0057,  0.0265,  ..., -0.1226,  0.0073,  0.0139]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0439,  0.0034,  0.0192,  ..., -0.1250,  0.0101,  0.0266]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0442,  0.0248, -0.0137,  ..., -0.1201,  0.0018,  0.0552]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1006,  0.0200, -0.0461,  ..., -0.0967,  0.0247,  0.1143]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0752, -0.0149, -0.0162,  ..., -0.1152, -0.0022,  0.0796]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0620, -0.0933, -0.0752,  ..., -0.0669,  0.0801,  0.0591]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0271, -0.0134, -0.0310,  ..., -0.1074,  0.0806,  0.0786]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1592, -0.0400, -0.1021,  ..., -0.1152,  0.0962,  0.0688]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2285, -0.0210, -0.1377,  ..., -0.1172,  0.0786,  0.0225]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1768,  0.0645, -0.0103,  ..., -0.1904,  0.0488,  0.0194]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0947,  0.0942, -0.0142,  ..., -0.1748, -0.0415,  0.0938]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0381,  0.0317,  0.2148,  ..., -0.3906, -0.0342,  0.1045]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0752, -0.0093,  0.1709,  ..., -0.2676, -0.0669,  0.0356]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0762, -0.0150,  0.2432,  ..., -0.3320,  0.0186,  0.0366]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1152, -0.1030,  0.2656,  ..., -0.2656,  0.0072, -0.0117]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2207,  0.0283,  0.3340,  ..., -0.1426,  0.0977, -0.1152]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3438,  0.1318,  0.3730,  ..., -0.2188,  0.1504, -0.1846]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3887,  0.1875,  0.2793,  ..., -0.3145,  0.0947, -0.2207]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3633,  0.0796,  0.2676,  ..., -0.3203,  0.1895, -0.3711]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0527,  0.0767,  0.3047,  ..., -0.4453,  0.6875, -0.1953]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0057,  0.2109,  0.4160,  ..., -0.4883,  0.7188, -0.1631]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1455,  0.2275,  0.4082,  ..., -0.4238,  0.7461, -0.2227]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0732,  0.1934,  0.4902,  ..., -0.5000,  0.8398, -0.2773]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4180,  0.6992,  0.7852,  ..., -1.4219,  0.4570, -0.2412]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4219,  0.7070,  0.8008,  ..., -1.6094,  0.3652, -0.2695]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2988,  0.4648,  0.7227,  ..., -1.6719,  0.0527, -0.2656]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0459,  0.4004,  0.3125,  ..., -1.7656,  0.2246, -0.0215]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.6211,  0.4629, -0.1445,  ..., -1.0312,  0.7070,  0.1943]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0131, -0.0088,  0.0052,  ..., -0.0278, -0.0260,  0.0161]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0151, -0.0242, -0.0097,  ..., -0.0618, -0.0291,  0.0200]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0488, -0.0110,  0.0181,  ..., -0.0806, -0.0591,  0.0183]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0625, -0.0410, -0.0508,  ..., -0.1582, -0.0684,  0.0396]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0771, -0.0322, -0.1064,  ..., -0.1006, -0.0508, -0.0034]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1147, -0.0161, -0.0996,  ..., -0.0928, -0.0771, -0.0016]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0811, -0.0317, -0.1069,  ..., -0.0869, -0.0415,  0.0127]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0996, -0.0525, -0.1611,  ..., -0.1660, -0.0156,  0.0786]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0654, -0.0869, -0.1172,  ..., -0.1504, -0.0008,  0.0190]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0522, -0.1309, -0.0654,  ..., -0.1006,  0.0035,  0.0306]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0016, -0.0752, -0.0640,  ..., -0.0928,  0.0057,  0.0601]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0371, -0.1172, -0.1934,  ..., -0.0645, -0.0281,  0.0184]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1025, -0.1001, -0.1641,  ..., -0.1670, -0.0122,  0.0317]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0090,  0.0098, -0.0698,  ..., -0.1777, -0.0869,  0.0830]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1157,  0.0085, -0.1191,  ..., -0.2197, -0.2480,  0.0618]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1030, -0.0879,  0.1436,  ..., -0.3184, -0.3262,  0.1934]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1235, -0.1514,  0.0284,  ..., -0.1484, -0.3633,  0.2051]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0654, -0.2500, -0.0029,  ..., -0.2520, -0.3379,  0.2676]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0830, -0.3086,  0.0156,  ..., -0.2539, -0.3809,  0.2402]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0806, -0.3828, -0.1670,  ..., -0.2285, -0.3984,  0.0781]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1533, -0.2256, -0.2080,  ..., -0.1855, -0.3730,  0.1133]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2598, -0.2393, -0.2207,  ..., -0.2207, -0.3672,  0.2188]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2578, -0.1084, -0.1992,  ..., -0.1328, -0.3965,  0.2734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3164, -0.3789, -0.2197,  ...,  0.0381, -0.5703,  0.3008]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4883, -0.3145, -0.0952,  ..., -0.1113, -0.6328,  0.3867]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.3848, -0.3477, -0.1621,  ..., -0.0776, -0.5586,  0.4453]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4316, -0.1162, -0.0469,  ..., -0.2695, -0.5508,  0.4102]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5703,  0.2178, -0.1699,  ..., -0.5898, -0.7266, -0.0232]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6758,  0.3320, -0.1064,  ..., -0.6836, -0.7461,  0.0349]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5430,  0.1406, -0.4512,  ..., -0.5703, -0.9844,  0.4180]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4629,  0.1846, -0.6680,  ..., -0.8672, -1.2344,  0.2812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6094,  0.3945, -0.4375,  ..., -0.1855, -1.1094,  0.8125]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0004, -0.0023,  0.0146,  ..., -0.0056, -0.0031,  0.0001]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0035, -0.0010,  0.0058,  ..., -0.0018, -0.0124,  0.0023]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0079, -0.0210,  0.0176,  ...,  0.0103, -0.0136, -0.0075]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0069, -0.0099,  0.0130,  ..., -0.0160,  0.0137, -0.0095]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0698, -0.0476,  0.0005,  ...,  0.0242,  0.0238, -0.0510]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0442, -0.0540,  0.0420,  ...,  0.0137,  0.0094, -0.0236]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0383, -0.0479,  0.0625,  ...,  0.0645, -0.0038, -0.0674]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0684, -0.0449, -0.0403,  ...,  0.0161,  0.0542,  0.0173]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1055, -0.0552,  0.0046,  ...,  0.0240, -0.0811, -0.0254]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-4.2725e-02, -1.4160e-01, -1.2207e-04,  ...,  5.1514e-02,\n",
       "             2.5391e-02, -5.4932e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0149, -0.1523, -0.0791,  ..., -0.0261,  0.0142, -0.0337]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0149, -0.2227, -0.1406,  ..., -0.0933, -0.0240, -0.0515]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0481, -0.2432, -0.0884,  ..., -0.0732, -0.1113, -0.0801]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0112, -0.1797,  0.0559,  ..., -0.2051, -0.1621,  0.1143]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.2051, -0.1836,  0.0491,  ..., -0.1367, -0.1826,  0.0635]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0254, -0.0879,  0.1299,  ..., -0.1562, -0.1514,  0.0117]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0449,  0.0259,  0.0703,  ..., -0.0649, -0.1582, -0.0417]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0376, -0.0884,  0.1738,  ..., -0.1934, -0.2412,  0.0752]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0483, -0.1504,  0.3066,  ..., -0.2080, -0.3203,  0.0791]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0059, -0.0283,  0.1748,  ..., -0.1211, -0.4883,  0.0615]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0576, -0.0664,  0.2451,  ..., -0.1562, -0.4766,  0.1836]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1904, -0.1328,  0.2656,  ..., -0.2754, -0.4453,  0.1875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1113, -0.1475,  0.2695,  ..., -0.2041, -0.5000,  0.2393]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0820, -0.5273,  0.2617,  ..., -0.4082, -0.6758,  0.5508]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0132, -0.4180,  0.3594,  ..., -0.4238, -0.7422,  0.5039]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0515, -0.5117,  0.3359,  ..., -0.4766, -0.5469,  0.6133]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0583, -0.5039,  0.2324,  ..., -0.4844, -0.5234,  0.5898]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.4141,  0.1445,  0.2734,  ..., -0.7969, -0.2402,  0.3281]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5508,  0.1245,  0.5781,  ..., -0.9688, -0.4531,  0.3047]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.6680, -0.2480,  0.3730,  ..., -0.9727, -0.6914,  0.3418]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.5039, -0.4805, -0.3066,  ..., -0.9062, -0.7383,  0.4473]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.1543, -0.5078,  0.3789,  ...,  0.2031, -0.7461,  0.6719]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 1.8921e-03,  4.0894e-03, -3.3188e-04,  ...,  1.9043e-02,\n",
       "            -4.4250e-03, -2.3651e-03],\n",
       "           [ 8.7280e-03,  1.7853e-03,  1.4526e-02,  ...,  1.0864e-02,\n",
       "            -1.7090e-02,  3.1433e-03],\n",
       "           [-2.6245e-03,  6.1035e-05,  2.2278e-03,  ...,  2.2705e-02,\n",
       "            -6.8359e-03, -3.2043e-03],\n",
       "           ...,\n",
       "           [-6.7139e-03, -5.3711e-03,  3.3569e-03,  ..., -2.6611e-02,\n",
       "            -2.7313e-03,  1.5564e-02],\n",
       "           [-7.5073e-03,  6.2256e-03, -3.6926e-03,  ..., -8.3008e-03,\n",
       "             6.5308e-03,  5.4932e-04],\n",
       "           [ 6.0425e-03, -1.8066e-02, -6.7139e-03,  ...,  9.0942e-03,\n",
       "            -2.0752e-02,  1.4954e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1128,  0.0574, -0.0723,  ...,  0.8477,  0.1738,  0.1138],\n",
       "           [ 0.0011,  0.0032,  0.0098,  ..., -0.0184, -0.0115,  0.0048],\n",
       "           [-0.1035,  0.0444, -0.0625,  ...,  0.7656,  0.1553,  0.1025],\n",
       "           ...,\n",
       "           [-0.0265,  0.0106,  0.0045,  ..., -0.0603, -0.0107,  0.0232],\n",
       "           [-0.0176,  0.0190,  0.0082,  ..., -0.0171, -0.0278, -0.0067],\n",
       "           [ 0.0220, -0.0197, -0.0062,  ...,  0.0016, -0.0442,  0.0074]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-1.1963e-01,  6.2256e-02, -6.6895e-02,  ...,  8.6328e-01,\n",
       "             1.8848e-01,  1.0889e-01],\n",
       "           [ 3.7537e-03,  1.3672e-02,  1.9043e-02,  ..., -5.5542e-03,\n",
       "            -5.4932e-04, -7.6294e-03],\n",
       "           [-1.0791e-01, -5.9509e-04, -7.5684e-02,  ...,  7.1875e-01,\n",
       "             1.8945e-01,  4.5654e-02],\n",
       "           ...,\n",
       "           [ 1.3794e-02,  4.7302e-03, -3.3264e-03,  ..., -6.9824e-02,\n",
       "            -4.6143e-02,  1.7700e-02],\n",
       "           [ 2.0386e-02,  1.2878e-02,  2.7100e-02,  ..., -5.7373e-03,\n",
       "            -4.6631e-02, -1.1475e-02],\n",
       "           [ 7.9346e-03, -4.4434e-02,  7.2632e-03,  ...,  3.5400e-02,\n",
       "            -5.2246e-02, -1.2817e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-9.9121e-02,  7.2266e-02, -4.3213e-02,  ...,  9.1406e-01,\n",
       "             1.8555e-01,  1.0400e-01],\n",
       "           [-2.3193e-02, -1.7334e-02,  1.3672e-02,  ...,  3.6621e-03,\n",
       "             1.9165e-02,  3.9062e-03],\n",
       "           [-1.3672e-02,  8.6426e-02, -6.3477e-02,  ...,  7.7734e-01,\n",
       "             1.5625e-01,  2.2583e-02],\n",
       "           ...,\n",
       "           [ 3.5156e-02,  1.1597e-02,  5.9082e-02,  ..., -4.5654e-02,\n",
       "            -3.6865e-02,  1.5381e-02],\n",
       "           [ 6.2988e-02, -1.2207e-04,  5.5176e-02,  ..., -4.7363e-02,\n",
       "            -9.0820e-02,  4.4434e-02],\n",
       "           [ 8.9111e-03, -9.0820e-02, -5.1758e-02,  ..., -5.2979e-02,\n",
       "            -4.8828e-02, -4.9805e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1226,  0.0718, -0.0417,  ...,  0.8906,  0.2197,  0.1138],\n",
       "           [ 0.0035, -0.0161,  0.0312,  ...,  0.0496,  0.0157,  0.0107],\n",
       "           [-0.0220,  0.0962, -0.1050,  ...,  0.7461,  0.1885,  0.0225],\n",
       "           ...,\n",
       "           [ 0.0276,  0.0737,  0.0457,  ..., -0.0405, -0.0165,  0.0280],\n",
       "           [ 0.0317,  0.0270,  0.0825,  ..., -0.0571, -0.0859,  0.0747],\n",
       "           [ 0.0466, -0.0894, -0.0684,  ..., -0.0669, -0.0659, -0.0149]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1157,  0.0781, -0.0281,  ...,  0.8828,  0.2363,  0.1206],\n",
       "           [ 0.0035, -0.0079,  0.0024,  ...,  0.0129,  0.0229,  0.0146],\n",
       "           [-0.0240,  0.1182, -0.1230,  ...,  0.7109,  0.1729,  0.0229],\n",
       "           ...,\n",
       "           [ 0.0654,  0.0623,  0.0515,  ..., -0.0544,  0.0080,  0.0106],\n",
       "           [ 0.1396,  0.0684,  0.1177,  ..., -0.1128, -0.0776,  0.0256],\n",
       "           [ 0.0017, -0.0127, -0.0271,  ..., -0.1152, -0.0238, -0.0315]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1221,  0.0981, -0.0195,  ...,  0.8398,  0.2451,  0.1167],\n",
       "           [ 0.0046, -0.0028,  0.0148,  ...,  0.0530,  0.0181,  0.0383],\n",
       "           [-0.0427,  0.1553, -0.0996,  ...,  0.6094,  0.1709,  0.0383],\n",
       "           ...,\n",
       "           [ 0.0708,  0.0718,  0.0593,  ...,  0.0161,  0.0869,  0.0566],\n",
       "           [ 0.1943,  0.0762,  0.0718,  ..., -0.0801, -0.0537,  0.0077],\n",
       "           [ 0.0286, -0.0219,  0.0031,  ..., -0.1104, -0.0569,  0.0164]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0728,  0.1650, -0.0155,  ...,  0.8008,  0.2207,  0.1104],\n",
       "           [-0.0957,  0.0142,  0.0898,  ..., -0.0403, -0.0527,  0.0464],\n",
       "           [-0.0215,  0.1226, -0.0199,  ...,  0.5625,  0.1240, -0.0010],\n",
       "           ...,\n",
       "           [ 0.0155,  0.1904,  0.1260,  ..., -0.0066,  0.0273,  0.1406],\n",
       "           [ 0.1738,  0.1069,  0.1650,  ..., -0.1250, -0.0684,  0.0269],\n",
       "           [-0.0508, -0.0615, -0.1206,  ..., -0.1807,  0.0122,  0.0405]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0806,  0.1982, -0.0013,  ...,  0.7344,  0.2305,  0.1157],\n",
       "           [-0.0820,  0.0859,  0.0728,  ...,  0.0281, -0.0247, -0.0236],\n",
       "           [-0.0459,  0.1152, -0.0189,  ...,  0.5273,  0.1191, -0.0391],\n",
       "           ...,\n",
       "           [ 0.0192,  0.2041,  0.1455,  ..., -0.0053,  0.0178,  0.1270],\n",
       "           [ 0.1572,  0.1455,  0.1367,  ..., -0.0698, -0.0664,  0.0051],\n",
       "           [-0.0056, -0.1104, -0.0928,  ..., -0.2197, -0.0181,  0.0762]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0684,  0.2217, -0.0308,  ...,  0.6875,  0.2500,  0.1465],\n",
       "           [-0.1001,  0.1514,  0.0598,  ..., -0.0332, -0.0164, -0.0034],\n",
       "           [-0.0261,  0.1953, -0.0400,  ...,  0.5195,  0.0386, -0.0713],\n",
       "           ...,\n",
       "           [ 0.0204,  0.1221,  0.0732,  ...,  0.0225, -0.0413,  0.1475],\n",
       "           [ 0.1050,  0.0591,  0.1260,  ..., -0.0415, -0.0593,  0.0061],\n",
       "           [ 0.0459, -0.1709, -0.1260,  ..., -0.1729,  0.0325,  0.1074]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-2.6855e-03,  3.0469e-01, -1.7822e-02,  ...,  5.7031e-01,\n",
       "             2.8125e-01,  1.6309e-01],\n",
       "           [-7.6172e-02,  1.4941e-01,  2.7344e-02,  ..., -3.2715e-02,\n",
       "            -3.8330e-02, -1.3428e-02],\n",
       "           [-5.2246e-02,  2.0312e-01, -4.2969e-02,  ...,  3.5742e-01,\n",
       "             4.3701e-02, -3.5400e-02],\n",
       "           ...,\n",
       "           [ 7.1289e-02,  1.0693e-01,  3.9307e-02,  ...,  9.5825e-03,\n",
       "            -6.5918e-03,  2.2461e-01],\n",
       "           [ 1.0352e-01,  3.1006e-02,  9.2285e-02,  ..., -1.6211e-01,\n",
       "             1.4160e-02,  3.5156e-02],\n",
       "           [-4.8828e-04, -1.7090e-01, -8.9355e-02,  ..., -2.0410e-01,\n",
       "            -1.2207e-03,  8.7891e-02]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0613,  0.3008, -0.0217,  ...,  0.5703,  0.2988,  0.1226],\n",
       "           [-0.1318,  0.0723,  0.0840,  ...,  0.0664, -0.0698,  0.0403],\n",
       "           [-0.2168,  0.0947, -0.0170,  ...,  0.3535,  0.0957, -0.0913],\n",
       "           ...,\n",
       "           [ 0.1504,  0.2188,  0.0771,  ...,  0.0039, -0.0052,  0.2930],\n",
       "           [ 0.1650,  0.1514,  0.1187,  ..., -0.1797,  0.1191,  0.0649],\n",
       "           [ 0.0031, -0.0732, -0.0342,  ..., -0.2734,  0.0603,  0.0811]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0498,  0.3203, -0.0071,  ...,  0.5000,  0.2832,  0.1128],\n",
       "           [-0.0732,  0.0884,  0.1001,  ...,  0.0466, -0.1128,  0.0027],\n",
       "           [-0.2178,  0.0684, -0.0601,  ...,  0.3125,  0.0645, -0.0674],\n",
       "           ...,\n",
       "           [ 0.2158,  0.2139,  0.0403,  ...,  0.0693, -0.0374,  0.3359],\n",
       "           [ 0.1904,  0.1650,  0.1143,  ..., -0.0913,  0.0981,  0.1045],\n",
       "           [-0.0177, -0.0986, -0.0139,  ..., -0.1758,  0.0864,  0.1011]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0513,  0.3027, -0.0513,  ...,  0.4297,  0.2734,  0.1201],\n",
       "           [-0.0576,  0.0894,  0.0840,  ...,  0.0200, -0.0806,  0.0757],\n",
       "           [-0.2676,  0.0698, -0.0796,  ...,  0.2988,  0.0508, -0.1074],\n",
       "           ...,\n",
       "           [ 0.2041,  0.2061,  0.0322,  ...,  0.0332,  0.0420,  0.1973],\n",
       "           [ 0.2207,  0.1592,  0.0579,  ..., -0.0996,  0.1235,  0.0432],\n",
       "           [ 0.0393, -0.0781,  0.0410,  ..., -0.0942,  0.1514, -0.0386]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0786,  0.2637, -0.0410,  ...,  0.4316,  0.3125,  0.0366],\n",
       "           [-0.0613,  0.0757,  0.0781,  ..., -0.0286, -0.1152,  0.0216],\n",
       "           [-0.3027, -0.0150, -0.0571,  ...,  0.2793,  0.0066, -0.1328],\n",
       "           ...,\n",
       "           [ 0.0786,  0.2949, -0.0608,  ...,  0.0630, -0.0317,  0.1069],\n",
       "           [ 0.1543,  0.2930,  0.0204,  ..., -0.0889,  0.0369, -0.0684],\n",
       "           [-0.0242,  0.0352, -0.1680,  ..., -0.1602,  0.0476, -0.0752]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1084,  0.1533, -0.0107,  ...,  0.3164,  0.3516,  0.0146],\n",
       "           [-0.0933,  0.0957,  0.2090,  ..., -0.0786, -0.1592, -0.0439],\n",
       "           [-0.3340, -0.2090, -0.0728,  ...,  0.1729,  0.0425, -0.0559],\n",
       "           ...,\n",
       "           [ 0.1367,  0.1045, -0.0615,  ...,  0.2412, -0.1084,  0.3418],\n",
       "           [ 0.2002,  0.1260,  0.0601,  ...,  0.0050, -0.0063,  0.0459],\n",
       "           [-0.0242, -0.0115, -0.1328,  ..., -0.1484, -0.0253, -0.0527]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1216,  0.1523, -0.0012,  ...,  0.3047,  0.3730,  0.0009],\n",
       "           [-0.0737,  0.0449,  0.2246,  ..., -0.1562, -0.1094, -0.0554],\n",
       "           [-0.3457, -0.2578, -0.0259,  ...,  0.1553,  0.0010, -0.0288],\n",
       "           ...,\n",
       "           [ 0.0669,  0.0034, -0.0991,  ...,  0.2617, -0.0386,  0.3789],\n",
       "           [ 0.1572,  0.1089,  0.1250,  ..., -0.0610,  0.1133,  0.0786],\n",
       "           [-0.0220, -0.0187, -0.1001,  ..., -0.2168, -0.0447,  0.0237]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1514,  0.1123,  0.0046,  ...,  0.2930,  0.4668, -0.0225],\n",
       "           [-0.1289,  0.0767,  0.1914,  ..., -0.2148, -0.1562, -0.1465],\n",
       "           [-0.3359, -0.2891,  0.0591,  ...,  0.1455,  0.0060, -0.1143],\n",
       "           ...,\n",
       "           [-0.0503,  0.0518,  0.0229,  ...,  0.2539, -0.0400,  0.3398],\n",
       "           [ 0.0674,  0.1055,  0.0444,  ..., -0.1973,  0.1406, -0.0481],\n",
       "           [-0.2090,  0.1855, -0.1025,  ..., -0.3047,  0.0742, -0.0898]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1680,  0.1079,  0.0198,  ...,  0.3047,  0.4023, -0.0493],\n",
       "           [-0.1875,  0.1279,  0.1865,  ..., -0.2598, -0.1309, -0.1738],\n",
       "           [-0.3320, -0.2002,  0.0317,  ...,  0.1196, -0.1484, -0.0938],\n",
       "           ...,\n",
       "           [-0.0654, -0.0015, -0.1157,  ...,  0.2598,  0.1465,  0.3516],\n",
       "           [ 0.0649,  0.0806,  0.0454,  ..., -0.3125,  0.2617, -0.0889],\n",
       "           [-0.2637,  0.1660, -0.1504,  ..., -0.3438,  0.1797, -0.0977]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1797,  0.0669,  0.0342,  ...,  0.2734,  0.5078,  0.0496],\n",
       "           [-0.1104,  0.1816,  0.2266,  ..., -0.3066, -0.0850, -0.1602],\n",
       "           [-0.2578, -0.2490,  0.1328,  ...,  0.1602, -0.2021, -0.1484],\n",
       "           ...,\n",
       "           [-0.0283,  0.0503, -0.2402,  ...,  0.1523,  0.1797,  0.2656],\n",
       "           [ 0.2051,  0.1011, -0.0752,  ..., -0.4492,  0.1406, -0.1377],\n",
       "           [-0.2891,  0.2188, -0.1562,  ..., -0.5234,  0.1719, -0.1758]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1699,  0.0461,  0.0283,  ...,  0.2578,  0.4551,  0.0605],\n",
       "           [-0.0479,  0.1445,  0.2148,  ..., -0.2715, -0.1367, -0.2432],\n",
       "           [-0.0625, -0.2656,  0.1177,  ...,  0.1406, -0.3242, -0.2285],\n",
       "           ...,\n",
       "           [-0.0430, -0.1230, -0.0898,  ...,  0.0161,  0.2412,  0.3008],\n",
       "           [ 0.2422,  0.1128, -0.1011,  ..., -0.6406,  0.1465, -0.1543],\n",
       "           [-0.4258,  0.2412, -0.1426,  ..., -0.6211,  0.1377, -0.2344]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1543,  0.0334,  0.0522,  ...,  0.2852,  0.4531,  0.0688],\n",
       "           [-0.1758,  0.0020,  0.1533,  ..., -0.2988, -0.0513, -0.5234],\n",
       "           [ 0.1035, -0.2559,  0.0571,  ...,  0.1514, -0.4512, -0.3770],\n",
       "           ...,\n",
       "           [-0.2168, -0.3359, -0.1157,  ..., -0.1553,  0.3789,  0.3203],\n",
       "           [ 0.1465,  0.0479, -0.0540,  ..., -0.7812,  0.3945, -0.1426],\n",
       "           [-0.3887,  0.1455, -0.2480,  ..., -0.5820,  0.2520, -0.5469]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1235,  0.0214,  0.0481,  ...,  0.3008,  0.4609,  0.0654],\n",
       "           [-0.2520,  0.1709,  0.2090,  ..., -0.2969, -0.0391, -0.5000],\n",
       "           [ 0.2891, -0.1387, -0.0337,  ...,  0.2070, -0.5859, -0.5273],\n",
       "           ...,\n",
       "           [ 0.0615, -0.2061, -0.1914,  ..., -0.2441,  0.3574,  0.4062],\n",
       "           [ 0.3242,  0.2773, -0.1299,  ..., -0.9570,  0.4160, -0.2324],\n",
       "           [-0.3340,  0.1729, -0.3477,  ..., -0.5547,  0.2070, -0.6211]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2100, -0.0232, -0.1250,  ...,  0.2363,  0.3887,  0.1562],\n",
       "           [-0.2676,  0.3496,  0.0630,  ..., -0.4727, -0.0776, -0.4473],\n",
       "           [ 0.2412, -0.1650, -0.2080,  ..., -0.0157, -0.6367, -0.5625],\n",
       "           ...,\n",
       "           [ 0.3594, -0.0967, -0.5156,  ..., -0.2520,  0.1582,  0.5000],\n",
       "           [ 0.6367,  0.4707, -0.3594,  ..., -1.0938,  0.4883,  0.0498],\n",
       "           [-0.2197,  0.4062, -0.4473,  ..., -0.7773,  0.1885, -0.7227]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1934, -0.0330, -0.1245,  ...,  0.2285,  0.3730,  0.1514],\n",
       "           [-0.2471,  0.4766,  0.0066,  ..., -0.3086, -0.1582, -0.4570],\n",
       "           [ 0.1836, -0.1807, -0.2559,  ..., -0.0601, -0.6953, -0.7734],\n",
       "           ...,\n",
       "           [ 0.2305, -0.0486, -0.6016,  ..., -0.2754,  0.1523,  0.4141],\n",
       "           [ 0.5820,  0.5742, -0.2500,  ..., -1.2500,  0.5156,  0.1387],\n",
       "           [-0.2969,  0.3145, -0.4980,  ..., -0.7148,  0.1172, -0.7812]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.2012, -0.0522, -0.1631,  ...,  0.2422,  0.3496,  0.1562],\n",
       "           [-0.2520,  0.4648,  0.0164,  ..., -0.4688, -0.1128, -0.4688],\n",
       "           [ 0.1562, -0.1484, -0.4609,  ..., -0.0820, -0.7656, -0.8828],\n",
       "           ...,\n",
       "           [ 0.2793,  0.1187, -0.6797,  ..., -0.2402,  0.2168,  0.3418],\n",
       "           [ 0.5938,  0.8516, -0.4844,  ..., -1.3359,  0.5312,  0.0364],\n",
       "           [-0.3145,  0.2070, -0.5273,  ..., -0.8281,  0.3125, -0.7461]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1797, -0.0381, -0.1934,  ...,  0.2246,  0.3789,  0.1436],\n",
       "           [-0.2891,  0.5039,  0.0835,  ..., -0.4902, -0.1396, -0.4180],\n",
       "           [-0.0222, -0.1099, -0.6719,  ..., -0.2012, -0.7148, -0.8672],\n",
       "           ...,\n",
       "           [ 0.3574,  0.1108, -0.6445,  ..., -0.0684,  0.1094,  0.3594],\n",
       "           [ 0.5039,  0.8008, -0.3477,  ..., -1.2734,  0.4863, -0.0342],\n",
       "           [-0.4512,  0.1553, -0.5703,  ..., -0.7305,  0.1934, -0.8516]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.1094, -0.0293, -0.2715,  ...,  0.1406,  0.5781,  0.1157],\n",
       "           [-0.3418,  0.7812,  0.0088,  ..., -0.1436, -0.3906, -0.3750],\n",
       "           [-0.0381,  0.0879, -0.9805,  ..., -0.1914, -0.8086, -1.1094],\n",
       "           ...,\n",
       "           [ 0.6562,  0.1875, -0.7422,  ..., -0.1709,  0.2051,  0.4434],\n",
       "           [ 0.6914,  0.9766, -0.2480,  ..., -1.3281,  0.5703, -0.0562],\n",
       "           [-0.3301,  0.4941, -0.6406,  ..., -0.5703,  0.1738, -0.8906]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0708,  0.0259, -0.2949,  ...,  0.1562,  0.5742,  0.1426],\n",
       "           [-0.4238,  0.5234,  0.1172,  ..., -0.3418, -0.4023, -0.3340],\n",
       "           [ 0.0122,  0.2227, -1.0547,  ...,  0.0791, -0.8672, -1.2344],\n",
       "           ...,\n",
       "           [ 0.1875, -0.1777, -1.0078,  ..., -0.1338,  0.2275,  0.4023],\n",
       "           [ 0.4727,  0.8906, -0.3672,  ..., -1.5312,  0.6172, -0.1406],\n",
       "           [-0.4844,  0.3906, -0.7500,  ..., -0.7148,  0.3086, -0.7734]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0396,  0.0908, -0.1641,  ...,  0.1050,  0.4062,  0.1338],\n",
       "           [-0.1230,  0.6250, -0.0068,  ..., -0.6289, -0.5781, -0.4883],\n",
       "           [-0.0547,  0.2012, -1.1719,  ...,  0.1738, -1.0469, -1.4453],\n",
       "           ...,\n",
       "           [ 0.1641, -0.4023, -1.2031,  ..., -0.1094,  0.3301,  0.3359],\n",
       "           [ 0.7383,  0.9297, -0.4668,  ..., -1.5234,  0.9297, -0.2402],\n",
       "           [-0.2188,  0.2119, -0.7148,  ..., -0.4805,  0.3789, -0.6875]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 2.4414e-04,  2.5586e-01,  1.0742e-01,  ...,  1.5039e-01,\n",
       "            -2.0898e-01,  3.2812e-01],\n",
       "           [-5.6250e-01,  5.0781e-01, -3.2031e-01,  ..., -8.9844e-01,\n",
       "            -6.5234e-01, -1.0547e+00],\n",
       "           [ 3.3984e-01,  6.2500e-01, -1.7422e+00,  ...,  3.2812e-01,\n",
       "            -1.5469e+00, -1.8359e+00],\n",
       "           ...,\n",
       "           [ 1.7676e-01, -6.2891e-01, -1.2656e+00,  ..., -1.9141e-01,\n",
       "             2.6367e-02, -1.3281e-01],\n",
       "           [ 7.8906e-01,  7.9688e-01, -1.9141e-01,  ..., -1.9375e+00,\n",
       "             1.0938e+00, -4.2578e-01],\n",
       "           [-3.0273e-01,  3.0469e-01, -8.0078e-01,  ..., -5.8594e-01,\n",
       "             7.0312e-01, -1.1328e+00]]], device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.4961,  2.6094,  0.9453,  ..., -0.9727,  1.9453,  2.7656],\n",
       "           [-1.1094,  1.9609,  0.2559,  ..., -0.5859,  1.2656, -1.0312],\n",
       "           [ 0.5469,  1.3750,  0.0156,  ..., -1.5625,  1.3359, -2.8125],\n",
       "           ...,\n",
       "           [-0.5312, -0.3281, -0.4492,  ...,  1.2266, -1.2188,  1.1250],\n",
       "           [-0.4844,  0.9492,  0.3008,  ..., -1.4141, -0.3984,  1.2891],\n",
       "           [-0.8047,  0.4434, -0.4375,  ..., -1.1562, -0.1582, -1.2422]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0029, -0.0071, -0.0012,  ..., -0.0046,  0.0016,  0.0004]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0096, -0.0276, -0.0009,  ..., -0.0078, -0.0084, -0.0009]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0009, -0.0513,  0.0405,  ..., -0.0178, -0.0074, -0.0454]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0103, -0.0703,  0.0786,  ..., -0.0684,  0.0033, -0.0366]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0415, -0.0352,  0.0317,  ..., -0.0596, -0.0796, -0.0103]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0247,  0.0371, -0.0186,  ..., -0.1465, -0.0684,  0.0110]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[ 0.0278,  0.0381,  0.0283,  ..., -0.1011, -0.0552, -0.0249]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  tensor([[[-0.0091,  0.0269,  0.0352,  ..., -0.1572,  0.0081, -0.0742]]],\n",
       "         device='cuda:0', dtype=torch.bfloat16),\n",
       "  ...]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_outputs_dict['cross_attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(attention_outputs_dict['self_attention'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 4096])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs_dict['cross_attention'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 4096])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs_dict['cross_attention'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs_dict['cross_attention'][0][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use PCA to reduce the dimensionality of the attention map to 32d\n",
    "sample_attn_map = attention_outputs_dict['cross_attention'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_attn_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def reduce_attention_map_dimension(attention_map, n_components=32):\n",
    "    \"\"\"\n",
    "    Reduce dimensions of attention map using PCA\n",
    "    \n",
    "    Args:\n",
    "        attention_map (torch.Tensor): Input attention map with shape [1, 11, 4096]\n",
    "        n_components (int): Number of components to reduce to\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Reduced attention map\n",
    "    \"\"\"\n",
    "    # Ensure tensor is on CPU and convert to numpy\n",
    "    attention_np = attention_map.squeeze(0).cpu().float().numpy()\n",
    "    \n",
    "    # Reshape to [11 * 4096] for PCA\n",
    "    attention_reshaped = attention_np.reshape(-1, attention_np.shape[-1])\n",
    "    \n",
    "    # Limit components to available dimensions\n",
    "    max_components = min(n_components, attention_reshaped.shape[0], attention_reshaped.shape[1])\n",
    "    \n",
    "    # Perform PCA\n",
    "    pca = PCA(n_components=max_components)\n",
    "    pca_result = pca.fit_transform(attention_reshaped)\n",
    "    \n",
    "    # Reshape back to [1, max_components, 4096]\n",
    "    return torch.from_numpy(pca_result).unsqueeze(0).float()\n",
    "\n",
    "# Example usage\n",
    "# attention_map = torch.randn(1, 11, 4096)\n",
    "reduced_map = reduce_attention_map_dimension(sample_attn_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 11])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_tokens_tsne(tensor):\n",
    "    \"\"\"\n",
    "    Visualize t-SNE for 11 tokens across images\n",
    "    \n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor [1, 11, 11]\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: t-SNE coordinates\n",
    "    \"\"\"\n",
    "    # Convert to numpy\n",
    "    data = tensor.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Perform t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=2)\n",
    "    tsne_result = tsne.fit_transform(data)\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(tsne_result[:, 0], tsne_result[:, 1], \n",
    "                c=range(len(tsne_result)), \n",
    "                cmap='viridis', \n",
    "                s=100)\n",
    "    \n",
    "    # Annotate each point with its token index\n",
    "    for i, (x, y) in enumerate(tsne_result):\n",
    "        plt.annotate(str(i), (x, y), xytext=(5, 5), \n",
    "                     textcoords='offset points', \n",
    "                     fontsize=9)\n",
    "    \n",
    "    plt.title('t-SNE of 11 Tokens')\n",
    "    plt.xlabel('t-SNE Feature 1')\n",
    "    plt.ylabel('t-SNE Feature 2')\n",
    "    plt.colorbar(label='Token Index')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_result\n",
    "\n",
    "# Example usage\n",
    "tsne_result = visualize_tokens_tsne(reduced_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 4096])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_outputs_dict['self_attention'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Vision Encoder Attention Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_model_attention_maps_dict = expert_model.get_vision_model_hidden_layers_attentions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BaseModelOutput(last_hidden_state=tensor([[[-1.2656, -4.0000, -0.0500,  ..., -0.7578,  0.4922, -2.8281],\n",
       "          [-2.9062, -4.4062,  1.6562,  ..., -3.4219,  5.7500, -0.6680],\n",
       "          [-0.9609, -2.0938,  2.8125,  ..., -2.2500,  0.8984,  2.1875],\n",
       "          ...,\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " BaseModelOutput(last_hidden_state=tensor([[[-1.2656, -4.0000, -0.0500,  ..., -0.7578,  0.4922, -2.8281],\n",
       "          [-2.9062, -4.4062,  1.6562,  ..., -3.4219,  5.7500, -0.6680],\n",
       "          [-0.9609, -2.0938,  2.8125,  ..., -2.2500,  0.8984,  2.1875],\n",
       "          ...,\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16), hidden_states=None, attentions=None),\n",
       " BaseModelOutput(last_hidden_state=tensor([[[-1.2656, -4.0000, -0.0500,  ..., -0.7578,  0.4922, -2.8281],\n",
       "          [-2.9062, -4.4062,  1.6562,  ..., -3.4219,  5.7500, -0.6680],\n",
       "          [-0.9609, -2.0938,  2.8125,  ..., -2.2500,  0.8984,  2.1875],\n",
       "          ...,\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375],\n",
       "          [ 0.1963, -0.5430,  3.0000,  ..., -1.5781,  4.3438,  0.4375]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), hidden_states=None, attentions=None)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model_attention_maps_dict['global_transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4128, 1280])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_model_attention_maps_dict['transformer'][0].last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:56 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   45C    P2             73W /  300W |   25072MiB /  46068MiB |      9%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sasika Pamith\n"
     ]
    }
   ],
   "source": [
    "expert_model.sasika_pamith()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = hololens_image\n",
    "\n",
    "# prompt = \"<|image|><|begin_of_text|>what you can see\"\n",
    "# inputs = expert_model_processor(image, prompt, return_tensors=\"pt\").to(expert_model.device)\n",
    "\n",
    "# output = expert_model.generate(**inputs, max_new_tokens=30)\n",
    "# print(expert_model_processor.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SelfAttentionDecoderLayer (tensor([[[ 1.8921e-03,  4.0894e-03, -3.3188e-04,  ...,  1.9043e-02,\n",
      "          -4.4250e-03, -2.3651e-03],\n",
      "         [ 8.7280e-03,  1.7853e-03,  1.4526e-02,  ...,  1.0864e-02,\n",
      "          -1.7090e-02,  3.1433e-03],\n",
      "         [-2.6245e-03,  6.1035e-05,  2.2278e-03,  ...,  2.2705e-02,\n",
      "          -6.8359e-03, -3.2043e-03],\n",
      "         ...,\n",
      "         [-6.7139e-03, -5.3711e-03,  3.3569e-03,  ..., -2.6611e-02,\n",
      "          -2.7313e-03,  1.5564e-02],\n",
      "         [-7.5073e-03,  6.2256e-03, -3.6926e-03,  ..., -8.3008e-03,\n",
      "           6.5308e-03,  5.4932e-04],\n",
      "         [ 6.0425e-03, -1.8066e-02, -6.7139e-03,  ...,  9.0942e-03,\n",
      "          -2.0752e-02,  1.4954e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1128,  0.0574, -0.0723,  ...,  0.8477,  0.1738,  0.1138],\n",
      "         [ 0.0011,  0.0032,  0.0098,  ..., -0.0184, -0.0115,  0.0048],\n",
      "         [-0.1035,  0.0444, -0.0625,  ...,  0.7656,  0.1553,  0.1025],\n",
      "         ...,\n",
      "         [-0.0265,  0.0106,  0.0045,  ..., -0.0603, -0.0107,  0.0232],\n",
      "         [-0.0176,  0.0190,  0.0082,  ..., -0.0171, -0.0278, -0.0067],\n",
      "         [ 0.0220, -0.0197, -0.0062,  ...,  0.0016, -0.0442,  0.0074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-1.1963e-01,  6.2256e-02, -6.6895e-02,  ...,  8.6328e-01,\n",
      "           1.8848e-01,  1.0889e-01],\n",
      "         [ 3.7537e-03,  1.3672e-02,  1.9043e-02,  ..., -5.5542e-03,\n",
      "          -5.4932e-04, -7.6294e-03],\n",
      "         [-1.0791e-01, -5.9509e-04, -7.5684e-02,  ...,  7.1875e-01,\n",
      "           1.8945e-01,  4.5654e-02],\n",
      "         ...,\n",
      "         [ 1.3794e-02,  4.7302e-03, -3.3264e-03,  ..., -6.9824e-02,\n",
      "          -4.6143e-02,  1.7700e-02],\n",
      "         [ 2.0386e-02,  1.2878e-02,  2.7100e-02,  ..., -5.7373e-03,\n",
      "          -4.6631e-02, -1.1475e-02],\n",
      "         [ 7.9346e-03, -4.4434e-02,  7.2632e-03,  ...,  3.5400e-02,\n",
      "          -5.2246e-02, -1.2817e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1201,  0.0588, -0.0630,  ...,  0.8633,  0.1855,  0.1035],\n",
      "         [-0.0282, -0.0233,  0.0135,  ..., -0.0200,  0.0121,  0.0138],\n",
      "         [-0.0889,  0.0282, -0.0796,  ...,  0.6758,  0.1670,  0.0386],\n",
      "         ...,\n",
      "         [ 0.0361,  0.0210,  0.0312,  ..., -0.1025, -0.0469, -0.0256],\n",
      "         [ 0.0625, -0.0162,  0.0055,  ..., -0.0026, -0.0684, -0.0083],\n",
      "         [ 0.0210, -0.0649, -0.0566,  ..., -0.0015, -0.0767, -0.0162]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-9.9121e-02,  7.2266e-02, -4.3213e-02,  ...,  9.1406e-01,\n",
      "           1.8555e-01,  1.0400e-01],\n",
      "         [-2.3193e-02, -1.7334e-02,  1.3672e-02,  ...,  3.6621e-03,\n",
      "           1.9165e-02,  3.9062e-03],\n",
      "         [-1.3672e-02,  8.6426e-02, -6.3477e-02,  ...,  7.7734e-01,\n",
      "           1.5625e-01,  2.2583e-02],\n",
      "         ...,\n",
      "         [ 3.5156e-02,  1.1597e-02,  5.9082e-02,  ..., -4.5654e-02,\n",
      "          -3.6865e-02,  1.5381e-02],\n",
      "         [ 6.2988e-02, -1.2207e-04,  5.5176e-02,  ..., -4.7363e-02,\n",
      "          -9.0820e-02,  4.4434e-02],\n",
      "         [ 8.9111e-03, -9.0820e-02, -5.1758e-02,  ..., -5.2979e-02,\n",
      "          -4.8828e-02, -4.9805e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1226,  0.0718, -0.0417,  ...,  0.8906,  0.2197,  0.1138],\n",
      "         [ 0.0035, -0.0161,  0.0312,  ...,  0.0496,  0.0157,  0.0107],\n",
      "         [-0.0220,  0.0962, -0.1050,  ...,  0.7461,  0.1885,  0.0225],\n",
      "         ...,\n",
      "         [ 0.0276,  0.0737,  0.0457,  ..., -0.0405, -0.0165,  0.0280],\n",
      "         [ 0.0317,  0.0270,  0.0825,  ..., -0.0571, -0.0859,  0.0747],\n",
      "         [ 0.0466, -0.0894, -0.0684,  ..., -0.0669, -0.0659, -0.0149]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1157,  0.0781, -0.0281,  ...,  0.8828,  0.2363,  0.1206],\n",
      "         [ 0.0035, -0.0079,  0.0024,  ...,  0.0129,  0.0229,  0.0146],\n",
      "         [-0.0240,  0.1182, -0.1230,  ...,  0.7109,  0.1729,  0.0229],\n",
      "         ...,\n",
      "         [ 0.0654,  0.0623,  0.0515,  ..., -0.0544,  0.0080,  0.0106],\n",
      "         [ 0.1396,  0.0684,  0.1177,  ..., -0.1128, -0.0776,  0.0256],\n",
      "         [ 0.0017, -0.0127, -0.0271,  ..., -0.1152, -0.0238, -0.0315]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1221,  0.0981, -0.0195,  ...,  0.8398,  0.2451,  0.1167],\n",
      "         [ 0.0046, -0.0028,  0.0148,  ...,  0.0530,  0.0181,  0.0383],\n",
      "         [-0.0427,  0.1553, -0.0996,  ...,  0.6094,  0.1709,  0.0383],\n",
      "         ...,\n",
      "         [ 0.0708,  0.0718,  0.0593,  ...,  0.0161,  0.0869,  0.0566],\n",
      "         [ 0.1943,  0.0762,  0.0718,  ..., -0.0801, -0.0537,  0.0077],\n",
      "         [ 0.0286, -0.0219,  0.0031,  ..., -0.1104, -0.0569,  0.0164]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0884,  0.1069,  0.0081,  ...,  0.8633,  0.2266,  0.0830],\n",
      "         [-0.0962,  0.0014,  0.1230,  ...,  0.0244, -0.0244,  0.0664],\n",
      "         [-0.0096,  0.1504, -0.0586,  ...,  0.6406,  0.1484,  0.0096],\n",
      "         ...,\n",
      "         [ 0.0386,  0.2441,  0.1001,  ...,  0.0106,  0.0376,  0.1138],\n",
      "         [ 0.1777,  0.1270,  0.1123,  ..., -0.1660, -0.0791,  0.0396],\n",
      "         [-0.0840,  0.0737, -0.1123,  ..., -0.1484,  0.0654,  0.0605]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0728,  0.1650, -0.0155,  ...,  0.8008,  0.2207,  0.1104],\n",
      "         [-0.0957,  0.0142,  0.0898,  ..., -0.0403, -0.0527,  0.0464],\n",
      "         [-0.0215,  0.1226, -0.0199,  ...,  0.5625,  0.1240, -0.0010],\n",
      "         ...,\n",
      "         [ 0.0155,  0.1904,  0.1260,  ..., -0.0066,  0.0273,  0.1406],\n",
      "         [ 0.1738,  0.1069,  0.1650,  ..., -0.1250, -0.0684,  0.0269],\n",
      "         [-0.0508, -0.0615, -0.1206,  ..., -0.1807,  0.0122,  0.0405]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0806,  0.1982, -0.0013,  ...,  0.7344,  0.2305,  0.1157],\n",
      "         [-0.0820,  0.0859,  0.0728,  ...,  0.0281, -0.0247, -0.0236],\n",
      "         [-0.0459,  0.1152, -0.0189,  ...,  0.5273,  0.1191, -0.0391],\n",
      "         ...,\n",
      "         [ 0.0192,  0.2041,  0.1455,  ..., -0.0053,  0.0178,  0.1270],\n",
      "         [ 0.1572,  0.1455,  0.1367,  ..., -0.0698, -0.0664,  0.0051],\n",
      "         [-0.0056, -0.1104, -0.0928,  ..., -0.2197, -0.0181,  0.0762]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0684,  0.2217, -0.0308,  ...,  0.6875,  0.2500,  0.1465],\n",
      "         [-0.1001,  0.1514,  0.0598,  ..., -0.0332, -0.0164, -0.0034],\n",
      "         [-0.0261,  0.1953, -0.0400,  ...,  0.5195,  0.0386, -0.0713],\n",
      "         ...,\n",
      "         [ 0.0204,  0.1221,  0.0732,  ...,  0.0225, -0.0413,  0.1475],\n",
      "         [ 0.1050,  0.0591,  0.1260,  ..., -0.0415, -0.0593,  0.0061],\n",
      "         [ 0.0459, -0.1709, -0.1260,  ..., -0.1729,  0.0325,  0.1074]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-2.6855e-03,  3.0469e-01, -1.7822e-02,  ...,  5.7031e-01,\n",
      "           2.8125e-01,  1.6309e-01],\n",
      "         [-7.6172e-02,  1.4941e-01,  2.7344e-02,  ..., -3.2715e-02,\n",
      "          -3.8330e-02, -1.3428e-02],\n",
      "         [-5.2246e-02,  2.0312e-01, -4.2969e-02,  ...,  3.5742e-01,\n",
      "           4.3701e-02, -3.5400e-02],\n",
      "         ...,\n",
      "         [ 7.1289e-02,  1.0693e-01,  3.9307e-02,  ...,  9.5825e-03,\n",
      "          -6.5918e-03,  2.2461e-01],\n",
      "         [ 1.0352e-01,  3.1006e-02,  9.2285e-02,  ..., -1.6211e-01,\n",
      "           1.4160e-02,  3.5156e-02],\n",
      "         [-4.8828e-04, -1.7090e-01, -8.9355e-02,  ..., -2.0410e-01,\n",
      "          -1.2207e-03,  8.7891e-02]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0503,  0.2734,  0.0208,  ...,  0.5938,  0.2891,  0.1240],\n",
      "         [-0.1523,  0.0508,  0.0674,  ...,  0.1172, -0.0767,  0.1055],\n",
      "         [-0.1484,  0.1426,  0.0079,  ...,  0.3594,  0.0859, -0.0737],\n",
      "         ...,\n",
      "         [ 0.0679,  0.1348,  0.0239,  ...,  0.0840, -0.0525,  0.2949],\n",
      "         [ 0.1011,  0.0493,  0.0732,  ..., -0.0879,  0.0781,  0.1226],\n",
      "         [ 0.0012, -0.1582, -0.0903,  ..., -0.2070,  0.0732,  0.1299]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0613,  0.3008, -0.0217,  ...,  0.5703,  0.2988,  0.1226],\n",
      "         [-0.1318,  0.0723,  0.0840,  ...,  0.0664, -0.0698,  0.0403],\n",
      "         [-0.2168,  0.0947, -0.0170,  ...,  0.3535,  0.0957, -0.0913],\n",
      "         ...,\n",
      "         [ 0.1504,  0.2188,  0.0771,  ...,  0.0039, -0.0052,  0.2930],\n",
      "         [ 0.1650,  0.1514,  0.1187,  ..., -0.1797,  0.1191,  0.0649],\n",
      "         [ 0.0031, -0.0732, -0.0342,  ..., -0.2734,  0.0603,  0.0811]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0498,  0.3203, -0.0071,  ...,  0.5000,  0.2832,  0.1128],\n",
      "         [-0.0732,  0.0884,  0.1001,  ...,  0.0466, -0.1128,  0.0027],\n",
      "         [-0.2178,  0.0684, -0.0601,  ...,  0.3125,  0.0645, -0.0674],\n",
      "         ...,\n",
      "         [ 0.2158,  0.2139,  0.0403,  ...,  0.0693, -0.0374,  0.3359],\n",
      "         [ 0.1904,  0.1650,  0.1143,  ..., -0.0913,  0.0981,  0.1045],\n",
      "         [-0.0177, -0.0986, -0.0139,  ..., -0.1758,  0.0864,  0.1011]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0513,  0.3027, -0.0513,  ...,  0.4297,  0.2734,  0.1201],\n",
      "         [-0.0576,  0.0894,  0.0840,  ...,  0.0200, -0.0806,  0.0757],\n",
      "         [-0.2676,  0.0698, -0.0796,  ...,  0.2988,  0.0508, -0.1074],\n",
      "         ...,\n",
      "         [ 0.2041,  0.2061,  0.0322,  ...,  0.0332,  0.0420,  0.1973],\n",
      "         [ 0.2207,  0.1592,  0.0579,  ..., -0.0996,  0.1235,  0.0432],\n",
      "         [ 0.0393, -0.0781,  0.0410,  ..., -0.0942,  0.1514, -0.0386]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0786,  0.2637, -0.0410,  ...,  0.4316,  0.3125,  0.0366],\n",
      "         [-0.0613,  0.0757,  0.0781,  ..., -0.0286, -0.1152,  0.0216],\n",
      "         [-0.3027, -0.0150, -0.0571,  ...,  0.2793,  0.0066, -0.1328],\n",
      "         ...,\n",
      "         [ 0.0786,  0.2949, -0.0608,  ...,  0.0630, -0.0317,  0.1069],\n",
      "         [ 0.1543,  0.2930,  0.0204,  ..., -0.0889,  0.0369, -0.0684],\n",
      "         [-0.0242,  0.0352, -0.1680,  ..., -0.1602,  0.0476, -0.0752]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.0544,  0.2891, -0.0781,  ...,  0.3945,  0.3125,  0.0537],\n",
      "         [-0.1777,  0.1299,  0.1699,  ..., -0.0957, -0.1328, -0.0254],\n",
      "         [-0.2969, -0.0092, -0.0874,  ...,  0.2852,  0.0040, -0.0532],\n",
      "         ...,\n",
      "         [ 0.0352,  0.2891, -0.1699,  ...,  0.1992, -0.1680,  0.2139],\n",
      "         [ 0.1108,  0.3047, -0.0566,  ...,  0.0269, -0.1079, -0.1138],\n",
      "         [-0.0491,  0.0327, -0.1973,  ...,  0.0107, -0.0205, -0.1328]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1084,  0.1533, -0.0107,  ...,  0.3164,  0.3516,  0.0146],\n",
      "         [-0.0933,  0.0957,  0.2090,  ..., -0.0786, -0.1592, -0.0439],\n",
      "         [-0.3340, -0.2090, -0.0728,  ...,  0.1729,  0.0425, -0.0559],\n",
      "         ...,\n",
      "         [ 0.1367,  0.1045, -0.0615,  ...,  0.2412, -0.1084,  0.3418],\n",
      "         [ 0.2002,  0.1260,  0.0601,  ...,  0.0050, -0.0063,  0.0459],\n",
      "         [-0.0242, -0.0115, -0.1328,  ..., -0.1484, -0.0253, -0.0527]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1216,  0.1523, -0.0012,  ...,  0.3047,  0.3730,  0.0009],\n",
      "         [-0.0737,  0.0449,  0.2246,  ..., -0.1562, -0.1094, -0.0554],\n",
      "         [-0.3457, -0.2578, -0.0259,  ...,  0.1553,  0.0010, -0.0288],\n",
      "         ...,\n",
      "         [ 0.0669,  0.0034, -0.0991,  ...,  0.2617, -0.0386,  0.3789],\n",
      "         [ 0.1572,  0.1089,  0.1250,  ..., -0.0610,  0.1133,  0.0786],\n",
      "         [-0.0220, -0.0187, -0.1001,  ..., -0.2168, -0.0447,  0.0237]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1514,  0.1123,  0.0046,  ...,  0.2930,  0.4668, -0.0225],\n",
      "         [-0.1289,  0.0767,  0.1914,  ..., -0.2148, -0.1562, -0.1465],\n",
      "         [-0.3359, -0.2891,  0.0591,  ...,  0.1455,  0.0060, -0.1143],\n",
      "         ...,\n",
      "         [-0.0503,  0.0518,  0.0229,  ...,  0.2539, -0.0400,  0.3398],\n",
      "         [ 0.0674,  0.1055,  0.0444,  ..., -0.1973,  0.1406, -0.0481],\n",
      "         [-0.2090,  0.1855, -0.1025,  ..., -0.3047,  0.0742, -0.0898]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1680,  0.1079,  0.0198,  ...,  0.3047,  0.4023, -0.0493],\n",
      "         [-0.1875,  0.1279,  0.1865,  ..., -0.2598, -0.1309, -0.1738],\n",
      "         [-0.3320, -0.2002,  0.0317,  ...,  0.1196, -0.1484, -0.0938],\n",
      "         ...,\n",
      "         [-0.0654, -0.0015, -0.1157,  ...,  0.2598,  0.1465,  0.3516],\n",
      "         [ 0.0649,  0.0806,  0.0454,  ..., -0.3125,  0.2617, -0.0889],\n",
      "         [-0.2637,  0.1660, -0.1504,  ..., -0.3438,  0.1797, -0.0977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1709,  0.0635,  0.0085,  ...,  0.2676,  0.5078, -0.0007],\n",
      "         [-0.1855,  0.2070,  0.1875,  ..., -0.2969, -0.0625, -0.3047],\n",
      "         [-0.3613, -0.2617,  0.0156,  ...,  0.0840, -0.1094, -0.0396],\n",
      "         ...,\n",
      "         [-0.0381, -0.0874, -0.1128,  ...,  0.2080,  0.2480,  0.3066],\n",
      "         [ 0.1328,  0.1055,  0.0310,  ..., -0.3203,  0.2891, -0.0791],\n",
      "         [-0.3008,  0.1260, -0.0737,  ..., -0.4473,  0.1475, -0.0151]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797,  0.0669,  0.0342,  ...,  0.2734,  0.5078,  0.0496],\n",
      "         [-0.1104,  0.1816,  0.2266,  ..., -0.3066, -0.0850, -0.1602],\n",
      "         [-0.2578, -0.2490,  0.1328,  ...,  0.1602, -0.2021, -0.1484],\n",
      "         ...,\n",
      "         [-0.0283,  0.0503, -0.2402,  ...,  0.1523,  0.1797,  0.2656],\n",
      "         [ 0.2051,  0.1011, -0.0752,  ..., -0.4492,  0.1406, -0.1377],\n",
      "         [-0.2891,  0.2188, -0.1562,  ..., -0.5234,  0.1719, -0.1758]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1699,  0.0461,  0.0283,  ...,  0.2578,  0.4551,  0.0605],\n",
      "         [-0.0479,  0.1445,  0.2148,  ..., -0.2715, -0.1367, -0.2432],\n",
      "         [-0.0625, -0.2656,  0.1177,  ...,  0.1406, -0.3242, -0.2285],\n",
      "         ...,\n",
      "         [-0.0430, -0.1230, -0.0898,  ...,  0.0161,  0.2412,  0.3008],\n",
      "         [ 0.2422,  0.1128, -0.1011,  ..., -0.6406,  0.1465, -0.1543],\n",
      "         [-0.4258,  0.2412, -0.1426,  ..., -0.6211,  0.1377, -0.2344]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1543,  0.0334,  0.0522,  ...,  0.2852,  0.4531,  0.0688],\n",
      "         [-0.1758,  0.0020,  0.1533,  ..., -0.2988, -0.0513, -0.5234],\n",
      "         [ 0.1035, -0.2559,  0.0571,  ...,  0.1514, -0.4512, -0.3770],\n",
      "         ...,\n",
      "         [-0.2168, -0.3359, -0.1157,  ..., -0.1553,  0.3789,  0.3203],\n",
      "         [ 0.1465,  0.0479, -0.0540,  ..., -0.7812,  0.3945, -0.1426],\n",
      "         [-0.3887,  0.1455, -0.2480,  ..., -0.5820,  0.2520, -0.5469]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1235,  0.0214,  0.0481,  ...,  0.3008,  0.4609,  0.0654],\n",
      "         [-0.2520,  0.1709,  0.2090,  ..., -0.2969, -0.0391, -0.5000],\n",
      "         [ 0.2891, -0.1387, -0.0337,  ...,  0.2070, -0.5859, -0.5273],\n",
      "         ...,\n",
      "         [ 0.0615, -0.2061, -0.1914,  ..., -0.2441,  0.3574,  0.4062],\n",
      "         [ 0.3242,  0.2773, -0.1299,  ..., -0.9570,  0.4160, -0.2324],\n",
      "         [-0.3340,  0.1729, -0.3477,  ..., -0.5547,  0.2070, -0.6211]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.2051, -0.0101, -0.0811,  ...,  0.2539,  0.4355,  0.1777],\n",
      "         [-0.2891,  0.3516, -0.0032,  ..., -0.3770, -0.0947, -0.5000],\n",
      "         [ 0.1709, -0.1924, -0.1514,  ...,  0.1582, -0.6211, -0.4160],\n",
      "         ...,\n",
      "         [ 0.2383, -0.1006, -0.3984,  ..., -0.3730,  0.1309,  0.3633],\n",
      "         [ 0.4863,  0.4297, -0.2891,  ..., -1.1016,  0.3730, -0.2363],\n",
      "         [-0.2891,  0.2734, -0.4824,  ..., -0.5430,  0.1543, -0.5977]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2100, -0.0232, -0.1250,  ...,  0.2363,  0.3887,  0.1562],\n",
      "         [-0.2676,  0.3496,  0.0630,  ..., -0.4727, -0.0776, -0.4473],\n",
      "         [ 0.2412, -0.1650, -0.2080,  ..., -0.0157, -0.6367, -0.5625],\n",
      "         ...,\n",
      "         [ 0.3594, -0.0967, -0.5156,  ..., -0.2520,  0.1582,  0.5000],\n",
      "         [ 0.6367,  0.4707, -0.3594,  ..., -1.0938,  0.4883,  0.0498],\n",
      "         [-0.2197,  0.4062, -0.4473,  ..., -0.7773,  0.1885, -0.7227]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1934, -0.0330, -0.1245,  ...,  0.2285,  0.3730,  0.1514],\n",
      "         [-0.2471,  0.4766,  0.0066,  ..., -0.3086, -0.1582, -0.4570],\n",
      "         [ 0.1836, -0.1807, -0.2559,  ..., -0.0601, -0.6953, -0.7734],\n",
      "         ...,\n",
      "         [ 0.2305, -0.0486, -0.6016,  ..., -0.2754,  0.1523,  0.4141],\n",
      "         [ 0.5820,  0.5742, -0.2500,  ..., -1.2500,  0.5156,  0.1387],\n",
      "         [-0.2969,  0.3145, -0.4980,  ..., -0.7148,  0.1172, -0.7812]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.2012, -0.0522, -0.1631,  ...,  0.2422,  0.3496,  0.1562],\n",
      "         [-0.2520,  0.4648,  0.0164,  ..., -0.4688, -0.1128, -0.4688],\n",
      "         [ 0.1562, -0.1484, -0.4609,  ..., -0.0820, -0.7656, -0.8828],\n",
      "         ...,\n",
      "         [ 0.2793,  0.1187, -0.6797,  ..., -0.2402,  0.2168,  0.3418],\n",
      "         [ 0.5938,  0.8516, -0.4844,  ..., -1.3359,  0.5312,  0.0364],\n",
      "         [-0.3145,  0.2070, -0.5273,  ..., -0.8281,  0.3125, -0.7461]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1797, -0.0381, -0.1934,  ...,  0.2246,  0.3789,  0.1436],\n",
      "         [-0.2891,  0.5039,  0.0835,  ..., -0.4902, -0.1396, -0.4180],\n",
      "         [-0.0222, -0.1099, -0.6719,  ..., -0.2012, -0.7148, -0.8672],\n",
      "         ...,\n",
      "         [ 0.3574,  0.1108, -0.6445,  ..., -0.0684,  0.1094,  0.3594],\n",
      "         [ 0.5039,  0.8008, -0.3477,  ..., -1.2734,  0.4863, -0.0342],\n",
      "         [-0.4512,  0.1553, -0.5703,  ..., -0.7305,  0.1934, -0.8516]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[-0.1167, -0.0493, -0.3184,  ...,  0.1045,  0.5312,  0.0952],\n",
      "         [-0.1650,  0.6133, -0.1943,  ..., -0.0859, -0.2070, -0.3086],\n",
      "         [ 0.0781, -0.0991, -0.7812,  ..., -0.2480, -0.6406, -1.0000],\n",
      "         ...,\n",
      "         [ 0.6250,  0.3555, -0.7500,  ..., -0.0154,  0.1465,  0.3965],\n",
      "         [ 0.7305,  1.1250, -0.3008,  ..., -1.1250,  0.4688,  0.0197],\n",
      "         [-0.2598,  0.5859, -0.7188,  ..., -0.4805,  0.2676, -0.8711]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.1094, -0.0293, -0.2715,  ...,  0.1406,  0.5781,  0.1157],\n",
      "         [-0.3418,  0.7812,  0.0088,  ..., -0.1436, -0.3906, -0.3750],\n",
      "         [-0.0381,  0.0879, -0.9805,  ..., -0.1914, -0.8086, -1.1094],\n",
      "         ...,\n",
      "         [ 0.6562,  0.1875, -0.7422,  ..., -0.1709,  0.2051,  0.4434],\n",
      "         [ 0.6914,  0.9766, -0.2480,  ..., -1.3281,  0.5703, -0.0562],\n",
      "         [-0.3301,  0.4941, -0.6406,  ..., -0.5703,  0.1738, -0.8906]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[-0.0708,  0.0259, -0.2949,  ...,  0.1562,  0.5742,  0.1426],\n",
      "         [-0.4238,  0.5234,  0.1172,  ..., -0.3418, -0.4023, -0.3340],\n",
      "         [ 0.0122,  0.2227, -1.0547,  ...,  0.0791, -0.8672, -1.2344],\n",
      "         ...,\n",
      "         [ 0.1875, -0.1777, -1.0078,  ..., -0.1338,  0.2275,  0.4023],\n",
      "         [ 0.4727,  0.8906, -0.3672,  ..., -1.5312,  0.6172, -0.1406],\n",
      "         [-0.4844,  0.3906, -0.7500,  ..., -0.7148,  0.3086, -0.7734]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.0396,  0.0908, -0.1641,  ...,  0.1050,  0.4062,  0.1338],\n",
      "         [-0.1230,  0.6250, -0.0068,  ..., -0.6289, -0.5781, -0.4883],\n",
      "         [-0.0547,  0.2012, -1.1719,  ...,  0.1738, -1.0469, -1.4453],\n",
      "         ...,\n",
      "         [ 0.1641, -0.4023, -1.2031,  ..., -0.1094,  0.3301,  0.3359],\n",
      "         [ 0.7383,  0.9297, -0.4668,  ..., -1.5234,  0.9297, -0.2402],\n",
      "         [-0.2188,  0.2119, -0.7148,  ..., -0.4805,  0.3789, -0.6875]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 2.4414e-04,  2.5586e-01,  1.0742e-01,  ...,  1.5039e-01,\n",
      "          -2.0898e-01,  3.2812e-01],\n",
      "         [-5.6250e-01,  5.0781e-01, -3.2031e-01,  ..., -8.9844e-01,\n",
      "          -6.5234e-01, -1.0547e+00],\n",
      "         [ 3.3984e-01,  6.2500e-01, -1.7422e+00,  ...,  3.2812e-01,\n",
      "          -1.5469e+00, -1.8359e+00],\n",
      "         ...,\n",
      "         [ 1.7676e-01, -6.2891e-01, -1.2656e+00,  ..., -1.9141e-01,\n",
      "           2.6367e-02, -1.3281e-01],\n",
      "         [ 7.8906e-01,  7.9688e-01, -1.9141e-01,  ..., -1.9375e+00,\n",
      "           1.0938e+00, -4.2578e-01],\n",
      "         [-3.0273e-01,  3.0469e-01, -8.0078e-01,  ..., -5.8594e-01,\n",
      "           7.0312e-01, -1.1328e+00]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "CrossAttentionDecoderLayer Output (tensor([[[ 2.3535e-01,  8.3594e-01,  2.3828e-01,  ...,  3.3984e-01,\n",
      "           1.8359e-01,  7.5391e-01],\n",
      "         [-7.6562e-01,  3.3594e-01, -1.4648e-03,  ..., -4.6875e-01,\n",
      "          -7.4219e-01, -1.7188e+00],\n",
      "         [ 7.6562e-01,  5.8203e-01, -1.8594e+00,  ...,  6.9922e-01,\n",
      "          -1.4922e+00, -1.4062e+00],\n",
      "         ...,\n",
      "         [ 5.0781e-01, -7.9297e-01, -1.2969e+00,  ...,  6.2109e-01,\n",
      "           3.8281e-01,  1.3672e-01],\n",
      "         [ 7.7344e-01,  7.6562e-01, -7.2656e-01,  ..., -1.2188e+00,\n",
      "           1.4531e+00,  5.7617e-02],\n",
      "         [-5.5859e-01, -1.5625e-02, -4.0625e-01,  ..., -7.6562e-01,\n",
      "           5.0781e-01, -1.0312e+00]]], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<AddBackward0>), DynamicCache())\n",
      "SelfAttentionDecoderLayer (tensor([[[ 0.4961,  2.6094,  0.9453,  ..., -0.9727,  1.9453,  2.7656],\n",
      "         [-1.1094,  1.9609,  0.2559,  ..., -0.5859,  1.2656, -1.0312],\n",
      "         [ 0.5469,  1.3750,  0.0156,  ..., -1.5625,  1.3359, -2.8125],\n",
      "         ...,\n",
      "         [-0.5312, -0.3281, -0.4492,  ...,  1.2266, -1.2188,  1.1250],\n",
      "         [-0.4844,  0.9492,  0.3008,  ..., -1.4141, -0.3984,  1.2891],\n",
      "         [-0.8047,  0.4434, -0.4375,  ..., -1.1562, -0.1582, -1.2422]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<AddBackward0>), DynamicCache())\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithPast(loss=None, logits=tensor([[[-0.4062, -0.1211,  3.3281,  ..., -0.3477, -0.3477, -0.3477],\n",
       "         [ 6.8438, 12.6875,  9.3750,  ..., -1.5000, -1.5000, -1.5000],\n",
       "         [-1.7500,  0.0442,  0.0776,  ...,  2.4219,  2.4219,  2.4219],\n",
       "         ...,\n",
       "         [ 6.3750,  8.8750,  4.9375,  ..., -3.4531, -3.4531, -3.4531],\n",
       "         [ 7.4375,  8.3750,  5.2812,  ..., -3.3906, -3.3906, -3.3906],\n",
       "         [ 8.3750,  9.3750,  5.9062,  ..., -2.7500, -2.7500, -2.7500]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>), past_key_values=DynamicCache(), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_model.forward(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jan 26 06:03:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 6000 Ada Gene...    On  |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   46C    P2            102W /  300W |   33786MiB /  46068MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sasika Pamith\n"
     ]
    }
   ],
   "source": [
    "expert_model.sasika_pamith()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
