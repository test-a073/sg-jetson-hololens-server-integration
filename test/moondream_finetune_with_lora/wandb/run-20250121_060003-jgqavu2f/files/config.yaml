_wandb:
    value:
        cli_version: 0.19.2
        m: []
        python_version: 3.11.10
        t:
            "1":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
            "2":
                - 1
                - 11
                - 41
                - 49
                - 51
                - 55
                - 71
                - 98
            "3":
                - 2
                - 16
                - 23
                - 55
            "4": 3.11.10
            "5": 0.19.2
            "6": 4.47.1
            "8":
                - 5
            "12": 0.19.2
            "13": linux-x86_64
BASE_LR:
    value: 3e-05
BATCH_SIZE:
    value: 4
EPOCHS:
    value: 10
GRAD_ACCUM_STEPS:
    value: 1
MAX_GRAD_NORM:
    value: 1
WARMUP_STEPS:
    value: 100
lora_config:
    value:
        _custom_modules: null
        auto_mapping: null
        base_model_name_or_path: vikhyatk/moondream2
        bias: lora_only
        eva_config: null
        exclude_modules: null
        fan_in_fan_out: false
        inference_mode: false
        init_lora_weights: true
        layer_replication: null
        layers_pattern: null
        layers_to_transform: null
        lora_alpha: 32
        lora_bias: false
        lora_dropout: 0.1
        megatron_config: null
        megatron_core: megatron.core
        modules_to_save: null
        peft_type: LORA
        r: 4
        revision: null
        runtime_config:
            ephemeral_gpu_offload: false
        target_modules:
            - fc2
            - fc1
        task_type: CAUSAL_LM
        use_dora: false
        use_rslora: false
use_4bit:
    value: null
use_lora:
    value: true
