2025-01-13 14:12:05,536 - __main__ - INFO - Initializing model...
2025-01-13 14:12:05,537 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-01-13 14:12:05,777 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/tokenizer_config.json HTTP/11" 200 0
2025-01-13 14:12:06,060 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/config.json HTTP/11" 200 0
2025-01-13 14:12:06,292 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/configuration_moondream.py HTTP/11" 200 0
2025-01-13 14:12:06,534 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/moondream.py HTTP/11" 200 0
PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-13 14:12:07,441 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/generation_config.json HTTP/11" 200 0
 * Serving Flask app 'flask_server_app_LARGE_Moondream'
 * Debug mode: off
2025-01-13 14:12:07,449 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://172.17.0.2:5000
2025-01-13 14:12:07,449 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-01-13 14:12:21,940 - __main__ - DEBUG - Received request: ImmutableMultiDict([('query', 'where is the water bottle')])
2025-01-13 14:12:21,940 - __main__ - DEBUG - Files in request: ImmutableMultiDict([('image', <FileStorage: 'sample.jpg' (None)>)])
2025-01-13 14:12:21,941 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-01-13 14:12:21,941 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x76d43c275e10>
2025-01-13 14:12:21,941 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-13 14:12:21,941 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-13 14:12:21,941 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-13 14:12:21,941 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-13 14:12:21,942 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-13 14:12:23,472 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Mon, 13 Jan 2025 14:12:23 GMT'), (b'Content-Length', b'367')])
2025-01-13 14:12:23,473 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-01-13 14:12:23,473 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-13 14:12:23,473 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-13 14:12:23,473 - httpcore.http11 - DEBUG - response_closed.started
2025-01-13 14:12:23,473 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-13 14:12:23,473 - __main__ - DEBUG - Full response: {'model': 'llama3.2-vision', 'created_at': '2025-01-13T14:12:23.472436146Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1530519656, 'load_duration': 17694492, 'prompt_eval_count': 17, 'prompt_eval_duration': 1252000000, 'eval_count': 15, 'eval_duration': 182000000, 'message': Message(role='assistant', content='The water bottle is on the desk in front of the computer monitor.', images=None, tool_calls=None)}
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48

{'time': '20250113-141223', 'text': 'where is the water bottle', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=760x428 at 0x76D43C2BF750>, 'image_name': 'sample.jpg_20250113-141223', 'qa': [{'question': 'where is the water bottle', 'answer': 'The water bottle is on the desk in front of the computer monitor.'}], 'moondream': 'On the desk', 'llama-vision': 'The water bottle is on the desk in front of the computer monitor.'}

 start finetuning the model
2025-01-13 14:12:23,744 - transformers_modules.vikhyatk.moondream2.79671eae7b5340017e91065d09c1ce1a352c0e8d.modeling_phi - WARNING - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Cleaning up...
2025-01-13 14:12:27,287 - werkzeug - INFO - 10.4.8.31 - - [13/Jan/2025 14:12:27] "POST /query HTTP/1.1" 200 -
