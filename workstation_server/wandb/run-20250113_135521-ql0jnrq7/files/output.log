2025-01-13 13:55:22,574 - __main__ - INFO - Initializing model...
2025-01-13 13:55:22,575 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-01-13 13:55:23,425 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/tokenizer_config.json HTTP/11" 200 0
2025-01-13 13:55:23,698 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/config.json HTTP/11" 200 0
2025-01-13 13:55:23,929 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/configuration_moondream.py HTTP/11" 200 0
2025-01-13 13:55:24,168 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/moondream.py HTTP/11" 200 0
PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-13 13:55:25,102 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/generation_config.json HTTP/11" 200 0
 * Serving Flask app 'flask_server_app_LARGE_Moondream'
 * Debug mode: off
2025-01-13 13:55:25,109 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://172.17.0.2:5000
2025-01-13 13:55:25,109 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-01-13 13:55:29,258 - __main__ - DEBUG - Received request: ImmutableMultiDict([('query', 'where is the phone')])
2025-01-13 13:55:29,259 - __main__ - DEBUG - Files in request: ImmutableMultiDict([('image', <FileStorage: 'sample.jpg' (None)>)])
2025-01-13 13:55:29,259 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-01-13 13:55:29,260 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x761e7015e950>
2025-01-13 13:55:29,260 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-13 13:55:29,260 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-13 13:55:29,260 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-13 13:55:29,260 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-13 13:55:29,260 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-13 13:55:29,528 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Mon, 13 Jan 2025 13:55:29 GMT'), (b'Content-Length', b'356')])
2025-01-13 13:55:29,528 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-01-13 13:55:29,528 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-13 13:55:29,528 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-13 13:55:29,528 - httpcore.http11 - DEBUG - response_closed.started
2025-01-13 13:55:29,528 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-13 13:55:29,529 - __main__ - DEBUG - Full response: {'model': 'llama3.2-vision', 'created_at': '2025-01-13T13:55:29.528062295Z', 'done': True, 'done_reason': 'stop', 'total_duration': 267598927, 'load_duration': 10188973, 'prompt_eval_count': 16, 'prompt_eval_duration': 16000000, 'eval_count': 13, 'eval_duration': 156000000, 'message': Message(role='assistant', content='The phone is on the desk between the keyboard and laptop.', images=None, tool_calls=None)}
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48

{'time': '20250113-135529', 'text': 'where is the phone', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=760x428 at 0x761E7015F2D0>, 'image_name': 'sample.jpg_20250113-135529', 'qa': [{'question': 'where is the phone', 'answer': 'The phone is on the desk between the keyboard and laptop.'}], 'moondream': 'On the desk', 'llama-vision': 'The phone is on the desk between the keyboard and laptop.'}

 start finetuning the model
2025-01-13 13:55:29,801 - transformers_modules.vikhyatk.moondream2.79671eae7b5340017e91065d09c1ce1a352c0e8d.modeling_phi - WARNING - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Cleaning up...
2025-01-13 13:55:33,713 - werkzeug - INFO - 10.4.8.31 - - [13/Jan/2025 13:55:33] "POST /query HTTP/1.1" 200 -
