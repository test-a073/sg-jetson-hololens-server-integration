2025-01-13 14:06:15,240 - __main__ - INFO - Initializing model...
2025-01-13 14:06:15,240 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-01-13 14:06:15,496 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/tokenizer_config.json HTTP/11" 200 0
2025-01-13 14:06:15,764 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/config.json HTTP/11" 200 0
2025-01-13 14:06:15,997 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/configuration_moondream.py HTTP/11" 200 0
2025-01-13 14:06:16,327 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/moondream.py HTTP/11" 200 0
PhiForCausalLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.
  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes
  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).
  - If you are not the owner of the model architecture class, please contact the model code owner to update it.
2025-01-13 14:06:17,229 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /vikhyatk/moondream2/resolve/2024-07-23/generation_config.json HTTP/11" 200 0
 * Serving Flask app 'flask_server_app_LARGE_Moondream'
 * Debug mode: off
2025-01-13 14:06:17,236 - werkzeug - INFO - [31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:5000
 * Running on http://172.17.0.2:5000
2025-01-13 14:06:17,236 - werkzeug - INFO - [33mPress CTRL+C to quit[0m
2025-01-13 14:06:32,682 - __main__ - DEBUG - Received request: ImmutableMultiDict([('query', 'where is the phone')])
2025-01-13 14:06:32,682 - __main__ - DEBUG - Files in request: ImmutableMultiDict([('image', <FileStorage: 'sample.jpg' (None)>)])
2025-01-13 14:06:32,683 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-01-13 14:06:32,683 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x760a725b6d50>
2025-01-13 14:06:32,683 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-13 14:06:32,683 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-13 14:06:32,683 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-13 14:06:32,683 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-13 14:06:32,683 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-13 14:06:35,971 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Mon, 13 Jan 2025 14:06:35 GMT'), (b'Content-Length', b'362')])
2025-01-13 14:06:35,971 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-01-13 14:06:35,971 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-13 14:06:35,971 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-13 14:06:35,971 - httpcore.http11 - DEBUG - response_closed.started
2025-01-13 14:06:35,971 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-13 14:06:35,972 - __main__ - DEBUG - Full response: {'model': 'llama3.2-vision', 'created_at': '2025-01-13T14:06:35.970985885Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3287352265, 'load_duration': 1615010410, 'prompt_eval_count': 16, 'prompt_eval_duration': 1404000000, 'eval_count': 14, 'eval_duration': 180000000, 'message': Message(role='assistant', content='The phone is on the desk, between the keyboard and laptop.', images=None, tool_calls=None)}
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48

{'time': '20250113-140635', 'text': 'where is the phone', 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=760x428 at 0x760A725B8B10>, 'image_name': 'sample.jpg_20250113-140635', 'qa': [{'question': 'where is the phone', 'answer': 'The phone is on the desk, between the keyboard and laptop.'}], 'moondream': 'On the desk', 'llama-vision': 'The phone is on the desk, between the keyboard and laptop.'}

 start finetuning the model
2025-01-13 14:06:36,238 - transformers_modules.vikhyatk.moondream2.79671eae7b5340017e91065d09c1ce1a352c0e8d.modeling_phi - WARNING - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Cleaning up...
2025-01-13 14:06:39,695 - werkzeug - INFO - 10.4.8.31 - - [13/Jan/2025 14:06:39] "POST /query HTTP/1.1" 200 -
2025-01-13 14:08:30,617 - __main__ - DEBUG - Received request: ImmutableMultiDict([('query', 'where is the phone')])
2025-01-13 14:08:30,617 - __main__ - DEBUG - Files in request: ImmutableMultiDict([('image', <FileStorage: 'sample.jpg' (None)>)])
2025-01-13 14:08:30,618 - httpcore.connection - DEBUG - close.started
2025-01-13 14:08:30,618 - httpcore.connection - DEBUG - close.complete
2025-01-13 14:08:30,618 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-01-13 14:08:30,618 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x760a700b8e90>
2025-01-13 14:08:30,618 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-13 14:08:30,618 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-13 14:08:30,618 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-13 14:08:30,618 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-13 14:08:30,618 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-13 14:08:32,163 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Mon, 13 Jan 2025 14:08:32 GMT'), (b'Content-Length', b'367')])
2025-01-13 14:08:32,164 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-01-13 14:08:32,164 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-13 14:08:32,164 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-13 14:08:32,164 - httpcore.http11 - DEBUG - response_closed.started
2025-01-13 14:08:32,164 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-13 14:08:32,164 - __main__ - DEBUG - Full response: {'model': 'llama3.2-vision', 'created_at': '2025-01-13T14:08:32.16370408Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1544935360, 'load_duration': 10077389, 'prompt_eval_count': 16, 'prompt_eval_duration': 1245000000, 'eval_count': 17, 'eval_duration': 207000000, 'message': Message(role='assistant', content='The phone is on the desk, to the right of the silver water bottle.', images=None, tool_calls=None)}
/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
2025-01-13 14:08:32,214 - __main__ - ERROR - Error in query_moondream: Sizes of tensors must match except in dimension 3. Expected size 743 but got size 1 for tensor number 1 in the list.
2025-01-13 14:08:32,214 - __main__ - ERROR - Error in query endpoint: Sizes of tensors must match except in dimension 3. Expected size 743 but got size 1 for tensor number 1 in the list.
Traceback (most recent call last):
  File "/home/Desktop/integration/workstation_server/flask_server_app_LARGE_Moondream.py", line 237, in query
    moondream_response = query_moondream(moondream, tokenizer, queries)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/Desktop/integration/workstation_server/flask_server_app_LARGE_Moondream.py", line 154, in query_moondream
    return model.answer_question(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/moondream.py", line 99, in answer_question
    answer = self.generate(
             ^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/moondream.py", line 83, in generate
    output_ids = self.text_model.generate(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py", line 2255, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/generation/utils.py", line 3257, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/modeling_phi.py", line 1051, in forward
    outputs = self.transformer(
              ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/modeling_phi.py", line 897, in forward
    layer_outputs = self._gradient_checkpointing_func(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/modeling_phi.py", line 710, in forward
    attn_outputs, self_attn_weights, present_key_value = self.mixer(
                                                         ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/modeling_phi.py", line 368, in forward
    query_states = torch.cat((query_rot, query_pass), dim=-1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 3. Expected size 743 but got size 1 for tensor number 1 in the list.
2025-01-13 14:08:32,216 - werkzeug - INFO - 10.4.8.31 - - [13/Jan/2025 14:08:32] "[35m[1mPOST /query HTTP/1.1[0m" 500 -
2025-01-13 14:11:12,185 - __main__ - DEBUG - Received request: ImmutableMultiDict([('query', 'where is the water bottle')])
2025-01-13 14:11:12,185 - __main__ - DEBUG - Files in request: ImmutableMultiDict([('image', <FileStorage: 'sample.jpg' (None)>)])
2025-01-13 14:11:12,185 - httpcore.connection - DEBUG - close.started
2025-01-13 14:11:12,186 - httpcore.connection - DEBUG - close.complete
2025-01-13 14:11:12,186 - httpcore.connection - DEBUG - connect_tcp.started host='127.0.0.1' port=11434 local_address=None timeout=None socket_options=None
2025-01-13 14:11:12,186 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x760a700b4490>
2025-01-13 14:11:12,186 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-01-13 14:11:12,186 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-01-13 14:11:12,186 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-01-13 14:11:12,186 - httpcore.http11 - DEBUG - send_request_body.complete
2025-01-13 14:11:12,186 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-01-13 14:11:13,708 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=utf-8'), (b'Date', b'Mon, 13 Jan 2025 14:11:13 GMT'), (b'Content-Length', b'367')])
2025-01-13 14:11:13,708 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/chat "HTTP/1.1 200 OK"
2025-01-13 14:11:13,708 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-01-13 14:11:13,708 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-01-13 14:11:13,709 - httpcore.http11 - DEBUG - response_closed.started
2025-01-13 14:11:13,709 - httpcore.http11 - DEBUG - response_closed.complete
2025-01-13 14:11:13,709 - __main__ - DEBUG - Full response: {'model': 'llama3.2-vision', 'created_at': '2025-01-13T14:11:13.708151438Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1521646064, 'load_duration': 10261744, 'prompt_eval_count': 17, 'prompt_eval_duration': 1248000000, 'eval_count': 15, 'eval_duration': 182000000, 'message': Message(role='assistant', content='The water bottle is on the desk in front of the computer monitor.', images=None, tool_calls=None)}
2025-01-13 14:11:13,726 - __main__ - ERROR - Error in query_moondream: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-01-13 14:11:13,727 - __main__ - ERROR - Error in query endpoint: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
Traceback (most recent call last):
  File "/home/Desktop/integration/workstation_server/flask_server_app_LARGE_Moondream.py", line 237, in query
    moondream_response = query_moondream(moondream, tokenizer, queries)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/Desktop/integration/workstation_server/flask_server_app_LARGE_Moondream.py", line 155, in query_moondream
    model.encode_image(image),
    ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/moondream.py", line 33, in encode_image
    return self.vision_encoder(image)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/vikhyatk/moondream2/79671eae7b5340017e91065d09c1ce1a352c0e8d/vision_encoder.py", line 285, in forward
    combined_images = combined_images.to(self.device, dtype=self.dtype)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

2025-01-13 14:11:13,727 - werkzeug - INFO - 10.4.8.31 - - [13/Jan/2025 14:11:13] "[35m[1mPOST /query HTTP/1.1[0m" 500 -
